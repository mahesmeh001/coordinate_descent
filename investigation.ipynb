{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "get and format dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81dcf1a8e907311"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "0      1    14.23       1.71  2.43               15.6        127   \n",
      "1      1    13.20       1.78  2.14               11.2        100   \n",
      "2      1    13.16       2.36  2.67               18.6        101   \n",
      "3      1    14.37       1.95  2.50               16.8        113   \n",
      "4      1    13.24       2.59  2.87               21.0        118   \n",
      "\n",
      "   Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# load in wine \n",
    "import pandas as pd\n",
    "\n",
    "# Define column names\n",
    "column_names = [\n",
    "    \"Class\", \"Alcohol\", \"Malicacid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\",\n",
    "    \"Total_phenols\", \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\",\n",
    "    \"Color_intensity\", \"Hue\", \"OD280_OD315_of_diluted_wines\", \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"wine/wine.data\", names=column_names)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:42.846515Z",
     "start_time": "2025-02-25T00:23:42.769834Z"
    }
   },
   "id": "initial_id",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "125      2    12.07       2.16  2.17               21.0         85   \n",
      "126      2    12.43       1.53  2.29               21.5         86   \n",
      "127      2    11.79       2.13  2.78               28.5         92   \n",
      "128      2    12.37       1.63  2.30               24.5         88   \n",
      "129      2    12.04       4.30  2.38               22.0         80   \n",
      "\n",
      "     Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "125           2.60        2.65                  0.37             1.35   \n",
      "126           2.74        3.15                  0.39             1.77   \n",
      "127           2.13        2.24                  0.58             1.76   \n",
      "128           2.22        2.45                  0.40             1.90   \n",
      "129           2.10        1.75                  0.42             1.35   \n",
      "\n",
      "     Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "125             2.76  0.86                          3.28      378  \n",
      "126             3.94  0.69                          2.84      352  \n",
      "127             3.00  0.97                          2.44      466  \n",
      "128             2.12  0.89                          2.78      342  \n",
      "129             2.60  0.79                          2.57      580  \n"
     ]
    }
   ],
   "source": [
    "# Remove rows where Class is 3\n",
    "df = df[df[\"Class\"] != 3]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.tail())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:42.923768Z",
     "start_time": "2025-02-25T00:23:42.855264Z"
    }
   },
   "id": "2d8045463ae6a81c",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement a logistic regression baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5225897f44a5679"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:42.924084Z",
     "start_time": "2025-02-25T00:23:42.895305Z"
    }
   },
   "id": "118114fbb2dd5fd0",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop(columns=['Class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:43.115127Z",
     "start_time": "2025-02-25T00:23:42.944567Z"
    }
   },
   "id": "7fd4db81cc7da185",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/Human-AI Research/Coordinate Descent/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=700)",
      "text/html": "<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-2 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=700)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=700)</pre></div> </div></div></div></div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=700)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:43.483324Z",
     "start_time": "2025-02-25T00:23:43.171684Z"
    }
   },
   "id": "dd62d0b2c944a9e3",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Baseline Logistic Regression Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:43.490061Z",
     "start_time": "2025-02-25T00:23:43.481927Z"
    }
   },
   "id": "258146464d8cdf32",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we specify a custom method of selecting the next coordinate. For this, lets create a function that takes in X and y, and returns an array specifying the order we want to test in. \n",
    "\n",
    "Solution idea: Cluster the data by features, and then rotate data points from each cluster. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "391738e96fb12e42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:24:21.848357Z",
     "start_time": "2025-02-25T00:24:18.165879Z"
    }
   },
   "id": "b1a67f2179b19057",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need a custom class so we can directly modify the loss function... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d419fb738aa61fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, max_iter=2000, tol=1e-4, fit_intercept=True, reg_lambda=0.01):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol  # Tolerance for weight convergence\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weight = None\n",
    "        self.feature_means = None\n",
    "        self.feature_stds = None\n",
    "        self.convergence_window = 10  # Number of iterations to check for weight stability\n",
    "\n",
    "    def _normalize_features(self, X):\n",
    "        \"\"\"Normalizes features using z-score standardization.\"\"\"\n",
    "        if self.feature_means is None or self.feature_stds is None:\n",
    "            self.feature_means = np.mean(X, axis=0)\n",
    "            self.feature_stds = np.std(X, axis=0) + 1e-8  # Avoid division by zero\n",
    "        \n",
    "        return (X - self.feature_means) / self.feature_stds\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Adds intercept term (bias) to feature matrix.\"\"\"\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip values to prevent overflow\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return expit(z)  # Numerically stable sigmoid\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"Computes the binary cross-entropy loss with regularization.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Add L2 regularization term (excluding bias)\n",
    "        reg_term = 0.5 * self.reg_lambda * np.sum(self.weight[1:] ** 2) / m\n",
    "        \n",
    "        # Compute binary cross-entropy\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        \n",
    "        return loss + reg_term\n",
    "    \n",
    "    def _compute_gradient_for_coordinate(self, X, y, j, sample_indices=None, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Computes the gradient for a single coordinate j using only the specified samples.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        j : int\n",
    "            Index of the coordinate to compute gradient for\n",
    "        sample_indices : array-like or None\n",
    "            Indices of samples to use for gradient computation. If None, use all samples.\n",
    "        sample_weights : array-like or None\n",
    "            Weights for each sample. If None, equal weights are used.\n",
    "        \"\"\"\n",
    "        if sample_indices is not None:\n",
    "            X_batch = X[sample_indices]\n",
    "            y_batch = y[sample_indices]\n",
    "            if sample_weights is not None:\n",
    "                weights_batch = sample_weights[sample_indices]\n",
    "            else:\n",
    "                weights_batch = None\n",
    "        else:\n",
    "            X_batch = X\n",
    "            y_batch = y\n",
    "            weights_batch = sample_weights\n",
    "        \n",
    "        m = len(y_batch)\n",
    "        h = self._sigmoid(X_batch @ self.weight)\n",
    "        \n",
    "        if weights_batch is None:\n",
    "            weights_batch = np.ones(m)\n",
    "            \n",
    "        # Gradient for single coordinate using only the batch\n",
    "        gradient_j = np.sum((h - y_batch) * weights_batch * X_batch[:, j]) / np.sum(weights_batch)\n",
    "        \n",
    "        # Add L2 regularization (except for bias term)\n",
    "        if j > 0 or not self.fit_intercept:\n",
    "            gradient_j += self.reg_lambda * self.weight[j] / m\n",
    "            \n",
    "        return gradient_j\n",
    "    \n",
    "    def _custom_gradient_for_coordinate(self, X, y, j, sample_indices=None, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Custom gradient computation for a single coordinate.\n",
    "        This implements a focal loss variant that puts more emphasis on hard examples.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        j : int\n",
    "            Index of the coordinate to compute gradient for\n",
    "        sample_indices : array-like or None\n",
    "            Indices of samples to use for gradient computation\n",
    "        sample_weights : array-like or None\n",
    "            Weights for each sample\n",
    "        \"\"\"\n",
    "        if sample_indices is not None:\n",
    "            X_batch = X[sample_indices]\n",
    "            y_batch = y[sample_indices]\n",
    "            if sample_weights is not None:\n",
    "                weights_batch = sample_weights[sample_indices]\n",
    "            else:\n",
    "                weights_batch = None\n",
    "        else:\n",
    "            X_batch = X\n",
    "            y_batch = y\n",
    "            weights_batch = sample_weights\n",
    "        \n",
    "        m = len(y_batch)\n",
    "        \n",
    "        # Convert labels from 1/2 to 0/1 for easier calculation\n",
    "        y_binary = (y_batch == 2).astype(int)\n",
    "        \n",
    "        # Get current predictions\n",
    "        h = self._sigmoid(X_batch @ self.weight)\n",
    "        \n",
    "        # Calculate errors with focal loss modulation\n",
    "        # Focal loss puts more weight on hard examples\n",
    "        gamma = 2.0  # Focusing parameter\n",
    "        \n",
    "        # pt is the probability of the true class\n",
    "        pt = h * y_binary + (1 - h) * (1 - y_binary)\n",
    "        # Focal weight is (1-pt)^gamma\n",
    "        focal_weights = np.power(1 - pt, gamma)\n",
    "        \n",
    "        if weights_batch is None:\n",
    "            weights_batch = np.ones(m)\n",
    "            \n",
    "        # Combine focal weights with sample weights\n",
    "        combined_weights = weights_batch * focal_weights\n",
    "        \n",
    "        # Gradient for single coordinate using focal loss\n",
    "        errors = h - y_binary\n",
    "        gradient_j = np.sum(errors * combined_weights * X_batch[:, j]) / np.sum(combined_weights)\n",
    "        \n",
    "        # Add L2 regularization (except for bias term)\n",
    "        if j > 0 or not self.fit_intercept:\n",
    "            gradient_j += self.reg_lambda * self.weight[j] / m\n",
    "            \n",
    "        return gradient_j\n",
    "\n",
    "    def _get_coordinates(self, X, y, n_features, method=\"random\", batch_size=32):\n",
    "        \"\"\"\n",
    "        Determine the coordinates and data points to use for stochastic coordinate descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        n_features : int\n",
    "            Number of features\n",
    "        method : str\n",
    "            Method to select coordinates (\"random\", \"gradient_based\", or \"cyclic\")\n",
    "        batch_size : int\n",
    "            Number of samples to use per update (for mini-batch)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (coordinates, batches)\n",
    "            coordinates: List of feature indices to update\n",
    "            batches: List of sample indices for each update\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Determine coordinates (features) to update\n",
    "        if method == \"random\":\n",
    "            # Randomly select coordinates for each iteration\n",
    "            coordinates = np.random.randint(0, n_features, size=self.max_iter)\n",
    "        elif method == \"gradient_based\":\n",
    "            # Select coordinates based on gradient magnitude (more informative features)\n",
    "            # First, calculate full gradient\n",
    "            # Compute initial gradients for each coordinate\n",
    "            initial_gradients = np.zeros(n_features)\n",
    "            for j in range(n_features):\n",
    "                initial_gradients[j] = abs(self._compute_gradient_for_coordinate(X, y, j))\n",
    "            \n",
    "            # Select coordinates proportional to the absolute gradient values\n",
    "            probs = initial_gradients / np.sum(initial_gradients)\n",
    "            coordinates = np.random.choice(n_features, size=self.max_iter, p=probs)\n",
    "        elif method == \"cyclic\":\n",
    "            # Cycle through all coordinates repeatedly\n",
    "            coordinates = np.arange(self.max_iter) % n_features\n",
    "        else:\n",
    "            # Default to random if invalid method\n",
    "            coordinates = np.random.randint(0, n_features, size=self.max_iter)\n",
    "            \n",
    "        # Create mini-batches of data points\n",
    "        # Number of batches needed for max_iter iterations\n",
    "        n_batches = self.max_iter\n",
    "        \n",
    "        # Generate random mini-batch indices for each iteration\n",
    "        batches = [np.random.choice(n_samples, size=batch_size) for _ in range(n_batches)]\n",
    "            \n",
    "        return coordinates, batches\n",
    "\n",
    "    def _handle_class_imbalance(self, X, y):\n",
    "        \"\"\"Calculate class weights for imbalanced datasets.\"\"\"\n",
    "        unique_classes = np.unique(y)\n",
    "        class_weights = {}\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            class_weights[cls] = len(y) / (len(unique_classes) * np.sum(y == cls))\n",
    "            \n",
    "        # Create sample weights array\n",
    "        sample_weights = np.ones(len(y))\n",
    "        for cls in unique_classes:\n",
    "            sample_weights[y == cls] = class_weights[cls]\n",
    "            \n",
    "        return sample_weights\n",
    "    \n",
    "    def fit(self, X, y, loss_type=\"baseline\", coordinate_choice=\"random\", \n",
    "        batch_size=32, handle_imbalance=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the logistic regression model using stochastic gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        loss_type : str, default='baseline'\n",
    "            Loss function type ('baseline' or 'custom')\n",
    "        coordinate_choice : str, default='random'\n",
    "            How to choose coordinates for SGD ('random', 'gradient_based', or 'cyclic')\n",
    "        batch_size : int, default=32\n",
    "            Number of samples to use in each mini-batch update\n",
    "        handle_imbalance : bool, default=True\n",
    "            Whether to weigh samples by inverse class frequency\n",
    "        verbose : bool, default=True\n",
    "            Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracies : array-like\n",
    "            Array of accuracy values during training\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "    \n",
    "        # Initialize weights with small random values\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        n_features = X_normalized.shape[1]\n",
    "        self.weight = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        if handle_imbalance:\n",
    "            sample_weights = self._handle_class_imbalance(X, y)\n",
    "        else:\n",
    "            sample_weights = np.ones(len(y))\n",
    "        \n",
    "        # Store weight history for convergence check\n",
    "        weight_history = [np.copy(self.weight)]\n",
    "        converged = False\n",
    "        \n",
    "        # Get coordinate order and data batches\n",
    "        coordinates, batches = self._get_coordinates(\n",
    "            X_normalized, y, n_features, method=coordinate_choice, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Array to track accuracies during training\n",
    "        accuracies = []\n",
    "        check_interval = max(1, self.max_iter // 100)  # Check accuracy ~100 times\n",
    "        \n",
    "        min_updates_per_coordinate = 3  # Ensure each coordinate has been updated enough times\n",
    "        coordinate_update_counts = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Get current coordinate and batch indices\n",
    "            j = coordinates[i]\n",
    "            batch_indices = batches[i]\n",
    "            \n",
    "            # Track coordinate updates\n",
    "            coordinate_update_counts[j] += 1\n",
    "            \n",
    "            # Use appropriate gradient calculation for the selected coordinate\n",
    "            if loss_type == \"baseline\":\n",
    "                gradient_j = self._compute_gradient_for_coordinate(\n",
    "                    X_normalized, y, j, batch_indices, sample_weights\n",
    "                )\n",
    "            else:  # Custom loss\n",
    "                gradient_j = self._custom_gradient_for_coordinate(\n",
    "                    X_normalized, y, j, batch_indices, sample_weights\n",
    "                )\n",
    "            \n",
    "            # Update only the selected coordinate\n",
    "            self.weight[j] -= self.lr * gradient_j\n",
    "            \n",
    "            # Check convergence and track accuracy periodically\n",
    "            if i % check_interval == 0 or i == self.max_iter - 1:\n",
    "                # Store current weights for convergence check\n",
    "                weight_history.append(np.copy(self.weight))\n",
    "                \n",
    "                # Keep only the most recent weights for convergence check\n",
    "                if len(weight_history) > self.convergence_window:\n",
    "                    weight_history.pop(0)\n",
    "                \n",
    "                # Calculate current accuracy\n",
    "                y_pred = (self._sigmoid(X_normalized @ self.weight) >= 0.5).astype(int) + 1\n",
    "                current_accuracy = np.mean(y_pred == y)\n",
    "                accuracies.append(current_accuracy)\n",
    "                \n",
    "                # Check for weight convergence if we have enough history\n",
    "                if len(weight_history) == self.convergence_window:\n",
    "                    # Check if all coordinates have been updated enough times\n",
    "                    if np.all(coordinate_update_counts >= min_updates_per_coordinate):\n",
    "                        # Calculate max weight change across the window and all coordinates\n",
    "                        weight_changes = []\n",
    "                        for w_idx in range(1, len(weight_history)):\n",
    "                            # Calculate absolute change in weights\n",
    "                            weight_diff = np.abs(weight_history[w_idx] - weight_history[w_idx-1])\n",
    "                            # Get maximum change for any coordinate\n",
    "                            weight_changes.append(np.max(weight_diff))\n",
    "                        \n",
    "                        # Check if max weight change is less than tolerance\n",
    "                        if np.max(weight_changes) < self.tol:\n",
    "                            if verbose:\n",
    "                                print(f'Converged at iteration {i}, weights stabilized with max change: {np.max(weight_changes):.6f}, accuracy: {current_accuracy:.4f}')\n",
    "                            converged = True\n",
    "                            # Fill the rest of the accuracy array with the final value\n",
    "                            accuracies.extend([current_accuracy] * ((self.max_iter // check_interval) - len(accuracies)))\n",
    "                            break\n",
    "                \n",
    "                # Adaptive learning rate\n",
    "                if i > 0 and i % (5 * n_features) == 0:\n",
    "                    self.lr *= 0.90\n",
    "        \n",
    "        if not converged and verbose:\n",
    "            print(f'Did not converge after {self.max_iter} iterations. Final accuracy: {current_accuracy:.4f}')\n",
    "            \n",
    "        # Make sure we return exactly 100 points for easier plotting\n",
    "        if len(accuracies) < 100:\n",
    "            # Repeat last value to fill\n",
    "            accuracies.extend([accuracies[-1]] * (100 - len(accuracies)))\n",
    "        elif len(accuracies) > 100:\n",
    "            # Downsample by taking evenly spaced points\n",
    "            indices = np.linspace(0, len(accuracies)-1, 100, dtype=int)\n",
    "            accuracies = [accuracies[i] for i in indices]\n",
    "            \n",
    "        return np.array(accuracies)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns the probability predictions.\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "\n",
    "        return self._sigmoid(X_normalized @ self.weight)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predicts class labels (0 or 1) based on a threshold.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int) + 1\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Returns the absolute weights as feature importance.\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            return np.abs(self.weight[1:])\n",
    "        return np.abs(self.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:30:23.410411Z",
     "start_time": "2025-02-25T00:30:23.336096Z"
    }
   },
   "id": "a2c657a8dc411921",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def grid_search(X, y, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=True, \n",
    "               loss_type=\"baseline\", coordinate_choice=\"random\"):\n",
    "    \"\"\"\n",
    "    Perform grid search to find optimal hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    param_grid : dict\n",
    "        Dictionary with parameter names as keys and lists of parameter values\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    scoring : str, default='accuracy'\n",
    "        Scoring metric ('accuracy', 'precision', 'recall', 'f1', 'auc')\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel (-1 means using all processors)\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress\n",
    "    loss_type : str, default='baseline'\n",
    "        Loss function type ('baseline' or 'custom')\n",
    "    coordinate_choice : str, default='random'\n",
    "        How to choose coordinates for SGD ('random', 'gradient_based', or 'cyclic')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Best parameters, corresponding score, and convergence curves\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays if they're not already\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Prepare parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(product(*param_values))\n",
    "    \n",
    "    # Split data into folds\n",
    "    n_samples = len(y)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.seed(42)  # For reproducible CV splits\n",
    "    np.random.shuffle(indices)\n",
    "    fold_sizes = np.full(cv, n_samples // cv, dtype=int)\n",
    "    fold_sizes[:n_samples % cv] += 1\n",
    "    \n",
    "    current_idx = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        fold_indices = indices[current_idx:current_idx + fold_size]\n",
    "        folds.append(fold_indices)\n",
    "        current_idx += fold_size\n",
    "        \n",
    "    # Define evaluation function for a single parameter combination\n",
    "    def evaluate_params(params):\n",
    "        param_dict = {param_names[i]: params[i] for i in range(len(param_names))}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Evaluating parameters: {param_dict}\")\n",
    "        \n",
    "        scores = []\n",
    "        convergence_curves = []\n",
    "        \n",
    "        for i in range(cv):\n",
    "            # Split data\n",
    "            test_idx = folds[i]\n",
    "            train_idx = np.concatenate([folds[j] for j in range(cv) if j != i])\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Train model with current parameters\n",
    "            model = LogisticRegression(**param_dict)\n",
    "            \n",
    "            # Use specified coordinate selection and loss type\n",
    "            # Store accuracy history from fit method\n",
    "            accuracy_history = model.fit(\n",
    "                X_train, y_train,\n",
    "                loss_type=loss_type,\n",
    "                coordinate_choice=coordinate_choice,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Store convergence curve for this fold\n",
    "            convergence_curves.append(accuracy_history)\n",
    "            \n",
    "            # Evaluate based on scoring metric\n",
    "            if scoring == 'accuracy':\n",
    "                score = model.score(X_test, y_test)\n",
    "            elif scoring == 'precision':\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "            elif scoring == 'recall':\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "            elif scoring == 'f1':\n",
    "                y_pred = model.predict(X_test)\n",
    "                precision = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "                recall = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "                score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            elif scoring == 'auc':\n",
    "                try:\n",
    "                    from sklearn.metrics import roc_auc_score\n",
    "                    y_prob = model.predict_proba(X_test)\n",
    "                    score = roc_auc_score(y_test == 2, y_prob)\n",
    "                except ImportError:\n",
    "                    # Fallback if sklearn is not available\n",
    "                    y_prob = model.predict_proba(X_test)\n",
    "                    # Manual AUC calculation (simplified)\n",
    "                    pos_scores = y_prob[y_test == 2]\n",
    "                    neg_scores = y_prob[y_test != 2]\n",
    "                    if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "                        score = 0.5\n",
    "                    else:\n",
    "                        n_pos = len(pos_scores)\n",
    "                        n_neg = len(neg_scores)\n",
    "                        score = sum(pos > neg for pos in pos_scores for neg in neg_scores) / (n_pos * n_neg)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown scoring metric: {scoring}\")\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        if verbose:\n",
    "            print(f\"Parameters {param_dict} - Mean {scoring}: {mean_score:.4f}\")\n",
    "            # Check if model converged in all folds\n",
    "            avg_iterations = np.mean([len(np.unique(curve)) for curve in convergence_curves])\n",
    "            print(f\"Average unique accuracy points: {avg_iterations:.1f}/100 (higher suggests better convergence)\")\n",
    "        \n",
    "        # Average convergence curve across folds\n",
    "        avg_convergence = np.mean(convergence_curves, axis=0)\n",
    "        \n",
    "        return param_dict, mean_score, avg_convergence\n",
    "    \n",
    "    # For small grid sizes or when parallelism causes issues, run sequentially\n",
    "    if len(param_combinations) <= 4 or n_jobs == 1:\n",
    "        results = [evaluate_params(params) for params in param_combinations]\n",
    "    else:\n",
    "        # Run evaluations in parallel\n",
    "        try:\n",
    "            n_jobs = n_jobs if n_jobs > 0 else multiprocessing.cpu_count()\n",
    "            results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(evaluate_params)(params) for params in param_combinations\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Parallel execution failed with error: {str(e)}\")\n",
    "                print(\"Falling back to sequential execution...\")\n",
    "            results = [evaluate_params(params) for params in param_combinations]\n",
    "    \n",
    "    # Find best parameters\n",
    "    best_params, best_score, best_convergence = max(results, key=lambda x: x[1])\n",
    "    \n",
    "    if verbose:\n",
    "        opt_method = f\"Coordinate descent with {coordinate_choice} selection\"\n",
    "        print(f\"\\nGrid Search Results (using {opt_method}):\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best {scoring}: {best_score:.4f}\")\n",
    "        \n",
    "        # Calculate iterations until convergence for best model\n",
    "        # Find point where accuracy stabilizes (change < 0.001)\n",
    "        diffs = np.abs(np.diff(best_convergence))\n",
    "        stable_idx = np.argmax(diffs < 0.001)\n",
    "        if stable_idx > 0:\n",
    "            print(f\"Best model converged after ~{stable_idx} iterations\")\n",
    "        else:\n",
    "            print(\"Best model may not have fully converged\")\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'best_convergence': best_convergence,\n",
    "        'all_results': [(params, score) for params, score, _ in results],\n",
    "        'convergence_curves': {str(params): curve for params, _, curve in results}\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:35:29.059411Z",
     "start_time": "2025-02-25T00:35:29.008748Z"
    }
   },
   "id": "39ea7861fad1eb60",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fullBreakdown(X, y, param_grid=None, cv=5, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Runs a comprehensive analysis of the LogisticRegression model:\n",
    "        1. Finds optimal hyperparameters using grid search\n",
    "        2. Compares baseline vs custom loss with random coordinate selection\n",
    "        3. Compares different coordinate selection methods with the custom loss\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        param_grid : dict, optional\n",
    "            Parameter grid for grid search (default provides a basic grid)\n",
    "        cv : int, default=5\n",
    "            Number of cross-validation folds\n",
    "        n_jobs : int, default=-1\n",
    "            Number of parallel jobs\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Results of the analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert inputs to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Default parameter grid if none provided\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'lr': [0.001, 0.01, 0.1],\n",
    "                'max_iter': [1000],\n",
    "                'reg_lambda': [0.001, 0.01, 0.1],\n",
    "                'fit_intercept': [True]\n",
    "            }\n",
    "        \n",
    "        print(\"Phase 1: Finding optimal hyperparameters...\")\n",
    "        grid_results = grid_search(\n",
    "            X_train, y_train, \n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            loss_type=\"baseline\",\n",
    "            coordinate_choice=\"random\"\n",
    "        )\n",
    "        \n",
    "        best_params = grid_results['best_params']\n",
    "        print(f\"Best parameters found: {best_params}\")\n",
    "        \n",
    "        # Fix max_iter to a larger value for convergence tracking\n",
    "        best_params['max_iter'] = 1000\n",
    "        \n",
    "        # Phase 2: Compare baseline vs custom loss\n",
    "        print(\"\\nPhase 2: Comparing baseline vs custom loss with random coordinate selection...\")\n",
    "        baseline_model = LogisticRegression(**best_params)\n",
    "        baseline_accuracies = baseline_model.fit(X_train, y_train, loss_type=\"baseline\", \n",
    "                                               coordinate_choice=\"random\", verbose=False)\n",
    "        \n",
    "        custom_model = LogisticRegression(**best_params)\n",
    "        custom_accuracies = custom_model.fit(X_train, y_train, loss_type=\"custom\", \n",
    "                                            coordinate_choice=\"random\", verbose=False)\n",
    "        \n",
    "        # Phase 3: Compare coordinate selection methods\n",
    "        print(\"\\nPhase 3: Comparing coordinate selection methods with custom loss...\")\n",
    "        random_model = custom_model  # Already trained above\n",
    "        random_accuracies = custom_accuracies  # Already computed\n",
    "        \n",
    "        gradient_model = LogisticRegression(**best_params)\n",
    "        gradient_accuracies = gradient_model.fit(X_train, y_train, loss_type=\"custom\", \n",
    "                                               coordinate_choice=\"gradient_based\", verbose=False)\n",
    "        \n",
    "        cyclic_model = LogisticRegression(**best_params)\n",
    "        cyclic_accuracies = cyclic_model.fit(X_train, y_train, loss_type=\"custom\", \n",
    "                                           coordinate_choice=\"cyclic\", verbose=False)\n",
    "        \n",
    "        # Evaluate all models on test set\n",
    "        baseline_test_acc = baseline_model.score(X_test, y_test)\n",
    "        custom_test_acc = custom_model.score(X_test, y_test)\n",
    "        gradient_test_acc = gradient_model.score(X_test, y_test)\n",
    "        cyclic_test_acc = cyclic_model.score(X_test, y_test)\n",
    "        \n",
    "        print(f\"\\nTest accuracies:\")\n",
    "        print(f\"Baseline loss with random coord: {baseline_test_acc:.4f}\")\n",
    "        print(f\"Custom loss with random coord: {custom_test_acc:.4f}\")\n",
    "        print(f\"Custom loss with gradient-based coord: {gradient_test_acc:.4f}\")\n",
    "        print(f\"Custom loss with cyclic coord: {cyclic_test_acc:.4f}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot 1: Loss comparison\n",
    "        plt.subplot(1, 2, 1)\n",
    "        iterations = np.arange(len(baseline_accuracies))\n",
    "        plt.plot(iterations, baseline_accuracies, label='Baseline Loss')\n",
    "        plt.plot(iterations, custom_accuracies, label='Custom Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Loss Function Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot 2: Coordinate selection comparison\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(iterations, random_accuracies, label='Random')\n",
    "        plt.plot(iterations, gradient_accuracies, label='Gradient-based')\n",
    "        plt.plot(iterations, cyclic_accuracies, label='Cyclic')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Coordinate Selection Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('logistic_regression_comparison.png')\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'grid_search_results': grid_results,\n",
    "            'test_accuracies': {\n",
    "                'baseline_random': baseline_test_acc,\n",
    "                'custom_random': custom_test_acc,\n",
    "                'custom_gradient': gradient_test_acc,\n",
    "                'custom_cyclic': cyclic_test_acc\n",
    "            },\n",
    "            'training_accuracies': {\n",
    "                'baseline_random': baseline_accuracies,\n",
    "                'custom_random': custom_accuracies,\n",
    "                'custom_gradient': gradient_accuracies,\n",
    "                'custom_cyclic': cyclic_accuracies\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:38:41.838711Z",
     "start_time": "2025-02-25T00:38:41.835051Z"
    }
   },
   "id": "d253c24d65992d4d",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution failed with error: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "Falling back to sequential execution...\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 21.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 500} - Mean accuracy: 0.6462\n",
      "Average unique accuracy points: 28.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.6538\n",
      "Average unique accuracy points: 20.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 21.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 500} - Mean accuracy: 0.6462\n",
      "Average unique accuracy points: 28.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.6538\n",
      "Average unique accuracy points: 20.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 21.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.6385\n",
      "Average unique accuracy points: 28.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.6538\n",
      "Average unique accuracy points: 20.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.8615\n",
      "Average unique accuracy points: 21.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.6385\n",
      "Average unique accuracy points: 29.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.6538\n",
      "Average unique accuracy points: 21.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 31.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 500} - Mean accuracy: 0.7769\n",
      "Average unique accuracy points: 22.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.7692\n",
      "Average unique accuracy points: 18.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 31.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 500} - Mean accuracy: 0.7769\n",
      "Average unique accuracy points: 22.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.7692\n",
      "Average unique accuracy points: 18.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 31.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.7769\n",
      "Average unique accuracy points: 22.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.7692\n",
      "Average unique accuracy points: 18.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.8692\n",
      "Average unique accuracy points: 32.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.7769\n",
      "Average unique accuracy points: 22.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.7692\n",
      "Average unique accuracy points: 18.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 100} - Mean accuracy: 0.5846\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 500} - Mean accuracy: 0.7462\n",
      "Average unique accuracy points: 28.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.7538\n",
      "Average unique accuracy points: 24.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 100} - Mean accuracy: 0.5846\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 500} - Mean accuracy: 0.7462\n",
      "Average unique accuracy points: 28.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.7538\n",
      "Average unique accuracy points: 24.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.5846\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.7462\n",
      "Average unique accuracy points: 28.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.7538\n",
      "Average unique accuracy points: 24.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.5846\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.7462\n",
      "Average unique accuracy points: 28.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.7538\n",
      "Average unique accuracy points: 24.4/100 (higher suggests better convergence)\n",
      "\n",
      "Grid Search Results (using Coordinate descent with random selection):\n",
      "Best parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Best accuracy: 0.8692\n",
      "Best model converged after ~18 iterations\n",
      "Did not converge after 100 iterations. Final accuracy: 0.8942\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0.46153846, 0.60576923, 0.53846154, 0.57692308, 0.69230769,\n       0.72115385, 0.73076923, 0.72115385, 0.76923077, 0.82692308,\n       0.85576923, 0.85576923, 0.86538462, 0.88461538, 0.88461538,\n       0.875     , 0.875     , 0.875     , 0.85576923, 0.86538462,\n       0.86538462, 0.86538462, 0.86538462, 0.88461538, 0.88461538,\n       0.88461538, 0.89423077, 0.91346154, 0.91346154, 0.91346154,\n       0.91346154, 0.90384615, 0.92307692, 0.92307692, 0.93269231,\n       0.93269231, 0.93269231, 0.93269231, 0.93269231, 0.93269231,\n       0.92307692, 0.94230769, 0.94230769, 0.89423077, 0.92307692,\n       0.93269231, 0.93269231, 0.90384615, 0.90384615, 0.93269231,\n       0.93269231, 0.91346154, 0.92307692, 0.91346154, 0.91346154,\n       0.92307692, 0.92307692, 0.92307692, 0.92307692, 0.92307692,\n       0.92307692, 0.92307692, 0.92307692, 0.93269231, 0.93269231,\n       0.94230769, 0.94230769, 0.93269231, 0.94230769, 0.96153846,\n       0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n       0.95192308, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n       0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n       0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n       0.89423077, 0.89423077, 0.88461538, 0.89423077, 0.89423077,\n       0.89423077, 0.89423077, 0.89423077, 0.89423077, 0.89423077])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'reg_lambda': [0.0, 0.01, 0.1, 1.0],\n",
    "    'max_iter': [100,500,1000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = grid_search(X, y, param_grid, verbose=True)\n",
    "\n",
    "# Create model with best parameters\n",
    "best_model = LogisticRegression(**results['best_params'])\n",
    "best_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:36:54.792848Z",
     "start_time": "2025-02-25T00:36:51.151047Z"
    }
   },
   "id": "188ffcd897f4717",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Finding optimal hyperparameters...\n",
      "Parallel execution failed with error: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "Falling back to sequential execution...\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100} - Mean accuracy: 0.8448\n",
      "Average unique accuracy points: 21.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 500} - Mean accuracy: 0.6905\n",
      "Average unique accuracy points: 20.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.6619\n",
      "Average unique accuracy points: 19.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 100} - Mean accuracy: 0.8448\n",
      "Average unique accuracy points: 21.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 500} - Mean accuracy: 0.6905\n",
      "Average unique accuracy points: 20.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.6619\n",
      "Average unique accuracy points: 19.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.8448\n",
      "Average unique accuracy points: 21.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.6905\n",
      "Average unique accuracy points: 21.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.6619\n",
      "Average unique accuracy points: 19.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.8448\n",
      "Average unique accuracy points: 20.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.6524\n",
      "Average unique accuracy points: 21.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.6619\n",
      "Average unique accuracy points: 19.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100} - Mean accuracy: 0.8938\n",
      "Average unique accuracy points: 28.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 500} - Mean accuracy: 0.8248\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.7867\n",
      "Average unique accuracy points: 15.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 100} - Mean accuracy: 0.8938\n",
      "Average unique accuracy points: 28.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 500} - Mean accuracy: 0.8248\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.7867\n",
      "Average unique accuracy points: 15.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.8938\n",
      "Average unique accuracy points: 28.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.8248\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.7867\n",
      "Average unique accuracy points: 15.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.8938\n",
      "Average unique accuracy points: 28.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.8248\n",
      "Average unique accuracy points: 17.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.7867\n",
      "Average unique accuracy points: 15.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 100} - Mean accuracy: 0.6243\n",
      "Average unique accuracy points: 14.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 500} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 23.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 19.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 100} - Mean accuracy: 0.6243\n",
      "Average unique accuracy points: 14.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 500} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 23.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 19.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.6243\n",
      "Average unique accuracy points: 14.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 23.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 19.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.6243\n",
      "Average unique accuracy points: 14.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 23.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.7871\n",
      "Average unique accuracy points: 19.6/100 (higher suggests better convergence)\n",
      "\n",
      "Grid Search Results (using Coordinate descent with random selection):\n",
      "Best parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "Best accuracy: 0.8938\n",
      "Best model converged after ~14 iterations\n",
      "Best parameters found: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100}\n",
      "\n",
      "Phase 2: Comparing baseline vs custom loss with random coordinate selection...\n",
      "\n",
      "Phase 3: Comparing coordinate selection methods with custom loss...\n",
      "\n",
      "Test accuracies:\n",
      "Baseline loss with random coord: 0.6923\n",
      "Custom loss with random coord: 0.9615\n",
      "Custom loss with gradient-based coord: 0.9615\n",
      "Custom loss with cyclic coord: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x500 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'grid_search_results': {'best_params': {'lr': 0.01,\n   'reg_lambda': 0.0,\n   'max_iter': 1000},\n  'best_score': np.float64(0.8938095238095236),\n  'best_convergence': array([0.48075158, 0.48313253, 0.43978772, 0.521572  , 0.55527826,\n         0.5624498 , 0.5551922 , 0.55757315, 0.6033276 , 0.62745267,\n         0.65390132, 0.65149168, 0.64670109, 0.66589214, 0.67555938,\n         0.67550201, 0.67791165, 0.68995984, 0.68273092, 0.69234079,\n         0.70909352, 0.73075158, 0.74515204, 0.74759036, 0.74279977,\n         0.75002869, 0.74520941, 0.76442915, 0.7788296 , 0.78846816,\n         0.78364888, 0.7908778 , 0.79090648, 0.82208835, 0.81967871,\n         0.82449799, 0.82211704, 0.83178428, 0.82455536, 0.81732645,\n         0.82214573, 0.81973609, 0.82937464, 0.82699369, 0.82461274,\n         0.84621343, 0.84856569, 0.83416523, 0.86061388, 0.87257602,\n         0.86540448, 0.86781411, 0.87263339, 0.88221457, 0.87498566,\n         0.88218589, 0.89420539, 0.89661503, 0.89420539, 0.89899598,\n         0.90140562, 0.90140562, 0.89899598, 0.89899598, 0.90140562,\n         0.90863454, 0.91345382, 0.91345382, 0.90866322, 0.91827309,\n         0.92550201, 0.92791165, 0.92547332, 0.9302926 , 0.9302926 ,\n         0.9302926 , 0.93270224, 0.93270224, 0.93752151, 0.93752151,\n         0.93752151, 0.94475043, 0.94234079, 0.94713138, 0.94716007,\n         0.9302926 , 0.9302926 , 0.93270224, 0.94475043, 0.94475043,\n         0.92306368, 0.92306368, 0.92547332, 0.92788296, 0.92788296,\n         0.92788296, 0.90625359, 0.90625359, 0.90625359, 0.90625359]),\n  'all_results': [({'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100},\n    np.float64(0.8447619047619048)),\n   ({'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 500},\n    np.float64(0.6904761904761905)),\n   ({'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000},\n    np.float64(0.6619047619047619)),\n   ({'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 100},\n    np.float64(0.8447619047619048)),\n   ({'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 500},\n    np.float64(0.6904761904761905)),\n   ({'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000},\n    np.float64(0.6619047619047619)),\n   ({'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 100},\n    np.float64(0.8447619047619048)),\n   ({'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 500},\n    np.float64(0.6904761904761905)),\n   ({'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000},\n    np.float64(0.6619047619047619)),\n   ({'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 100},\n    np.float64(0.8447619047619048)),\n   ({'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 500},\n    np.float64(0.6523809523809524)),\n   ({'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000},\n    np.float64(0.6619047619047619)),\n   ({'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000},\n    np.float64(0.8938095238095236)),\n   ({'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 500},\n    np.float64(0.8247619047619047)),\n   ({'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000},\n    np.float64(0.7866666666666666)),\n   ({'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 100},\n    np.float64(0.8938095238095236)),\n   ({'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 500},\n    np.float64(0.8247619047619047)),\n   ({'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000},\n    np.float64(0.7866666666666666)),\n   ({'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100},\n    np.float64(0.8938095238095236)),\n   ({'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500},\n    np.float64(0.8247619047619047)),\n   ({'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000},\n    np.float64(0.7866666666666666)),\n   ({'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100},\n    np.float64(0.8938095238095236)),\n   ({'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500},\n    np.float64(0.8247619047619047)),\n   ({'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000},\n    np.float64(0.7866666666666666)),\n   ({'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 100},\n    np.float64(0.6242857142857142)),\n   ({'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 500},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 100},\n    np.float64(0.6242857142857142)),\n   ({'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 500},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100},\n    np.float64(0.6242857142857142)),\n   ({'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100},\n    np.float64(0.6242857142857142)),\n   ({'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500},\n    np.float64(0.7871428571428571)),\n   ({'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000},\n    np.float64(0.7871428571428571))],\n  'convergence_curves': {\"{'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 100}\": array([0.50720023, 0.52897303, 0.55760184, 0.57682157, 0.63686173,\n          0.67283419, 0.67524383, 0.66081469, 0.72099828, 0.75479059,\n          0.79566839, 0.79566839, 0.80045898, 0.80768789, 0.80768789,\n          0.81726908, 0.81965003, 0.82449799, 0.82687894, 0.83410786,\n          0.84130809, 0.85573723, 0.85573723, 0.85097533, 0.85820425,\n          0.86781411, 0.87022375, 0.88462421, 0.89664372, 0.89182444,\n          0.88703385, 0.88703385, 0.88221457, 0.89661503, 0.89423408,\n          0.89182444, 0.8894148 , 0.88938612, 0.89179575, 0.89179575,\n          0.88938612, 0.8894148 , 0.89664372, 0.86299484, 0.86778543,\n          0.87501434, 0.87501434, 0.8773953 , 0.8894148 , 0.89179575,\n          0.89661503, 0.89658635, 0.89658635, 0.89899598, 0.89899598,\n          0.90140562, 0.91104418, 0.91104418, 0.90863454, 0.91101549,\n          0.91101549, 0.90863454, 0.91101549, 0.91583477, 0.91586345,\n          0.92065404, 0.92547332, 0.92785427, 0.93026391, 0.93026391,\n          0.93267355, 0.93026391, 0.93023523, 0.93264487, 0.9350545 ,\n          0.9350545 , 0.9350545 , 0.9350545 , 0.93746414, 0.9350545 ,\n          0.93508319, 0.93508319, 0.93990247, 0.94228342, 0.94713138,\n          0.90863454, 0.90863454, 0.91104418, 0.89902467, 0.90143431,\n          0.89420539, 0.89420539, 0.89661503, 0.89661503, 0.89661503,\n          0.89661503, 0.88218589, 0.88459552, 0.88218589, 0.88218589]),\n   \"{'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 500}\": array([0.48795181, 0.70433161, 0.84371773, 0.8894148 , 0.89434882,\n          0.90874928, 0.92320711, 0.92802639, 0.93281698, 0.89919679,\n          0.92076879, 0.9207401 , 0.91835915, 0.93998853, 0.93998853,\n          0.9543603 , 0.95676994, 0.93514056, 0.90393001, 0.90633964,\n          0.87022375, 0.87022375, 0.86781411, 0.86061388, 0.86540448,\n          0.84624211, 0.83663224, 0.83425129, 0.8390132 , 0.82943201,\n          0.83181297, 0.8342226 , 0.84386116, 0.84868044, 0.85344234,\n          0.85344234, 0.85344234, 0.83184165, 0.83184165, 0.83666093,\n          0.83187034, 0.83427998, 0.82948939, 0.81508893, 0.80309811,\n          0.78141136, 0.7670109 , 0.7381813 , 0.75022949, 0.75983936,\n          0.74781985, 0.75504877, 0.73577166, 0.73577166, 0.73574297,\n          0.74291452, 0.73336202, 0.7261331 , 0.74526678, 0.74767642,\n          0.72607573, 0.73089501, 0.73330465, 0.72848537, 0.71890419,\n          0.72128514, 0.72369478, 0.73333333, 0.74053356, 0.72372347,\n          0.72851406, 0.71649455, 0.71649455, 0.71649455, 0.72369478,\n          0.7141136 , 0.71649455, 0.69724613, 0.706856  , 0.70926563,\n          0.7020654 , 0.69724613, 0.69483649, 0.69483649, 0.69962708,\n          0.69962708, 0.69483649, 0.69483649, 0.70203672, 0.69965577,\n          0.7020654 , 0.69965577, 0.70444636, 0.70923695, 0.69965577,\n          0.69965577, 0.69965577, 0.69965577, 0.69965577, 0.70203672]),\n   \"{'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000}\": array([0.45430293, 0.89193919, 0.86778543, 0.90628227, 0.93032129,\n          0.93752151, 0.94234079, 0.95676994, 0.9543603 , 0.93511188,\n          0.8773953 , 0.88218589, 0.87251865, 0.84139415, 0.84136546,\n          0.83657487, 0.84618474, 0.85582329, 0.86061388, 0.8390132 ,\n          0.82937464, 0.81012622, 0.79340218, 0.75496271, 0.73333333,\n          0.71646586, 0.71164659, 0.7188755 , 0.71646586, 0.71643718,\n          0.7020654 , 0.70203672, 0.68037866, 0.70200803, 0.71158921,\n          0.68275961, 0.68037866, 0.69242685, 0.68037866, 0.67555938,\n          0.67558807, 0.66594951, 0.66594951, 0.66353987, 0.66594951,\n          0.66353987, 0.67076879, 0.67076879, 0.67076879, 0.68040734,\n          0.68040734, 0.68281698, 0.67317843, 0.67317843, 0.67799771,\n          0.68281698, 0.68040734, 0.68281698, 0.67799771, 0.67076879,\n          0.66353987, 0.67076879, 0.67076879, 0.67317843, 0.67317843,\n          0.67317843, 0.66594951, 0.66594951, 0.66594951, 0.66594951,\n          0.66835915, 0.66594951, 0.67312106, 0.68275961, 0.68275961,\n          0.67314974, 0.68040734, 0.69236948, 0.67799771, 0.67558807,\n          0.68040734, 0.67076879, 0.67317843, 0.67558807, 0.67794033,\n          0.67555938, 0.67794033, 0.67794033, 0.67555938, 0.67555938,\n          0.67314974, 0.67076879, 0.66833046, 0.66594951, 0.66594951,\n          0.67794033, 0.67553069, 0.67553069, 0.67312106, 0.68032129]),\n   \"{'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 100}\": array([0.50720023, 0.52897303, 0.55760184, 0.57682157, 0.63686173,\n          0.67283419, 0.67524383, 0.66081469, 0.72099828, 0.75479059,\n          0.79566839, 0.79566839, 0.80045898, 0.80768789, 0.80768789,\n          0.81726908, 0.81965003, 0.82449799, 0.82687894, 0.83410786,\n          0.84130809, 0.85573723, 0.85573723, 0.85097533, 0.85820425,\n          0.86781411, 0.87022375, 0.88462421, 0.89664372, 0.89182444,\n          0.88703385, 0.88703385, 0.88221457, 0.89661503, 0.89423408,\n          0.89182444, 0.8894148 , 0.88938612, 0.89179575, 0.89179575,\n          0.88938612, 0.8894148 , 0.89664372, 0.86299484, 0.86778543,\n          0.87501434, 0.87501434, 0.8773953 , 0.8894148 , 0.89179575,\n          0.89661503, 0.89658635, 0.89658635, 0.89899598, 0.89899598,\n          0.90140562, 0.91104418, 0.91104418, 0.90863454, 0.91101549,\n          0.91101549, 0.90863454, 0.91101549, 0.91583477, 0.91586345,\n          0.92065404, 0.92547332, 0.92785427, 0.93026391, 0.93026391,\n          0.93267355, 0.93026391, 0.93023523, 0.93264487, 0.9350545 ,\n          0.9350545 , 0.9350545 , 0.9350545 , 0.93746414, 0.9350545 ,\n          0.93508319, 0.93508319, 0.93990247, 0.94228342, 0.94713138,\n          0.90863454, 0.90863454, 0.91104418, 0.89902467, 0.90143431,\n          0.89420539, 0.89420539, 0.89661503, 0.89661503, 0.89661503,\n          0.89661503, 0.88218589, 0.88459552, 0.88218589, 0.88218589]),\n   \"{'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 500}\": array([0.48795181, 0.70433161, 0.84371773, 0.8894148 , 0.89434882,\n          0.90874928, 0.92320711, 0.92802639, 0.93281698, 0.89919679,\n          0.92076879, 0.9207401 , 0.91835915, 0.93998853, 0.93998853,\n          0.9543603 , 0.95676994, 0.93514056, 0.90393001, 0.90633964,\n          0.87022375, 0.87022375, 0.86781411, 0.86061388, 0.86540448,\n          0.84624211, 0.83663224, 0.83425129, 0.8390132 , 0.82943201,\n          0.83181297, 0.8342226 , 0.84386116, 0.84868044, 0.85344234,\n          0.85344234, 0.85582329, 0.83184165, 0.83184165, 0.83666093,\n          0.83187034, 0.83427998, 0.82948939, 0.81508893, 0.80309811,\n          0.78141136, 0.7670109 , 0.7381813 , 0.75022949, 0.75983936,\n          0.74781985, 0.75504877, 0.73577166, 0.73577166, 0.73574297,\n          0.74291452, 0.73336202, 0.7261331 , 0.74526678, 0.74767642,\n          0.72607573, 0.73089501, 0.73330465, 0.72848537, 0.71890419,\n          0.72128514, 0.72369478, 0.73333333, 0.74053356, 0.72372347,\n          0.72851406, 0.71649455, 0.71649455, 0.71649455, 0.72369478,\n          0.7141136 , 0.71649455, 0.69724613, 0.706856  , 0.70926563,\n          0.7020654 , 0.69724613, 0.69483649, 0.69483649, 0.69962708,\n          0.69962708, 0.69483649, 0.69483649, 0.70203672, 0.69965577,\n          0.7020654 , 0.69965577, 0.70444636, 0.70923695, 0.69965577,\n          0.69965577, 0.69965577, 0.69965577, 0.69965577, 0.70203672]),\n   \"{'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000}\": array([0.45430293, 0.89193919, 0.86778543, 0.90628227, 0.93032129,\n          0.93752151, 0.94234079, 0.95676994, 0.9543603 , 0.93511188,\n          0.8773953 , 0.88218589, 0.87251865, 0.84139415, 0.84136546,\n          0.83657487, 0.84618474, 0.85582329, 0.86061388, 0.8390132 ,\n          0.82937464, 0.81012622, 0.79340218, 0.75496271, 0.73333333,\n          0.71646586, 0.71164659, 0.7188755 , 0.71646586, 0.71643718,\n          0.7020654 , 0.70203672, 0.68037866, 0.69959839, 0.71158921,\n          0.68275961, 0.68037866, 0.69242685, 0.68037866, 0.67555938,\n          0.67558807, 0.66594951, 0.66594951, 0.66353987, 0.66594951,\n          0.66353987, 0.67076879, 0.67076879, 0.67076879, 0.68040734,\n          0.68040734, 0.68040734, 0.67317843, 0.67317843, 0.67799771,\n          0.68281698, 0.68040734, 0.68281698, 0.67799771, 0.67076879,\n          0.66353987, 0.67076879, 0.67076879, 0.67317843, 0.67317843,\n          0.67317843, 0.66594951, 0.66594951, 0.66594951, 0.66594951,\n          0.66835915, 0.66594951, 0.67312106, 0.68275961, 0.68275961,\n          0.67314974, 0.68040734, 0.69236948, 0.67799771, 0.67558807,\n          0.68040734, 0.67076879, 0.67317843, 0.67558807, 0.67794033,\n          0.67555938, 0.67794033, 0.67794033, 0.67555938, 0.67555938,\n          0.67314974, 0.67076879, 0.66833046, 0.66594951, 0.66594951,\n          0.67794033, 0.67314974, 0.67553069, 0.67312106, 0.68032129]),\n   \"{'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 100}\": array([0.50720023, 0.52897303, 0.55760184, 0.57682157, 0.63686173,\n          0.67283419, 0.67524383, 0.66081469, 0.72099828, 0.75479059,\n          0.79566839, 0.79566839, 0.80045898, 0.80768789, 0.80768789,\n          0.81726908, 0.81965003, 0.82449799, 0.82687894, 0.83410786,\n          0.84130809, 0.85573723, 0.85573723, 0.85097533, 0.85820425,\n          0.86781411, 0.87022375, 0.88462421, 0.89664372, 0.89182444,\n          0.88703385, 0.88703385, 0.88221457, 0.89661503, 0.89423408,\n          0.89182444, 0.8894148 , 0.88938612, 0.89179575, 0.89179575,\n          0.88938612, 0.8894148 , 0.89664372, 0.86299484, 0.86778543,\n          0.87501434, 0.87501434, 0.8773953 , 0.8894148 , 0.89179575,\n          0.89661503, 0.89658635, 0.89658635, 0.89899598, 0.89899598,\n          0.90140562, 0.91104418, 0.91104418, 0.90863454, 0.91101549,\n          0.91101549, 0.90863454, 0.91101549, 0.91583477, 0.91586345,\n          0.92065404, 0.92547332, 0.92785427, 0.93026391, 0.93026391,\n          0.93267355, 0.93026391, 0.93023523, 0.93264487, 0.9350545 ,\n          0.9350545 , 0.9350545 , 0.9350545 , 0.93746414, 0.9350545 ,\n          0.93508319, 0.93508319, 0.93990247, 0.94228342, 0.94713138,\n          0.90863454, 0.90863454, 0.91104418, 0.89902467, 0.90143431,\n          0.89420539, 0.89420539, 0.89661503, 0.89661503, 0.89661503,\n          0.89661503, 0.88218589, 0.88218589, 0.88218589, 0.88218589]),\n   \"{'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 500}\": array([0.48795181, 0.70433161, 0.84371773, 0.8894148 , 0.89434882,\n          0.90874928, 0.92320711, 0.92802639, 0.93281698, 0.89919679,\n          0.92076879, 0.9207401 , 0.91835915, 0.93998853, 0.93998853,\n          0.9543603 , 0.95676994, 0.93514056, 0.90393001, 0.90633964,\n          0.87022375, 0.87022375, 0.86781411, 0.86061388, 0.86540448,\n          0.84386116, 0.83663224, 0.83425129, 0.8390132 , 0.82943201,\n          0.82943201, 0.83184165, 0.84386116, 0.84868044, 0.85344234,\n          0.85344234, 0.85582329, 0.83184165, 0.83184165, 0.83666093,\n          0.83187034, 0.83427998, 0.82707975, 0.81270797, 0.80309811,\n          0.77662077, 0.7670109 , 0.7381813 , 0.74781985, 0.75983936,\n          0.74541021, 0.75263913, 0.73577166, 0.73577166, 0.73333333,\n          0.74291452, 0.73095238, 0.7261331 , 0.74285714, 0.74767642,\n          0.72607573, 0.73089501, 0.73089501, 0.72607573, 0.71890419,\n          0.72128514, 0.72369478, 0.73333333, 0.74053356, 0.72372347,\n          0.72610442, 0.71649455, 0.71649455, 0.71649455, 0.72369478,\n          0.7141136 , 0.71170396, 0.69724613, 0.7020654 , 0.706856  ,\n          0.69965577, 0.69724613, 0.69242685, 0.69483649, 0.69962708,\n          0.69724613, 0.69483649, 0.69483649, 0.70203672, 0.69965577,\n          0.69965577, 0.69965577, 0.70444636, 0.70682731, 0.69965577,\n          0.69965577, 0.69483649, 0.69724613, 0.69965577, 0.70203672]),\n   \"{'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000}\": array([0.45430293, 0.89193919, 0.86778543, 0.90628227, 0.93032129,\n          0.93752151, 0.94234079, 0.95676994, 0.9543603 , 0.93511188,\n          0.87498566, 0.88218589, 0.87010901, 0.84139415, 0.84136546,\n          0.83419392, 0.84618474, 0.85582329, 0.86061388, 0.8390132 ,\n          0.826965  , 0.81012622, 0.79340218, 0.75258176, 0.73333333,\n          0.71646586, 0.71164659, 0.7188755 , 0.71646586, 0.71643718,\n          0.69965577, 0.69962708, 0.68037866, 0.6948078 , 0.71158921,\n          0.68275961, 0.68037866, 0.69001721, 0.68037866, 0.67555938,\n          0.67558807, 0.66594951, 0.66594951, 0.66353987, 0.66594951,\n          0.66353987, 0.67076879, 0.67076879, 0.67076879, 0.68040734,\n          0.67799771, 0.68040734, 0.67317843, 0.67317843, 0.67799771,\n          0.68281698, 0.68040734, 0.68281698, 0.67558807, 0.67076879,\n          0.66353987, 0.66835915, 0.66835915, 0.67317843, 0.67317843,\n          0.67076879, 0.66594951, 0.66594951, 0.66594951, 0.66594951,\n          0.66835915, 0.66594951, 0.66594951, 0.68275961, 0.67794033,\n          0.66835915, 0.67799771, 0.69236948, 0.67558807, 0.67076879,\n          0.67558807, 0.67076879, 0.67317843, 0.67317843, 0.67317843,\n          0.67555938, 0.67794033, 0.67555938, 0.67555938, 0.67314974,\n          0.67314974, 0.66835915, 0.66594951, 0.66594951, 0.66594951,\n          0.67314974, 0.67314974, 0.67314974, 0.67071142, 0.67553069]),\n   \"{'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 100}\": array([0.50720023, 0.52897303, 0.55760184, 0.57682157, 0.63686173,\n          0.67042456, 0.67524383, 0.66081469, 0.72099828, 0.75238095,\n          0.79566839, 0.79566839, 0.80045898, 0.80768789, 0.80768789,\n          0.81726908, 0.81483075, 0.82687894, 0.82687894, 0.83410786,\n          0.84130809, 0.8533276 , 0.85573723, 0.85097533, 0.85820425,\n          0.86781411, 0.86781411, 0.88462421, 0.89423408, 0.89182444,\n          0.88703385, 0.88703385, 0.88221457, 0.89179575, 0.89423408,\n          0.89182444, 0.8894148 , 0.88697648, 0.89179575, 0.89179575,\n          0.88938612, 0.8894148 , 0.89664372, 0.86299484, 0.86537579,\n          0.87501434, 0.87501434, 0.8773953 , 0.8894148 , 0.89179575,\n          0.89661503, 0.89658635, 0.89658635, 0.89899598, 0.89899598,\n          0.90140562, 0.90863454, 0.91104418, 0.90863454, 0.91101549,\n          0.91101549, 0.90863454, 0.91101549, 0.91583477, 0.91586345,\n          0.92065404, 0.92547332, 0.92785427, 0.93026391, 0.93026391,\n          0.93267355, 0.93026391, 0.93023523, 0.93023523, 0.9350545 ,\n          0.9350545 , 0.9350545 , 0.9350545 , 0.9350545 , 0.9350545 ,\n          0.93508319, 0.93508319, 0.93990247, 0.94228342, 0.94713138,\n          0.90863454, 0.90863454, 0.91104418, 0.89902467, 0.89902467,\n          0.89420539, 0.89420539, 0.89661503, 0.89661503, 0.89661503,\n          0.89661503, 0.87980493, 0.88218589, 0.88218589, 0.88218589]),\n   \"{'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 500}\": array([0.48795181, 0.70433161, 0.84371773, 0.8894148 , 0.89193919,\n          0.90874928, 0.92320711, 0.92802639, 0.93281698, 0.90160643,\n          0.92076879, 0.9207401 , 0.91835915, 0.93757889, 0.93998853,\n          0.9543603 , 0.95676994, 0.93514056, 0.90393001, 0.90633964,\n          0.86781411, 0.87022375, 0.86781411, 0.86061388, 0.86302352,\n          0.84386116, 0.8342226 , 0.83425129, 0.8390132 , 0.82702238,\n          0.82943201, 0.82702238, 0.84386116, 0.84868044, 0.85341365,\n          0.85582329, 0.85582329, 0.82943201, 0.83184165, 0.83666093,\n          0.83187034, 0.83427998, 0.82231784, 0.81032702, 0.80068847,\n          0.76942054, 0.75745841, 0.73336202, 0.74541021, 0.75022949,\n          0.74300057, 0.74300057, 0.73095238, 0.73095238, 0.73333333,\n          0.73092369, 0.72854274, 0.71890419, 0.73330465, 0.74285714,\n          0.71890419, 0.72607573, 0.72607573, 0.71890419, 0.71408491,\n          0.71890419, 0.71890419, 0.7261331 , 0.73092369, 0.71649455,\n          0.71649455, 0.70929432, 0.71170396, 0.70929432, 0.7141136 ,\n          0.7020654 , 0.70688468, 0.69242685, 0.69724613, 0.69724613,\n          0.69483649, 0.69001721, 0.68037866, 0.6827883 , 0.69483649,\n          0.6827883 , 0.6827883 , 0.68037866, 0.69001721, 0.68760757,\n          0.6827883 , 0.68760757, 0.69242685, 0.69483649, 0.69001721,\n          0.69242685, 0.68519793, 0.68519793, 0.69001721, 0.69242685]),\n   \"{'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000}\": array([0.45430293, 0.89193919, 0.86778543, 0.90628227, 0.93032129,\n          0.93752151, 0.94234079, 0.95676994, 0.95195066, 0.93270224,\n          0.87498566, 0.88218589, 0.87010901, 0.83898451, 0.84136546,\n          0.82937464, 0.8437751 , 0.84862306, 0.85820425, 0.83660356,\n          0.82217441, 0.80533563, 0.78623064, 0.75258176, 0.72851406,\n          0.71405622, 0.70682731, 0.71164659, 0.71164659, 0.70441767,\n          0.68519793, 0.6827883 , 0.67314974, 0.68516925, 0.70197935,\n          0.67555938, 0.67314974, 0.6827883 , 0.67076879, 0.66353987,\n          0.66594951, 0.66113024, 0.66594951, 0.6587206 , 0.6587206 ,\n          0.6587206 , 0.66353987, 0.66835915, 0.66835915, 0.67076879,\n          0.67076879, 0.67076879, 0.66835915, 0.66353987, 0.66353987,\n          0.67076879, 0.67076879, 0.67317843, 0.66594951, 0.66353987,\n          0.66113024, 0.66353987, 0.66353987, 0.66353987, 0.66594951,\n          0.66353987, 0.6587206 , 0.65631096, 0.65149168, 0.65631096,\n          0.6587206 , 0.66353987, 0.66113024, 0.66353987, 0.66113024,\n          0.66113024, 0.66113024, 0.66835915, 0.66353987, 0.66353987,\n          0.66353987, 0.6587206 , 0.66594951, 0.66594951, 0.66353987,\n          0.66113024, 0.66594951, 0.66113024, 0.66113024, 0.66113024,\n          0.66113024, 0.66113024, 0.66113024, 0.66113024, 0.66113024,\n          0.66113024, 0.66113024, 0.66113024, 0.66113024, 0.66113024]),\n   \"{'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 100}\": array([0.48075158, 0.48313253, 0.43978772, 0.521572  , 0.55527826,\n          0.5624498 , 0.5551922 , 0.55757315, 0.6033276 , 0.62745267,\n          0.65390132, 0.65149168, 0.64670109, 0.66589214, 0.67555938,\n          0.67550201, 0.67791165, 0.68995984, 0.68273092, 0.69234079,\n          0.70909352, 0.73075158, 0.74515204, 0.74759036, 0.74279977,\n          0.75002869, 0.74520941, 0.76442915, 0.7788296 , 0.78846816,\n          0.78364888, 0.7908778 , 0.79090648, 0.82208835, 0.81967871,\n          0.82449799, 0.82211704, 0.83178428, 0.82455536, 0.81732645,\n          0.82214573, 0.81973609, 0.82937464, 0.82699369, 0.82461274,\n          0.84621343, 0.84856569, 0.83416523, 0.86061388, 0.87257602,\n          0.86540448, 0.86781411, 0.87263339, 0.88221457, 0.87498566,\n          0.88218589, 0.89420539, 0.89661503, 0.89420539, 0.89899598,\n          0.90140562, 0.90140562, 0.89899598, 0.89899598, 0.90140562,\n          0.90863454, 0.91345382, 0.91345382, 0.90866322, 0.91827309,\n          0.92550201, 0.92791165, 0.92547332, 0.9302926 , 0.9302926 ,\n          0.9302926 , 0.93270224, 0.93270224, 0.93752151, 0.93752151,\n          0.93752151, 0.94475043, 0.94234079, 0.94713138, 0.94716007,\n          0.9302926 , 0.9302926 , 0.93270224, 0.94475043, 0.94475043,\n          0.92306368, 0.92306368, 0.92547332, 0.92788296, 0.92788296,\n          0.92788296, 0.90625359, 0.90625359, 0.90625359, 0.90625359]),\n   \"{'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 500}\": array([0.48316122, 0.59368904, 0.68981641, 0.72831325, 0.74039013,\n          0.79331612, 0.81738382, 0.82458405, 0.85579461, 0.85100402,\n          0.87745267, 0.89672978, 0.91830178, 0.93514056, 0.93752151,\n          0.94472174, 0.94713138, 0.95195066, 0.94236948, 0.94713138,\n          0.88950086, 0.89432014, 0.89911073, 0.88227194, 0.89185313,\n          0.87025244, 0.86543316, 0.87025244, 0.87266208, 0.86302352,\n          0.86543316, 0.87504303, 0.87266208, 0.88944349, 0.89182444,\n          0.89664372, 0.89423408, 0.86543316, 0.87980493, 0.8846529 ,\n          0.87507172, 0.88224326, 0.87504303, 0.87263339, 0.86543316,\n          0.85823293, 0.85823293, 0.8390132 , 0.84624211, 0.8510327 ,\n          0.84383247, 0.84865175, 0.84142283, 0.84624211, 0.84624211,\n          0.84380379, 0.84142283, 0.8390132 , 0.84618474, 0.84856569,\n          0.8390132 , 0.84139415, 0.84380379, 0.8390132 , 0.8342226 ,\n          0.8390132 , 0.83660356, 0.84383247, 0.84862306, 0.83904188,\n          0.84145152, 0.82943201, 0.82943201, 0.83184165, 0.83425129,\n          0.82705106, 0.82705106, 0.82705106, 0.82943201, 0.82943201,\n          0.82943201, 0.82467011, 0.82226047, 0.82467011, 0.82707975,\n          0.82467011, 0.82226047, 0.82226047, 0.82948939, 0.82948939,\n          0.82707975, 0.82707975, 0.83187034, 0.83187034, 0.82707975,\n          0.8294607 , 0.82707975, 0.82707975, 0.82707975, 0.83187034]),\n   \"{'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000}\": array([0.47834194, 0.73080895, 0.73075158, 0.78602983, 0.8365175 ,\n          0.88700516, 0.89423408, 0.9302926 , 0.93752151, 0.93273092,\n          0.90625359, 0.91110155, 0.90866322, 0.89179575, 0.89420539,\n          0.88938612, 0.89179575, 0.90143431, 0.90384395, 0.8894148 ,\n          0.88700516, 0.88459552, 0.87498566, 0.8605852 , 0.85097533,\n          0.84615605, 0.84615605, 0.84856569, 0.84856569, 0.84615605,\n          0.84136546, 0.83895582, 0.82455536, 0.83416523, 0.84615605,\n          0.83172691, 0.82693632, 0.83657487, 0.82214573, 0.81250717,\n          0.8149455 , 0.79093517, 0.80292599, 0.79093517, 0.79331612,\n          0.79331612, 0.80536431, 0.79816408, 0.79575445, 0.81018359,\n          0.80536431, 0.81259323, 0.805393  , 0.80780264, 0.81021228,\n          0.81021228, 0.81259323, 0.81259323, 0.80298336, 0.79816408,\n          0.79334481, 0.80057372, 0.80057372, 0.80298336, 0.80780264,\n          0.80057372, 0.79575445, 0.79337349, 0.78620195, 0.78620195,\n          0.79096386, 0.80298336, 0.79575445, 0.80298336, 0.80780264,\n          0.80060241, 0.805393  , 0.80780264, 0.79822146, 0.79340218,\n          0.79822146, 0.79340218, 0.79822146, 0.79822146, 0.79822146,\n          0.79822146, 0.80301205, 0.79822146, 0.79581182, 0.79340218,\n          0.78861159, 0.78620195, 0.78620195, 0.78620195, 0.78620195,\n          0.79340218, 0.79340218, 0.79340218, 0.78620195, 0.79099254]),\n   \"{'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 100}\": array([0.48075158, 0.48313253, 0.43978772, 0.521572  , 0.55527826,\n          0.5624498 , 0.5551922 , 0.55757315, 0.6033276 , 0.62745267,\n          0.65390132, 0.65149168, 0.64670109, 0.66589214, 0.67555938,\n          0.67550201, 0.67791165, 0.68995984, 0.68273092, 0.69234079,\n          0.70909352, 0.73075158, 0.74515204, 0.74759036, 0.74279977,\n          0.75002869, 0.74520941, 0.76442915, 0.7788296 , 0.78846816,\n          0.78364888, 0.7908778 , 0.79090648, 0.82208835, 0.81967871,\n          0.82449799, 0.82211704, 0.83178428, 0.82455536, 0.81732645,\n          0.82214573, 0.81973609, 0.82937464, 0.82699369, 0.82461274,\n          0.84621343, 0.84856569, 0.83416523, 0.86061388, 0.87257602,\n          0.86540448, 0.86781411, 0.87263339, 0.88221457, 0.87498566,\n          0.88218589, 0.89420539, 0.89661503, 0.89420539, 0.89899598,\n          0.90140562, 0.90140562, 0.89899598, 0.89899598, 0.90140562,\n          0.90863454, 0.91345382, 0.91345382, 0.90866322, 0.91827309,\n          0.92550201, 0.92791165, 0.92547332, 0.9302926 , 0.9302926 ,\n          0.9302926 , 0.93270224, 0.93270224, 0.93752151, 0.93752151,\n          0.93752151, 0.94475043, 0.94234079, 0.94713138, 0.94716007,\n          0.9302926 , 0.9302926 , 0.93270224, 0.94475043, 0.94475043,\n          0.92306368, 0.92306368, 0.92547332, 0.92788296, 0.92788296,\n          0.92788296, 0.90625359, 0.90625359, 0.90625359, 0.90625359]),\n   \"{'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 500}\": array([0.48316122, 0.59368904, 0.68981641, 0.72831325, 0.74039013,\n          0.79331612, 0.81738382, 0.82458405, 0.85579461, 0.85100402,\n          0.87745267, 0.89672978, 0.91830178, 0.93514056, 0.93752151,\n          0.94472174, 0.94713138, 0.95195066, 0.94236948, 0.94713138,\n          0.88950086, 0.89432014, 0.89911073, 0.88227194, 0.89185313,\n          0.87025244, 0.86543316, 0.87025244, 0.87266208, 0.86302352,\n          0.86543316, 0.87504303, 0.87266208, 0.88944349, 0.89182444,\n          0.89664372, 0.89423408, 0.86543316, 0.87980493, 0.8846529 ,\n          0.87507172, 0.88224326, 0.87504303, 0.87263339, 0.86543316,\n          0.85823293, 0.85823293, 0.8390132 , 0.84624211, 0.8510327 ,\n          0.84383247, 0.84865175, 0.84142283, 0.84624211, 0.84624211,\n          0.84380379, 0.84142283, 0.8390132 , 0.84618474, 0.84856569,\n          0.8390132 , 0.84139415, 0.84380379, 0.8390132 , 0.8342226 ,\n          0.8390132 , 0.83660356, 0.84383247, 0.84862306, 0.83904188,\n          0.84145152, 0.82943201, 0.82943201, 0.83184165, 0.83425129,\n          0.82705106, 0.82705106, 0.82705106, 0.82943201, 0.82943201,\n          0.82943201, 0.82467011, 0.82226047, 0.82467011, 0.82707975,\n          0.82467011, 0.82226047, 0.82226047, 0.82948939, 0.82948939,\n          0.82707975, 0.82707975, 0.83187034, 0.83187034, 0.82707975,\n          0.8294607 , 0.82707975, 0.82707975, 0.82707975, 0.83187034]),\n   \"{'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000}\": array([0.47834194, 0.73080895, 0.73075158, 0.78602983, 0.8365175 ,\n          0.88700516, 0.89423408, 0.9302926 , 0.93752151, 0.93273092,\n          0.90625359, 0.91110155, 0.90866322, 0.89179575, 0.89420539,\n          0.88938612, 0.89179575, 0.90143431, 0.90384395, 0.8894148 ,\n          0.88700516, 0.88459552, 0.87498566, 0.8605852 , 0.85097533,\n          0.84615605, 0.84615605, 0.84856569, 0.84856569, 0.84615605,\n          0.84136546, 0.83895582, 0.82455536, 0.83416523, 0.84615605,\n          0.83172691, 0.82693632, 0.83657487, 0.82214573, 0.81250717,\n          0.8149455 , 0.79093517, 0.80292599, 0.79093517, 0.79331612,\n          0.79331612, 0.80536431, 0.79816408, 0.79575445, 0.81018359,\n          0.80536431, 0.81259323, 0.805393  , 0.80780264, 0.81021228,\n          0.81021228, 0.81259323, 0.81259323, 0.80298336, 0.79816408,\n          0.79334481, 0.80057372, 0.80057372, 0.80298336, 0.80780264,\n          0.80057372, 0.79575445, 0.79337349, 0.78620195, 0.78620195,\n          0.79096386, 0.80298336, 0.79575445, 0.80298336, 0.80780264,\n          0.80060241, 0.805393  , 0.80780264, 0.79822146, 0.79340218,\n          0.79822146, 0.79340218, 0.79822146, 0.79822146, 0.79822146,\n          0.79822146, 0.80301205, 0.79822146, 0.79581182, 0.79340218,\n          0.78861159, 0.78620195, 0.78620195, 0.78620195, 0.78620195,\n          0.79099254, 0.79340218, 0.79340218, 0.78620195, 0.79099254]),\n   \"{'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100}\": array([0.48075158, 0.48313253, 0.43978772, 0.521572  , 0.55527826,\n          0.5624498 , 0.5551922 , 0.55757315, 0.6033276 , 0.62745267,\n          0.65390132, 0.65149168, 0.64670109, 0.66589214, 0.67555938,\n          0.67550201, 0.67791165, 0.68995984, 0.68273092, 0.69234079,\n          0.70909352, 0.73075158, 0.74515204, 0.74759036, 0.74279977,\n          0.75002869, 0.74520941, 0.76442915, 0.7788296 , 0.78846816,\n          0.78364888, 0.7908778 , 0.79090648, 0.82208835, 0.81967871,\n          0.82449799, 0.82211704, 0.83178428, 0.82455536, 0.81732645,\n          0.82214573, 0.81973609, 0.82937464, 0.82699369, 0.82461274,\n          0.84621343, 0.84856569, 0.83416523, 0.86061388, 0.87257602,\n          0.86540448, 0.86781411, 0.87263339, 0.88221457, 0.87498566,\n          0.88218589, 0.89420539, 0.89661503, 0.89420539, 0.89899598,\n          0.90140562, 0.90140562, 0.89899598, 0.89899598, 0.90140562,\n          0.90863454, 0.91345382, 0.91345382, 0.90866322, 0.91827309,\n          0.92550201, 0.92791165, 0.92547332, 0.9302926 , 0.9302926 ,\n          0.9302926 , 0.93270224, 0.93270224, 0.93752151, 0.93752151,\n          0.93752151, 0.94475043, 0.94234079, 0.94713138, 0.94716007,\n          0.9302926 , 0.9302926 , 0.93270224, 0.94475043, 0.94475043,\n          0.92306368, 0.92306368, 0.92547332, 0.92788296, 0.92788296,\n          0.92788296, 0.90625359, 0.90625359, 0.90625359, 0.90625359]),\n   \"{'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\": array([0.48316122, 0.59368904, 0.68981641, 0.72831325, 0.74039013,\n          0.79331612, 0.81738382, 0.82458405, 0.85579461, 0.85100402,\n          0.87745267, 0.89672978, 0.91830178, 0.93514056, 0.93752151,\n          0.94472174, 0.94713138, 0.95195066, 0.94236948, 0.94713138,\n          0.88950086, 0.89432014, 0.89911073, 0.88227194, 0.89185313,\n          0.87025244, 0.86543316, 0.87025244, 0.87266208, 0.86302352,\n          0.86543316, 0.87504303, 0.87266208, 0.88944349, 0.89182444,\n          0.89664372, 0.89423408, 0.86543316, 0.87980493, 0.8846529 ,\n          0.87507172, 0.88224326, 0.87504303, 0.87263339, 0.86543316,\n          0.85823293, 0.85823293, 0.8390132 , 0.84624211, 0.8510327 ,\n          0.84383247, 0.84865175, 0.84142283, 0.84624211, 0.84624211,\n          0.84380379, 0.84142283, 0.8390132 , 0.84618474, 0.84856569,\n          0.8390132 , 0.84139415, 0.84380379, 0.8390132 , 0.8342226 ,\n          0.8390132 , 0.83660356, 0.84383247, 0.84862306, 0.83904188,\n          0.84145152, 0.82943201, 0.82943201, 0.83184165, 0.83425129,\n          0.82705106, 0.82705106, 0.82705106, 0.82943201, 0.82943201,\n          0.82705106, 0.82467011, 0.82226047, 0.82467011, 0.82467011,\n          0.82467011, 0.82226047, 0.82226047, 0.82948939, 0.82948939,\n          0.82707975, 0.82707975, 0.83187034, 0.83187034, 0.82707975,\n          0.8294607 , 0.82707975, 0.82707975, 0.82707975, 0.83187034]),\n   \"{'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\": array([0.47834194, 0.73080895, 0.73075158, 0.78602983, 0.8365175 ,\n          0.88700516, 0.89423408, 0.9302926 , 0.93752151, 0.93273092,\n          0.90625359, 0.91110155, 0.90866322, 0.89179575, 0.89420539,\n          0.88938612, 0.89179575, 0.90143431, 0.90384395, 0.8894148 ,\n          0.88700516, 0.88459552, 0.87498566, 0.8605852 , 0.85097533,\n          0.84615605, 0.84615605, 0.84856569, 0.84856569, 0.84615605,\n          0.84136546, 0.83895582, 0.82455536, 0.83416523, 0.84615605,\n          0.83172691, 0.82693632, 0.83657487, 0.81973609, 0.81250717,\n          0.8149455 , 0.79093517, 0.80051635, 0.79093517, 0.79331612,\n          0.79331612, 0.80295468, 0.79816408, 0.79575445, 0.81018359,\n          0.80536431, 0.81259323, 0.805393  , 0.805393  , 0.81021228,\n          0.81021228, 0.81259323, 0.81259323, 0.80298336, 0.79816408,\n          0.79093517, 0.80057372, 0.80057372, 0.80057372, 0.80780264,\n          0.80057372, 0.79575445, 0.79337349, 0.78620195, 0.78620195,\n          0.79096386, 0.80057372, 0.79575445, 0.80298336, 0.805393  ,\n          0.80060241, 0.805393  , 0.80780264, 0.79581182, 0.79340218,\n          0.79822146, 0.79340218, 0.79822146, 0.79822146, 0.79822146,\n          0.79822146, 0.80301205, 0.79822146, 0.79581182, 0.79102123,\n          0.78861159, 0.78620195, 0.78620195, 0.78620195, 0.78620195,\n          0.79099254, 0.79340218, 0.79099254, 0.78620195, 0.79099254]),\n   \"{'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100}\": array([0.48075158, 0.48313253, 0.43978772, 0.521572  , 0.55527826,\n          0.5624498 , 0.5551922 , 0.55757315, 0.6033276 , 0.62745267,\n          0.65390132, 0.65149168, 0.64670109, 0.66589214, 0.67555938,\n          0.67550201, 0.67791165, 0.68995984, 0.68273092, 0.69234079,\n          0.70909352, 0.73075158, 0.74515204, 0.74759036, 0.74279977,\n          0.75002869, 0.74520941, 0.76442915, 0.7788296 , 0.78846816,\n          0.78364888, 0.7908778 , 0.79090648, 0.82208835, 0.81967871,\n          0.82449799, 0.81973609, 0.83178428, 0.82455536, 0.81732645,\n          0.82214573, 0.81973609, 0.82937464, 0.82699369, 0.82461274,\n          0.84621343, 0.84856569, 0.83416523, 0.86061388, 0.87257602,\n          0.86540448, 0.86781411, 0.87263339, 0.88221457, 0.87498566,\n          0.88218589, 0.89420539, 0.89661503, 0.89420539, 0.89899598,\n          0.90140562, 0.90140562, 0.89899598, 0.89899598, 0.90140562,\n          0.90863454, 0.91345382, 0.91345382, 0.90625359, 0.91827309,\n          0.92791165, 0.92791165, 0.92547332, 0.9302926 , 0.9302926 ,\n          0.9302926 , 0.93270224, 0.93270224, 0.93752151, 0.93752151,\n          0.93752151, 0.94475043, 0.94234079, 0.94713138, 0.94716007,\n          0.9302926 , 0.9302926 , 0.93270224, 0.94475043, 0.94475043,\n          0.92306368, 0.92068273, 0.92547332, 0.92788296, 0.92788296,\n          0.92788296, 0.90625359, 0.90625359, 0.90625359, 0.90625359]),\n   \"{'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500}\": array([0.48316122, 0.59368904, 0.68981641, 0.72831325, 0.74039013,\n          0.79331612, 0.81738382, 0.82458405, 0.85579461, 0.85100402,\n          0.87745267, 0.89672978, 0.91830178, 0.93514056, 0.93752151,\n          0.94472174, 0.94713138, 0.95195066, 0.94236948, 0.94713138,\n          0.88950086, 0.89432014, 0.89911073, 0.88227194, 0.89185313,\n          0.87025244, 0.86543316, 0.87025244, 0.87266208, 0.86302352,\n          0.86543316, 0.87504303, 0.87266208, 0.88703385, 0.89182444,\n          0.89664372, 0.89423408, 0.86543316, 0.87980493, 0.8846529 ,\n          0.87507172, 0.88224326, 0.87504303, 0.87022375, 0.86543316,\n          0.85823293, 0.85823293, 0.8390132 , 0.84383247, 0.8510327 ,\n          0.84383247, 0.84865175, 0.84142283, 0.84624211, 0.84383247,\n          0.84142283, 0.84142283, 0.8390132 , 0.84618474, 0.84856569,\n          0.8390132 , 0.8390132 , 0.84142283, 0.8390132 , 0.8342226 ,\n          0.8390132 , 0.83660356, 0.84142283, 0.84624211, 0.83904188,\n          0.84145152, 0.82943201, 0.82943201, 0.83184165, 0.83184165,\n          0.82705106, 0.82705106, 0.82705106, 0.82943201, 0.82943201,\n          0.82705106, 0.82467011, 0.82226047, 0.82467011, 0.82467011,\n          0.82467011, 0.82226047, 0.82226047, 0.82948939, 0.82948939,\n          0.82226047, 0.82226047, 0.83187034, 0.83187034, 0.82707975,\n          0.82707975, 0.82707975, 0.82707975, 0.82707975, 0.82707975]),\n   \"{'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\": array([0.47834194, 0.73080895, 0.73075158, 0.78602983, 0.8365175 ,\n          0.88700516, 0.89423408, 0.9302926 , 0.93752151, 0.93273092,\n          0.90625359, 0.91110155, 0.90866322, 0.89179575, 0.89420539,\n          0.88938612, 0.89179575, 0.90143431, 0.90384395, 0.8894148 ,\n          0.88459552, 0.88218589, 0.87498566, 0.8605852 , 0.85097533,\n          0.84615605, 0.84615605, 0.84856569, 0.84856569, 0.84615605,\n          0.84136546, 0.83895582, 0.82455536, 0.83416523, 0.84615605,\n          0.82693632, 0.82693632, 0.83416523, 0.81732645, 0.81250717,\n          0.81012622, 0.79093517, 0.80051635, 0.78852553, 0.79331612,\n          0.79090648, 0.80054504, 0.79816408, 0.79575445, 0.805393  ,\n          0.80536431, 0.81259323, 0.805393  , 0.80057372, 0.81021228,\n          0.81021228, 0.81021228, 0.81259323, 0.80298336, 0.79816408,\n          0.78852553, 0.79575445, 0.79816408, 0.80057372, 0.80780264,\n          0.80057372, 0.79575445, 0.78620195, 0.78620195, 0.78620195,\n          0.78620195, 0.79575445, 0.79334481, 0.80298336, 0.80298336,\n          0.79819277, 0.80060241, 0.80780264, 0.79340218, 0.79340218,\n          0.79581182, 0.79340218, 0.79581182, 0.79822146, 0.79822146,\n          0.79822146, 0.80060241, 0.79581182, 0.79340218, 0.78620195,\n          0.78620195, 0.783821  , 0.78138267, 0.78620195, 0.78138267,\n          0.78620195, 0.78861159, 0.78620195, 0.78620195, 0.78620195]),\n   \"{'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 100}\": array([0.4759323 , 0.4759323 , 0.47349398, 0.48313253, 0.48554217,\n          0.48795181, 0.48554217, 0.48554217, 0.49277108, 0.49518072,\n          0.49759036, 0.49759036, 0.49759036, 0.49759036, 0.50238095,\n          0.50238095, 0.50238095, 0.50238095, 0.50238095, 0.50238095,\n          0.50238095, 0.50238095, 0.50479059, 0.50479059, 0.50479059,\n          0.50479059, 0.50479059, 0.50479059, 0.50717154, 0.50958118,\n          0.51199082, 0.51199082, 0.51199082, 0.5168101 , 0.5168101 ,\n          0.51678141, 0.51440046, 0.51440046, 0.5168101 , 0.5168101 ,\n          0.52162937, 0.51921974, 0.52403901, 0.52641997, 0.5288296 ,\n          0.53364888, 0.53605852, 0.53605852, 0.54328744, 0.54569707,\n          0.54569707, 0.54569707, 0.54569707, 0.54569707, 0.54569707,\n          0.54810671, 0.55533563, 0.55533563, 0.55533563, 0.56256454,\n          0.56732645, 0.5697074 , 0.5697074 , 0.57690763, 0.58413655,\n          0.58654618, 0.59136546, 0.59136546, 0.5937751 , 0.59618474,\n          0.59618474, 0.59618474, 0.60097533, 0.60338497, 0.60579461,\n          0.6105852 , 0.61299484, 0.61299484, 0.60817556, 0.60817556,\n          0.60817556, 0.6105852 , 0.61299484, 0.61540448, 0.61781411,\n          0.61540448, 0.61540448, 0.61781411, 0.62263339, 0.62504303,\n          0.62745267, 0.62745267, 0.62986231, 0.63227194, 0.6346529 ,\n          0.6346529 , 0.63706254, 0.64429145, 0.64188181, 0.64188181]),\n   \"{'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 500}\": array([0.4759323 , 0.49997131, 0.50717154, 0.5047619 , 0.50717154,\n          0.50958118, 0.51678141, 0.51921974, 0.52641997, 0.53364888,\n          0.55048766, 0.55530694, 0.56491681, 0.576965  , 0.59139415,\n          0.60582329, 0.61302352, 0.60817556, 0.60823293, 0.62025244,\n          0.64432014, 0.64672978, 0.65874928, 0.67071142, 0.67550201,\n          0.67555938, 0.68516925, 0.69477912, 0.69954102, 0.69716007,\n          0.70197935, 0.70679862, 0.70917958, 0.71397017, 0.71878944,\n          0.73075158, 0.73557085, 0.72596099, 0.73559954, 0.74520941,\n          0.7452381 , 0.74761905, 0.7452381 , 0.74764773, 0.75005737,\n          0.75005737, 0.75246701, 0.74285714, 0.75246701, 0.75728629,\n          0.76689616, 0.76930579, 0.76927711, 0.76927711, 0.76686747,\n          0.76686747, 0.76686747, 0.76686747, 0.77165806, 0.77888698,\n          0.78370625, 0.78849684, 0.78849684, 0.78373494, 0.78852553,\n          0.78852553, 0.79090648, 0.79331612, 0.79572576, 0.79572576,\n          0.7981354 , 0.80051635, 0.80051635, 0.80051635, 0.80292599,\n          0.80051635, 0.80292599, 0.80051635, 0.80530694, 0.80292599,\n          0.80292599, 0.79575445, 0.79096386, 0.79334481, 0.79334481,\n          0.79575445, 0.79575445, 0.7981354 , 0.80054504, 0.80054504,\n          0.80292599, 0.80054504, 0.80054504, 0.80054504, 0.80292599,\n          0.80292599, 0.80051635, 0.80292599, 0.80292599, 0.80292599]),\n   \"{'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000}\": array([0.4759323 , 0.50479059, 0.50479059, 0.51921974, 0.53364888,\n          0.55530694, 0.57205967, 0.61299484, 0.6226047 , 0.64182444,\n          0.65146299, 0.67309237, 0.68267355, 0.68746414, 0.69228342,\n          0.70433161, 0.71632243, 0.72355135, 0.72834194, 0.7259323 ,\n          0.74759036, 0.75002869, 0.74764773, 0.75005737, 0.75002869,\n          0.75484796, 0.75487665, 0.76210557, 0.76207688, 0.76689616,\n          0.77412507, 0.77891566, 0.77409639, 0.78611589, 0.79331612,\n          0.79328744, 0.79328744, 0.79569707, 0.78129662, 0.78129662,\n          0.78370625, 0.77409639, 0.7813253 , 0.7813253 , 0.79090648,\n          0.78608721, 0.79331612, 0.7981354 , 0.79575445, 0.80295468,\n          0.80054504, 0.80536431, 0.80774527, 0.80536431, 0.81015491,\n          0.80777395, 0.81015491, 0.80774527, 0.80774527, 0.80292599,\n          0.79810671, 0.80051635, 0.8028973 , 0.8028973 , 0.80771658,\n          0.80530694, 0.80530694, 0.80048766, 0.79807803, 0.80048766,\n          0.8028973 , 0.80530694, 0.80530694, 0.80771658, 0.8028973 ,\n          0.8028973 , 0.80530694, 0.80530694, 0.80530694, 0.80530694,\n          0.80530694, 0.8028973 , 0.80530694, 0.80771658, 0.80771658,\n          0.80768789, 0.81009753, 0.80768789, 0.80768789, 0.80768789,\n          0.80768789, 0.8028973 , 0.79810671, 0.80051635, 0.80051635,\n          0.80292599, 0.80292599, 0.80292599, 0.79810671, 0.80051635]),\n   \"{'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 100}\": array([0.4759323 , 0.4759323 , 0.47349398, 0.48313253, 0.48554217,\n          0.48795181, 0.48554217, 0.48554217, 0.49277108, 0.49518072,\n          0.49759036, 0.49759036, 0.49759036, 0.49759036, 0.50238095,\n          0.50238095, 0.50238095, 0.50238095, 0.50238095, 0.50238095,\n          0.50238095, 0.50238095, 0.50479059, 0.50479059, 0.50479059,\n          0.50479059, 0.50479059, 0.50479059, 0.50717154, 0.50958118,\n          0.51199082, 0.51199082, 0.51199082, 0.5168101 , 0.5168101 ,\n          0.51678141, 0.51440046, 0.51440046, 0.5168101 , 0.5168101 ,\n          0.52162937, 0.51921974, 0.52403901, 0.52641997, 0.5288296 ,\n          0.53364888, 0.53605852, 0.53605852, 0.54328744, 0.54569707,\n          0.54569707, 0.54569707, 0.54569707, 0.54569707, 0.54569707,\n          0.54810671, 0.55533563, 0.55533563, 0.55533563, 0.56256454,\n          0.56732645, 0.5697074 , 0.5697074 , 0.57690763, 0.58413655,\n          0.58654618, 0.59136546, 0.59136546, 0.5937751 , 0.59618474,\n          0.59618474, 0.59618474, 0.60097533, 0.60338497, 0.60579461,\n          0.6105852 , 0.61299484, 0.61299484, 0.60817556, 0.60817556,\n          0.60817556, 0.6105852 , 0.61299484, 0.61540448, 0.61781411,\n          0.61540448, 0.61540448, 0.61781411, 0.62263339, 0.62504303,\n          0.62745267, 0.62745267, 0.62986231, 0.63227194, 0.6346529 ,\n          0.6346529 , 0.63706254, 0.64429145, 0.64188181, 0.64188181]),\n   \"{'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 500}\": array([0.4759323 , 0.49997131, 0.50717154, 0.5047619 , 0.50717154,\n          0.50958118, 0.51678141, 0.51921974, 0.52641997, 0.53364888,\n          0.55048766, 0.55530694, 0.56491681, 0.576965  , 0.59139415,\n          0.60582329, 0.61302352, 0.60817556, 0.60823293, 0.62025244,\n          0.64432014, 0.64672978, 0.65874928, 0.67071142, 0.67550201,\n          0.67555938, 0.68516925, 0.69477912, 0.69954102, 0.69716007,\n          0.70197935, 0.70679862, 0.70917958, 0.71397017, 0.71878944,\n          0.73075158, 0.73557085, 0.72596099, 0.73559954, 0.74520941,\n          0.7452381 , 0.74761905, 0.7452381 , 0.74764773, 0.75005737,\n          0.75005737, 0.75246701, 0.74285714, 0.75246701, 0.75728629,\n          0.76689616, 0.76930579, 0.76927711, 0.76927711, 0.76686747,\n          0.76686747, 0.76686747, 0.76686747, 0.77165806, 0.77888698,\n          0.78370625, 0.78849684, 0.78849684, 0.78373494, 0.78852553,\n          0.78852553, 0.79090648, 0.79331612, 0.79572576, 0.79572576,\n          0.7981354 , 0.80051635, 0.80051635, 0.80051635, 0.80292599,\n          0.80051635, 0.80292599, 0.80051635, 0.80530694, 0.80292599,\n          0.80292599, 0.79575445, 0.79096386, 0.79334481, 0.79334481,\n          0.79575445, 0.79575445, 0.7981354 , 0.80054504, 0.80054504,\n          0.80292599, 0.80054504, 0.80054504, 0.80054504, 0.80292599,\n          0.80292599, 0.80051635, 0.80292599, 0.80292599, 0.80292599]),\n   \"{'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000}\": array([0.4759323 , 0.50479059, 0.50479059, 0.51921974, 0.53364888,\n          0.55530694, 0.57205967, 0.61299484, 0.6226047 , 0.64182444,\n          0.65146299, 0.67309237, 0.68267355, 0.68746414, 0.69228342,\n          0.70433161, 0.71632243, 0.72355135, 0.72834194, 0.7259323 ,\n          0.74759036, 0.75002869, 0.74764773, 0.75005737, 0.75002869,\n          0.75484796, 0.75487665, 0.76210557, 0.76207688, 0.76689616,\n          0.77412507, 0.77891566, 0.77409639, 0.78611589, 0.79331612,\n          0.79328744, 0.79328744, 0.79569707, 0.78129662, 0.78129662,\n          0.78370625, 0.77409639, 0.7813253 , 0.7813253 , 0.79090648,\n          0.78608721, 0.79331612, 0.7981354 , 0.79575445, 0.80295468,\n          0.80054504, 0.80536431, 0.80774527, 0.80536431, 0.81015491,\n          0.80777395, 0.81015491, 0.80774527, 0.80774527, 0.80292599,\n          0.79810671, 0.80051635, 0.8028973 , 0.8028973 , 0.80771658,\n          0.80530694, 0.80530694, 0.80048766, 0.79807803, 0.80048766,\n          0.8028973 , 0.80530694, 0.80530694, 0.80771658, 0.8028973 ,\n          0.8028973 , 0.80530694, 0.80530694, 0.80530694, 0.80530694,\n          0.80530694, 0.8028973 , 0.80530694, 0.80771658, 0.80771658,\n          0.80768789, 0.81009753, 0.80768789, 0.80768789, 0.80768789,\n          0.80768789, 0.8028973 , 0.79810671, 0.80051635, 0.80051635,\n          0.80292599, 0.80292599, 0.80292599, 0.79810671, 0.80051635]),\n   \"{'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100}\": array([0.4759323 , 0.4759323 , 0.47349398, 0.48313253, 0.48554217,\n          0.48795181, 0.48554217, 0.48554217, 0.49277108, 0.49518072,\n          0.49759036, 0.49759036, 0.49759036, 0.49759036, 0.50238095,\n          0.50238095, 0.50238095, 0.50238095, 0.50238095, 0.50238095,\n          0.50238095, 0.50238095, 0.50479059, 0.50479059, 0.50479059,\n          0.50479059, 0.50479059, 0.50479059, 0.50717154, 0.50958118,\n          0.51199082, 0.51199082, 0.51199082, 0.5168101 , 0.5168101 ,\n          0.51678141, 0.51440046, 0.51440046, 0.5168101 , 0.5168101 ,\n          0.52162937, 0.51921974, 0.52403901, 0.52641997, 0.5288296 ,\n          0.53364888, 0.53605852, 0.53605852, 0.54328744, 0.54569707,\n          0.54569707, 0.54569707, 0.54569707, 0.54569707, 0.54569707,\n          0.54810671, 0.55533563, 0.55533563, 0.55533563, 0.56256454,\n          0.56732645, 0.5697074 , 0.5697074 , 0.57690763, 0.58413655,\n          0.58654618, 0.59136546, 0.59136546, 0.5937751 , 0.59618474,\n          0.59618474, 0.59618474, 0.60097533, 0.60338497, 0.60579461,\n          0.6105852 , 0.61299484, 0.61299484, 0.60817556, 0.60817556,\n          0.60817556, 0.6105852 , 0.61299484, 0.61540448, 0.61781411,\n          0.61540448, 0.61540448, 0.61781411, 0.62263339, 0.62504303,\n          0.62745267, 0.62745267, 0.62986231, 0.63227194, 0.6346529 ,\n          0.6346529 , 0.63706254, 0.64429145, 0.64188181, 0.64188181]),\n   \"{'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500}\": array([0.4759323 , 0.49997131, 0.50717154, 0.5047619 , 0.50717154,\n          0.50958118, 0.51678141, 0.51921974, 0.52641997, 0.53364888,\n          0.55048766, 0.55530694, 0.56491681, 0.576965  , 0.59139415,\n          0.60582329, 0.61302352, 0.60817556, 0.60823293, 0.62025244,\n          0.64432014, 0.64672978, 0.65874928, 0.67071142, 0.67550201,\n          0.67555938, 0.68516925, 0.69477912, 0.69954102, 0.69716007,\n          0.70197935, 0.70679862, 0.70917958, 0.71397017, 0.71878944,\n          0.73075158, 0.73557085, 0.72596099, 0.73559954, 0.74520941,\n          0.7452381 , 0.74761905, 0.7452381 , 0.74764773, 0.75005737,\n          0.75005737, 0.75246701, 0.74285714, 0.75246701, 0.75728629,\n          0.76689616, 0.76930579, 0.76927711, 0.76927711, 0.76686747,\n          0.76686747, 0.76686747, 0.76686747, 0.77165806, 0.77888698,\n          0.78370625, 0.78849684, 0.78849684, 0.78373494, 0.78852553,\n          0.78852553, 0.79090648, 0.79331612, 0.79572576, 0.79572576,\n          0.7981354 , 0.80051635, 0.80051635, 0.80051635, 0.80292599,\n          0.80051635, 0.80292599, 0.80051635, 0.80530694, 0.80292599,\n          0.80292599, 0.79575445, 0.79096386, 0.79334481, 0.79334481,\n          0.79575445, 0.79575445, 0.7981354 , 0.80054504, 0.80054504,\n          0.80292599, 0.80054504, 0.80054504, 0.80054504, 0.80292599,\n          0.80292599, 0.80051635, 0.80292599, 0.80292599, 0.80292599]),\n   \"{'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\": array([0.4759323 , 0.50479059, 0.50479059, 0.51921974, 0.53364888,\n          0.55530694, 0.57205967, 0.61299484, 0.6226047 , 0.64182444,\n          0.65146299, 0.67309237, 0.68267355, 0.68746414, 0.69228342,\n          0.70433161, 0.71632243, 0.72355135, 0.72834194, 0.7259323 ,\n          0.74759036, 0.75002869, 0.74764773, 0.75005737, 0.75002869,\n          0.75484796, 0.75487665, 0.76210557, 0.76207688, 0.76689616,\n          0.77412507, 0.77891566, 0.77409639, 0.78611589, 0.79331612,\n          0.79328744, 0.79328744, 0.79569707, 0.78129662, 0.78129662,\n          0.78370625, 0.77409639, 0.7813253 , 0.7813253 , 0.79090648,\n          0.78608721, 0.79331612, 0.7981354 , 0.79575445, 0.80295468,\n          0.80054504, 0.80536431, 0.80774527, 0.80536431, 0.81015491,\n          0.80777395, 0.81015491, 0.80774527, 0.80774527, 0.80292599,\n          0.79810671, 0.80051635, 0.8028973 , 0.8028973 , 0.80771658,\n          0.80530694, 0.80530694, 0.80048766, 0.79807803, 0.80048766,\n          0.8028973 , 0.80530694, 0.80530694, 0.80771658, 0.8028973 ,\n          0.8028973 , 0.80530694, 0.80530694, 0.80530694, 0.80530694,\n          0.80530694, 0.8028973 , 0.80530694, 0.80771658, 0.80771658,\n          0.80768789, 0.81009753, 0.80768789, 0.80768789, 0.80768789,\n          0.80768789, 0.8028973 , 0.79810671, 0.80051635, 0.80051635,\n          0.80292599, 0.80292599, 0.80292599, 0.79810671, 0.80051635]),\n   \"{'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100}\": array([0.4759323 , 0.4759323 , 0.47349398, 0.48313253, 0.48554217,\n          0.48795181, 0.48554217, 0.48554217, 0.49277108, 0.49518072,\n          0.49759036, 0.49759036, 0.49759036, 0.49759036, 0.50238095,\n          0.50238095, 0.50238095, 0.50238095, 0.50238095, 0.50238095,\n          0.50238095, 0.50238095, 0.50479059, 0.50479059, 0.50479059,\n          0.50479059, 0.50479059, 0.50479059, 0.50717154, 0.50958118,\n          0.51199082, 0.51199082, 0.51199082, 0.5168101 , 0.5168101 ,\n          0.51678141, 0.51440046, 0.51440046, 0.5168101 , 0.5168101 ,\n          0.52162937, 0.51921974, 0.52403901, 0.52641997, 0.5288296 ,\n          0.53364888, 0.53605852, 0.53605852, 0.54328744, 0.54569707,\n          0.54569707, 0.54569707, 0.54569707, 0.54569707, 0.54569707,\n          0.54810671, 0.55533563, 0.55533563, 0.55533563, 0.56256454,\n          0.56732645, 0.5697074 , 0.5697074 , 0.57690763, 0.58413655,\n          0.58654618, 0.59136546, 0.59136546, 0.5937751 , 0.59618474,\n          0.59618474, 0.59618474, 0.60097533, 0.60338497, 0.60579461,\n          0.6105852 , 0.61299484, 0.61299484, 0.60817556, 0.60817556,\n          0.60817556, 0.6105852 , 0.61299484, 0.61540448, 0.61781411,\n          0.61540448, 0.61540448, 0.61781411, 0.62263339, 0.62504303,\n          0.62745267, 0.62745267, 0.62986231, 0.63227194, 0.6346529 ,\n          0.6346529 , 0.63706254, 0.64429145, 0.64188181, 0.64188181]),\n   \"{'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500}\": array([0.4759323 , 0.49997131, 0.50717154, 0.5047619 , 0.50717154,\n          0.50958118, 0.51678141, 0.51921974, 0.52641997, 0.53364888,\n          0.55048766, 0.55530694, 0.56491681, 0.576965  , 0.59139415,\n          0.60582329, 0.61302352, 0.60817556, 0.60823293, 0.62025244,\n          0.64432014, 0.64672978, 0.65874928, 0.67071142, 0.67550201,\n          0.67555938, 0.68516925, 0.69477912, 0.69954102, 0.69716007,\n          0.70197935, 0.70679862, 0.70917958, 0.71397017, 0.71878944,\n          0.73075158, 0.73557085, 0.72596099, 0.73559954, 0.74520941,\n          0.7452381 , 0.74761905, 0.7452381 , 0.74764773, 0.75005737,\n          0.75005737, 0.75246701, 0.74285714, 0.75246701, 0.75728629,\n          0.76689616, 0.76930579, 0.76927711, 0.76927711, 0.76686747,\n          0.76686747, 0.76686747, 0.76686747, 0.77165806, 0.77888698,\n          0.78370625, 0.78849684, 0.78849684, 0.78373494, 0.78852553,\n          0.78852553, 0.79090648, 0.79331612, 0.79572576, 0.79572576,\n          0.7981354 , 0.7981354 , 0.80051635, 0.80051635, 0.80292599,\n          0.80051635, 0.80292599, 0.80051635, 0.80530694, 0.80292599,\n          0.80054504, 0.79575445, 0.79096386, 0.79334481, 0.79334481,\n          0.79575445, 0.79337349, 0.7981354 , 0.80054504, 0.80054504,\n          0.80292599, 0.80054504, 0.80054504, 0.80054504, 0.80292599,\n          0.80292599, 0.80051635, 0.80292599, 0.80292599, 0.80292599]),\n   \"{'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\": array([0.4759323 , 0.50479059, 0.50479059, 0.51921974, 0.53364888,\n          0.55530694, 0.57205967, 0.61299484, 0.6226047 , 0.64182444,\n          0.65146299, 0.67309237, 0.68267355, 0.68746414, 0.69228342,\n          0.70433161, 0.71632243, 0.72355135, 0.72834194, 0.7259323 ,\n          0.74759036, 0.75002869, 0.74764773, 0.75005737, 0.75002869,\n          0.75484796, 0.75487665, 0.76210557, 0.76207688, 0.76689616,\n          0.77412507, 0.77891566, 0.77409639, 0.78611589, 0.79331612,\n          0.79328744, 0.79328744, 0.79569707, 0.78129662, 0.78129662,\n          0.78370625, 0.77409639, 0.7813253 , 0.7813253 , 0.79090648,\n          0.78608721, 0.79331612, 0.7981354 , 0.79575445, 0.80295468,\n          0.80054504, 0.80536431, 0.80774527, 0.80536431, 0.81015491,\n          0.80777395, 0.81015491, 0.80774527, 0.80536431, 0.80292599,\n          0.79810671, 0.80051635, 0.8028973 , 0.8028973 , 0.80771658,\n          0.80530694, 0.80530694, 0.80048766, 0.79807803, 0.80048766,\n          0.8028973 , 0.80530694, 0.80530694, 0.80771658, 0.8028973 ,\n          0.8028973 , 0.80530694, 0.80530694, 0.80530694, 0.80530694,\n          0.80530694, 0.8028973 , 0.80530694, 0.80771658, 0.80771658,\n          0.80768789, 0.81009753, 0.80768789, 0.80768789, 0.8028973 ,\n          0.80768789, 0.80051635, 0.79810671, 0.80051635, 0.80051635,\n          0.80292599, 0.80292599, 0.80292599, 0.79810671, 0.80051635])}},\n 'test_accuracies': {'baseline_random': np.float64(0.6923076923076923),\n  'custom_random': np.float64(0.9615384615384616),\n  'custom_gradient': np.float64(0.9615384615384616),\n  'custom_cyclic': np.float64(0.9615384615384616)},\n 'training_accuracies': {'baseline_random': array([0.48076923, 0.71153846, 0.76923077, 0.79807692, 0.82692308,\n         0.89423077, 0.875     , 0.89423077, 0.91346154, 0.90384615,\n         0.89423077, 0.90384615, 0.88461538, 0.875     , 0.88461538,\n         0.86538462, 0.875     , 0.875     , 0.875     , 0.85576923,\n         0.85576923, 0.84615385, 0.84615385, 0.82692308, 0.81730769,\n         0.82692308, 0.82692308, 0.82692308, 0.82692308, 0.82692308,\n         0.82692308, 0.82692308, 0.82692308, 0.82692308, 0.83653846,\n         0.82692308, 0.82692308, 0.84615385, 0.82692308, 0.82692308,\n         0.81730769, 0.80769231, 0.81730769, 0.80769231, 0.81730769,\n         0.80769231, 0.80769231, 0.80769231, 0.80769231, 0.80769231,\n         0.80769231, 0.81730769, 0.80769231, 0.81730769, 0.81730769,\n         0.81730769, 0.81730769, 0.81730769, 0.81730769, 0.79807692,\n         0.79807692, 0.79807692, 0.79807692, 0.80769231, 0.80769231,\n         0.80769231, 0.80769231, 0.80769231, 0.79807692, 0.80769231,\n         0.80769231, 0.80769231, 0.80769231, 0.81730769, 0.81730769,\n         0.80769231, 0.81730769, 0.82692308, 0.81730769, 0.81730769,\n         0.81730769, 0.81730769, 0.81730769, 0.81730769, 0.81730769,\n         0.81730769, 0.81730769, 0.80769231, 0.80769231, 0.80769231,\n         0.80769231, 0.80769231, 0.80769231, 0.80769231, 0.80769231,\n         0.80769231, 0.80769231, 0.80769231, 0.80769231, 0.80769231]),\n  'custom_random': array([0.48076923, 0.70192308, 0.78846154, 0.85576923, 0.89423077,\n         0.93269231, 0.94230769, 0.94230769, 0.93269231, 0.94230769,\n         0.95192308, 0.94230769, 0.93269231, 0.95192308, 0.95192308,\n         0.95192308, 0.94230769, 0.94230769, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.96153846, 0.95192308, 0.95192308, 0.95192308,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.93269231,\n         0.94230769, 0.94230769, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846]),\n  'custom_gradient': array([0.48076923, 0.75      , 0.85576923, 0.875     , 0.91346154,\n         0.94230769, 0.92307692, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.97115385, 0.97115385, 0.96153846, 0.96153846,\n         0.96153846, 0.97115385, 0.97115385, 0.97115385, 0.96153846,\n         0.95192308, 0.95192308, 0.95192308, 0.96153846, 0.96153846,\n         0.96153846, 0.97115385, 0.96153846, 0.96153846, 0.96153846,\n         0.97115385, 0.96153846, 0.96153846, 0.97115385, 0.96153846,\n         0.96153846, 0.97115385, 0.97115385, 0.97115385, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385]),\n  'custom_cyclic': array([0.48076923, 0.72115385, 0.80769231, 0.90384615, 0.94230769,\n         0.95192308, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.97115385, 0.96153846, 0.97115385, 0.97115385,\n         0.97115385, 0.98076923, 0.98076923, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.96153846, 0.97115385, 0.97115385, 0.97115385, 0.96153846,\n         0.97115385, 0.97115385, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846])}}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullBreakdown(X, y, param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:38:50.667715Z",
     "start_time": "2025-02-25T00:38:45.751196Z"
    }
   },
   "id": "2e7e1cd3c72d61ea",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Baseline Logistic Regression Accuracy: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Evaluate performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Custom Baseline Logistic Regression Accuracy: {accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:39:47.925028Z",
     "start_time": "2025-02-25T00:39:47.916488Z"
    }
   },
   "id": "86f606e51141a5d6",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(y_pred[:20])\n",
    "print(y_test[:20].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T00:23:44.189261Z",
     "start_time": "2025-02-25T00:23:44.124930Z"
    }
   },
   "id": "88e9743176381ab8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T00:23:44.151204Z"
    }
   },
   "id": "a22005022a497800",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
