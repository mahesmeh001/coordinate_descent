{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "get and format dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81dcf1a8e907311"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "0      1    14.23       1.71  2.43               15.6        127   \n",
      "1      1    13.20       1.78  2.14               11.2        100   \n",
      "2      1    13.16       2.36  2.67               18.6        101   \n",
      "3      1    14.37       1.95  2.50               16.8        113   \n",
      "4      1    13.24       2.59  2.87               21.0        118   \n",
      "\n",
      "   Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# load in wine \n",
    "import pandas as pd\n",
    "\n",
    "# Define column names\n",
    "column_names = [\n",
    "    \"Class\", \"Alcohol\", \"Malicacid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\",\n",
    "    \"Total_phenols\", \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\",\n",
    "    \"Color_intensity\", \"Hue\", \"OD280_OD315_of_diluted_wines\", \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"wine/wine.data\", names=column_names)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:15.003805Z",
     "start_time": "2025-02-24T20:14:13.517365Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "125      2    12.07       2.16  2.17               21.0         85   \n",
      "126      2    12.43       1.53  2.29               21.5         86   \n",
      "127      2    11.79       2.13  2.78               28.5         92   \n",
      "128      2    12.37       1.63  2.30               24.5         88   \n",
      "129      2    12.04       4.30  2.38               22.0         80   \n",
      "\n",
      "     Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "125           2.60        2.65                  0.37             1.35   \n",
      "126           2.74        3.15                  0.39             1.77   \n",
      "127           2.13        2.24                  0.58             1.76   \n",
      "128           2.22        2.45                  0.40             1.90   \n",
      "129           2.10        1.75                  0.42             1.35   \n",
      "\n",
      "     Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "125             2.76  0.86                          3.28      378  \n",
      "126             3.94  0.69                          2.84      352  \n",
      "127             3.00  0.97                          2.44      466  \n",
      "128             2.12  0.89                          2.78      342  \n",
      "129             2.60  0.79                          2.57      580  \n"
     ]
    }
   ],
   "source": [
    "# Remove rows where Class is 3\n",
    "df = df[df[\"Class\"] != 3]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.tail())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:15.010533Z",
     "start_time": "2025-02-24T20:14:15.000724Z"
    }
   },
   "id": "2d8045463ae6a81c",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement a logistic regression baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5225897f44a5679"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:17.865505Z",
     "start_time": "2025-02-24T20:14:15.010809Z"
    }
   },
   "id": "118114fbb2dd5fd0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop(columns=['Class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:17.873498Z",
     "start_time": "2025-02-24T20:14:17.868634Z"
    }
   },
   "id": "7fd4db81cc7da185",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=700)",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=700)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=700)</pre></div> </div></div></div></div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=700)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:17.943306Z",
     "start_time": "2025-02-24T20:14:17.876133Z"
    }
   },
   "id": "dd62d0b2c944a9e3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Baseline Logistic Regression Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:17.944146Z",
     "start_time": "2025-02-24T20:14:17.940027Z"
    }
   },
   "id": "258146464d8cdf32",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need a custom class so we can directly modify the loss function... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d419fb738aa61fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, max_iter=2000, tol=1e-4, fit_intercept=True, reg_lambda=0.01):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol  # Tolerance for convergence (minimum improvement required)\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weight = None\n",
    "        self.feature_means = None\n",
    "        self.feature_stds = None\n",
    "\n",
    "    def _normalize_features(self, X):\n",
    "        \"\"\"Normalizes features using z-score standardization.\"\"\"\n",
    "        if self.feature_means is None or self.feature_stds is None:\n",
    "            self.feature_means = np.mean(X, axis=0)\n",
    "            self.feature_stds = np.std(X, axis=0) + 1e-8  # Avoid division by zero\n",
    "        \n",
    "        return (X - self.feature_means) / self.feature_stds\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Adds intercept term (bias) to feature matrix.\"\"\"\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip values to prevent overflow\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return expit(z)  # Numerically stable sigmoid\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"Computes the binary cross-entropy loss with regularization.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Add L2 regularization term (excluding bias)\n",
    "        reg_term = 0.5 * self.reg_lambda * np.sum(self.weight[1:] ** 2) / m\n",
    "        \n",
    "        # Compute binary cross-entropy\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        \n",
    "        return loss + reg_term\n",
    "\n",
    "    def _compute_gradient(self, X, y, sample_weights=None):\n",
    "        \"\"\"Computes the gradient of the loss function with regularization.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        \n",
    "        if sample_weights is None:\n",
    "            sample_weights = np.ones(m)\n",
    "            \n",
    "        # Weighted gradient\n",
    "        gradient = (X.T @ ((h - y) * sample_weights)) / np.sum(sample_weights)\n",
    "        \n",
    "        # Add L2 regularization\n",
    "        reg_term = np.zeros_like(self.weight)\n",
    "        reg_term[1:] = self.reg_lambda * self.weight[1:] / m  # Don't regularize bias\n",
    "        \n",
    "        return gradient + reg_term\n",
    "    \n",
    "    def _compute_gradient_for_coordinate(self, X, y, j, sample_weights=None):\n",
    "        \"\"\"Computes the gradient for a single coordinate j.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        \n",
    "        if sample_weights is None:\n",
    "            sample_weights = np.ones(m)\n",
    "            \n",
    "        # Gradient for single coordinate\n",
    "        gradient_j = np.sum((h - y) * sample_weights * X[:, j]) / np.sum(sample_weights)\n",
    "        \n",
    "        # Add L2 regularization (except for bias term)\n",
    "        if j > 0 or not self.fit_intercept:\n",
    "            gradient_j += self.reg_lambda * self.weight[j] / m\n",
    "            \n",
    "        return gradient_j\n",
    "\n",
    "    def _custom_loss(self, X, y, sample_weights=None):\n",
    "        \"\"\"Custom loss function placeholder to be implemented later.\"\"\"\n",
    "        # This is a placeholder for future implementation\n",
    "        # Currently just redirects to the regular gradient computation\n",
    "        return self._compute_gradient(X, y, sample_weights)\n",
    "    \n",
    "    def _custom_gradient_for_coordinate(self, X, y, j, sample_weights=None):\n",
    "        \"\"\"Custom gradient computation for a single coordinate.\"\"\"\n",
    "        # Placeholder for future implementation\n",
    "        # Currently just redirects to the regular coordinate gradient computation\n",
    "        return self._compute_gradient_for_coordinate(X, y, j, sample_weights)\n",
    "\n",
    "    def _get_coordinates(self, n_features, method=\"random\", X=None, y=None):\n",
    "        \"\"\"Determine the order of coordinates for coordinate descent.\"\"\"\n",
    "        if method == \"random\":\n",
    "            # Randomly select coordinates\n",
    "            coordinates = np.random.randint(0, n_features, size=self.max_iter)\n",
    "        elif method == \"custom\":\n",
    "            # Placeholder for a custom coordinate selection strategy\n",
    "            # For now, just return random coordinates\n",
    "            coordinates = np.random.randint(0, n_features, size=self.max_iter)\n",
    "            # TODO: Implement intelligent coordinate selection (e.g., based on feature importance, clustering)\n",
    "        else:\n",
    "            # Default to random if invalid method\n",
    "            coordinates = np.random.randint(0, n_features, size=self.max_iter)\n",
    "            \n",
    "        return coordinates\n",
    "\n",
    "    def _handle_class_imbalance(self, X, y):\n",
    "        \"\"\"Calculate class weights for imbalanced datasets.\"\"\"\n",
    "        unique_classes = np.unique(y)\n",
    "        class_weights = {}\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            class_weights[cls] = len(y) / (len(unique_classes) * np.sum(y == cls))\n",
    "            \n",
    "        # Create sample weights array\n",
    "        sample_weights = np.ones(len(y))\n",
    "        for cls in unique_classes:\n",
    "            sample_weights[y == cls] = class_weights[cls]\n",
    "            \n",
    "        return sample_weights\n",
    "    \n",
    "    def fit(self, X, y, descent_scheme=\"full_batch\", loss_type=\"baseline\", coordinate_choice=\"random\", \n",
    "            handle_imbalance=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the logistic regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        descent_scheme : str, default='full_batch'\n",
    "            Descent method ('full_batch' or 'SGD')\n",
    "        loss_type : str, default='baseline'\n",
    "            Loss function type ('baseline' or 'custom')\n",
    "        coordinate_choice : str, default='random'\n",
    "            How to choose coordinates for SGD ('random' or 'custom')\n",
    "        handle_imbalance : bool, default=True\n",
    "            Whether to weigh samples by inverse class frequency\n",
    "        verbose : bool, default=True\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "\n",
    "        # Initialize weights with small random values\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        n_features = X_normalized.shape[1]\n",
    "        self.weight = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        if handle_imbalance:\n",
    "            sample_weights = self._handle_class_imbalance(X, y)\n",
    "        else:\n",
    "            sample_weights = np.ones(len(y))\n",
    "        \n",
    "        previous_loss = float('inf')\n",
    "        converged = False\n",
    "        \n",
    "        # Choose descent scheme\n",
    "        if descent_scheme == \"full_batch\":\n",
    "            # Full batch gradient descent\n",
    "            for i in range(self.max_iter):\n",
    "                # Compute gradient based on loss type\n",
    "                if loss_type == \"baseline\":\n",
    "                    gradient = self._compute_gradient(X_normalized, y, sample_weights)\n",
    "                else:  # Custom loss\n",
    "                    gradient = self._custom_loss(X_normalized, y, sample_weights)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weight = self.weight - self.lr * gradient\n",
    "                \n",
    "                # Calculate loss for convergence check\n",
    "                current_loss = self._compute_loss(X_normalized, y)\n",
    "                \n",
    "                # Check convergence based on loss improvement\n",
    "                if abs(previous_loss - current_loss) < self.tol:\n",
    "                    if verbose:\n",
    "                        print(f'Converged at iteration {i}, loss: {current_loss:.6f}')\n",
    "                    converged = True\n",
    "                    break\n",
    "                    \n",
    "                previous_loss = current_loss\n",
    "                \n",
    "                # Optional: adaptive learning rate\n",
    "                if i > 0 and i % 50 == 0:\n",
    "                    self.lr *= 0.9  # Reduce learning rate over time\n",
    "                    \n",
    "        elif descent_scheme == \"SGD\":\n",
    "            # Stochastic coordinate descent\n",
    "            # Get coordinate order\n",
    "            coordinates = self._get_coordinates(n_features, method=coordinate_choice, X=X_normalized, y=y)\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                # Get current coordinate\n",
    "                j = coordinates[i % len(coordinates)]\n",
    "                \n",
    "                # Use appropriate gradient calculation for the selected coordinate\n",
    "                if loss_type == \"baseline\":\n",
    "                    gradient_j = self._compute_gradient_for_coordinate(X_normalized, y, j, sample_weights)\n",
    "                else:  # Custom loss\n",
    "                    gradient_j = self._custom_gradient_for_coordinate(X_normalized, y, j, sample_weights)\n",
    "                \n",
    "                # Update only the selected coordinate\n",
    "                self.weight[j] -= self.lr * gradient_j\n",
    "                \n",
    "                # Check convergence every 10*n_features updates\n",
    "                if i % (10 * n_features) == 0:\n",
    "                    current_loss = self._compute_loss(X_normalized, y)\n",
    "                    if abs(previous_loss - current_loss) < self.tol:\n",
    "                        if verbose:\n",
    "                            print(f'Converged at iteration {i}, loss: {current_loss:.6f}')\n",
    "                        converged = True\n",
    "                        break\n",
    "                    previous_loss = current_loss\n",
    "                    \n",
    "                    # Adaptive learning rate\n",
    "                    if i > 0 and i % (50 * n_features) == 0:\n",
    "                        self.lr *= 0.9\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown descent scheme: {descent_scheme}. Use 'full_batch' or 'SGD'.\")\n",
    "        \n",
    "        if not converged and verbose:\n",
    "            print(f'Did not converge after {self.max_iter} iterations. Final loss: {previous_loss:.6f}')\n",
    "            \n",
    "        return self\n",
    "\n",
    "    # For backward compatibility\n",
    "    def fit_stochastic_coordinate_descent(self, X, y, handle_imbalance=True, verbose=True):\n",
    "        \"\"\"Legacy method that calls fit with SGD descent scheme for backward compatibility.\"\"\"\n",
    "        return self.fit(X, y, descent_scheme=\"SGD\", coordinate_choice=\"random\",\n",
    "                       handle_imbalance=handle_imbalance, verbose=verbose)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns the probability predictions.\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "\n",
    "        return self._sigmoid(X_normalized @ self.weight)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predicts class labels (0 or 1) based on a threshold.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int) + 1\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Returns the absolute weights as feature importance.\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            return np.abs(self.weight[1:])\n",
    "        return np.abs(self.weight)  \n",
    "    \n",
    "    @staticmethod\n",
    "    def grid_search(X, y, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=True, \n",
    "                   descent_scheme=\"SGD\", loss_type=\"baseline\", coordinate_choice=\"random\"):\n",
    "       \"\"\"\n",
    "       Perform grid search to find optimal hyperparameters.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       X : array-like, shape (n_samples, n_features)\n",
    "           Training data\n",
    "       y : array-like, shape (n_samples,)\n",
    "           Target values\n",
    "       param_grid : dict\n",
    "           Dictionary with parameter names as keys and lists of parameter values\n",
    "       cv : int, default=5\n",
    "           Number of cross-validation folds\n",
    "       scoring : str, default='accuracy'\n",
    "           Scoring metric ('accuracy', 'precision', 'recall', 'f1', 'auc')\n",
    "       n_jobs : int, default=-1\n",
    "           Number of jobs to run in parallel (-1 means using all processors)\n",
    "       verbose : bool, default=True\n",
    "           Whether to print progress\n",
    "       descent_scheme : str, default='SGD'\n",
    "           Descent method ('full_batch' or 'SGD')\n",
    "       loss_type : str, default='baseline'\n",
    "           Loss function type ('baseline' or 'custom')\n",
    "       coordinate_choice : str, default='random'\n",
    "           How to choose coordinates for SGD ('random' or 'custom')\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       dict : Best parameters and corresponding score\n",
    "       \"\"\"\n",
    "       # Convert inputs to numpy arrays if they're not already\n",
    "       X = np.asarray(X)\n",
    "       y = np.asarray(y)\n",
    "       \n",
    "       # Prepare parameter combinations\n",
    "       param_names = list(param_grid.keys())\n",
    "       param_values = list(param_grid.values())\n",
    "       param_combinations = list(product(*param_values))\n",
    "       \n",
    "       # Split data into folds\n",
    "       n_samples = len(y)\n",
    "       indices = np.arange(n_samples)\n",
    "       np.random.seed(42)  # For reproducible CV splits\n",
    "       np.random.shuffle(indices)\n",
    "       fold_sizes = np.full(cv, n_samples // cv, dtype=int)\n",
    "       fold_sizes[:n_samples % cv] += 1\n",
    "       \n",
    "       current_idx = 0\n",
    "       folds = []\n",
    "       for fold_size in fold_sizes:\n",
    "           fold_indices = indices[current_idx:current_idx + fold_size]\n",
    "           folds.append(fold_indices)\n",
    "           current_idx += fold_size\n",
    "           \n",
    "       # Define evaluation function for a single parameter combination\n",
    "       def evaluate_params(params):\n",
    "           param_dict = {param_names[i]: params[i] for i in range(len(param_names))}\n",
    "           \n",
    "           if verbose:\n",
    "               print(f\"Evaluating parameters: {param_dict}\")\n",
    "           \n",
    "           scores = []\n",
    "           for i in range(cv):\n",
    "               # Split data\n",
    "               test_idx = folds[i]\n",
    "               train_idx = np.concatenate([folds[j] for j in range(cv) if j != i])\n",
    "               \n",
    "               X_train, X_test = X[train_idx], X[test_idx]\n",
    "               y_train, y_test = y[train_idx], y[test_idx]\n",
    "               \n",
    "               # Train model with current parameters\n",
    "               model = LogisticRegression(**param_dict)\n",
    "               \n",
    "               # Use specified descent scheme\n",
    "               model.fit(X_train, y_train, \n",
    "                        descent_scheme=descent_scheme,\n",
    "                        loss_type=loss_type,\n",
    "                        coordinate_choice=coordinate_choice,\n",
    "                        verbose=False)\n",
    "               \n",
    "               # Evaluate based on scoring metric\n",
    "               if scoring == 'accuracy':\n",
    "                   score = model.score(X_test, y_test)\n",
    "               elif scoring == 'precision':\n",
    "                   y_pred = model.predict(X_test)\n",
    "                   score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "               elif scoring == 'recall':\n",
    "                   y_pred = model.predict(X_test)\n",
    "                   score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "               elif scoring == 'f1':\n",
    "                   y_pred = model.predict(X_test)\n",
    "                   precision = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "                   recall = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "                   score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "               elif scoring == 'auc':\n",
    "                   try:\n",
    "                       from sklearn.metrics import roc_auc_score\n",
    "                       y_prob = model.predict_proba(X_test)\n",
    "                       score = roc_auc_score(y_test == 2, y_prob)\n",
    "                   except ImportError:\n",
    "                       # Fallback if sklearn is not available\n",
    "                       y_prob = model.predict_proba(X_test)\n",
    "                       # Manual AUC calculation (simplified)\n",
    "                       pos_scores = y_prob[y_test == 2]\n",
    "                       neg_scores = y_prob[y_test != 2]\n",
    "                       if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "                           score = 0.5\n",
    "                       else:\n",
    "                           n_pos = len(pos_scores)\n",
    "                           n_neg = len(neg_scores)\n",
    "                           score = sum(pos > neg for pos in pos_scores for neg in neg_scores) / (n_pos * n_neg)\n",
    "               else:\n",
    "                   raise ValueError(f\"Unknown scoring metric: {scoring}\")\n",
    "               \n",
    "               scores.append(score)\n",
    "           \n",
    "           mean_score = np.mean(scores)\n",
    "           if verbose:\n",
    "               print(f\"Parameters {param_dict} - Mean {scoring}: {mean_score:.4f}\")\n",
    "           \n",
    "           return param_dict, mean_score\n",
    "       \n",
    "       # For small grid sizes or when parallelism causes issues, run sequentially\n",
    "       if len(param_combinations) <= 4 or n_jobs == 1:\n",
    "           results = [evaluate_params(params) for params in param_combinations]\n",
    "       else:\n",
    "           # Run evaluations in parallel\n",
    "           try:\n",
    "               n_jobs = n_jobs if n_jobs > 0 else multiprocessing.cpu_count()\n",
    "               results = Parallel(n_jobs=n_jobs)(\n",
    "                   delayed(evaluate_params)(params) for params in param_combinations\n",
    "               )\n",
    "           except Exception as e:\n",
    "               if verbose:\n",
    "                   print(f\"Parallel execution failed with error: {str(e)}\")\n",
    "                   print(\"Falling back to sequential execution...\")\n",
    "               results = [evaluate_params(params) for params in param_combinations]\n",
    "       \n",
    "       # Find best parameters\n",
    "       best_params, best_score = max(results, key=lambda x: x[1])\n",
    "       \n",
    "       if verbose:\n",
    "           opt_method = f\"{descent_scheme} with {coordinate_choice} coordinate selection\"\n",
    "           print(f\"\\nGrid Search Results (using {opt_method}):\")\n",
    "           print(f\"Best parameters: {best_params}\")\n",
    "           print(f\"Best {scoring}: {best_score:.4f}\")\n",
    "       \n",
    "       return {\n",
    "           'best_params': best_params,\n",
    "           'best_score': best_score,\n",
    "           'all_results': results\n",
    "       }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:17.995482Z",
     "start_time": "2025-02-24T20:14:17.940549Z"
    }
   },
   "id": "a2c657a8dc411921",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution failed with error: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "Falling back to sequential execution...\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.6615\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.6615\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.6615\n",
      "Evaluating parameters: {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.1, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.6538\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.7385\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.7385\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.7385\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.7385\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000} - Mean accuracy: 0.7846\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.01, 'max_iter': 1000} - Mean accuracy: 0.7846\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.7846\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.7846\n",
      "\n",
      "Grid Search Results (using SGD with random coordinate selection):\n",
      "Best parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Best accuracy: 0.7846\n",
      "Did not converge after 1000 iterations. Final loss: 0.054192\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.LogisticRegression at 0x1467c3f40>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'reg_lambda': [0.0, 0.01, 0.1, 1.0],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = LogisticRegression.grid_search(X, y, param_grid, verbose=True)\n",
    "\n",
    "# Create model with best parameters\n",
    "best_model = LogisticRegression(**results['best_params'])\n",
    "best_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:19.236914Z",
     "start_time": "2025-02-24T20:14:17.977578Z"
    }
   },
   "id": "188ffcd897f4717",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Baseline Logistic Regression Accuracy: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Evaluate performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Custom Baseline Logistic Regression Accuracy: {accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:19.240900Z",
     "start_time": "2025-02-24T20:14:19.234805Z"
    }
   },
   "id": "86f606e51141a5d6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 1 2 1 1 2 2 2 2 1 1 2 2 2 2 2 1]\n",
      "[1 2 1 1 1 2 1 1 2 1 2 2 1 1 2 2 2 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:20])\n",
    "print(y_test[:20].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:19.248781Z",
     "start_time": "2025-02-24T20:14:19.243660Z"
    }
   },
   "id": "88e9743176381ab8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:14:19.250166Z",
     "start_time": "2025-02-24T20:14:19.247411Z"
    }
   },
   "id": "a22005022a497800",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
