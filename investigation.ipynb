{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "get and format dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81dcf1a8e907311"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "0      1    14.23       1.71  2.43               15.6        127   \n",
      "1      1    13.20       1.78  2.14               11.2        100   \n",
      "2      1    13.16       2.36  2.67               18.6        101   \n",
      "3      1    14.37       1.95  2.50               16.8        113   \n",
      "4      1    13.24       2.59  2.87               21.0        118   \n",
      "\n",
      "   Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# load in wine \n",
    "import pandas as pd\n",
    "\n",
    "# Define column names\n",
    "column_names = [\n",
    "    \"Class\", \"Alcohol\", \"Malicacid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\",\n",
    "    \"Total_phenols\", \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\",\n",
    "    \"Color_intensity\", \"Hue\", \"OD280_OD315_of_diluted_wines\", \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"wine/wine.data\", names=column_names)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.845062Z",
     "start_time": "2025-02-25T04:43:33.808935Z"
    }
   },
   "id": "initial_id",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "125      2    12.07       2.16  2.17               21.0         85   \n",
      "126      2    12.43       1.53  2.29               21.5         86   \n",
      "127      2    11.79       2.13  2.78               28.5         92   \n",
      "128      2    12.37       1.63  2.30               24.5         88   \n",
      "129      2    12.04       4.30  2.38               22.0         80   \n",
      "\n",
      "     Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "125           2.60        2.65                  0.37             1.35   \n",
      "126           2.74        3.15                  0.39             1.77   \n",
      "127           2.13        2.24                  0.58             1.76   \n",
      "128           2.22        2.45                  0.40             1.90   \n",
      "129           2.10        1.75                  0.42             1.35   \n",
      "\n",
      "     Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "125             2.76  0.86                          3.28      378  \n",
      "126             3.94  0.69                          2.84      352  \n",
      "127             3.00  0.97                          2.44      466  \n",
      "128             2.12  0.89                          2.78      342  \n",
      "129             2.60  0.79                          2.57      580  \n"
     ]
    }
   ],
   "source": [
    "# Remove rows where Class is 3\n",
    "df = df[df[\"Class\"] != 3]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.tail())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.846901Z",
     "start_time": "2025-02-25T04:43:33.840011Z"
    }
   },
   "id": "2d8045463ae6a81c",
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement a logistic regression baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5225897f44a5679"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.847610Z",
     "start_time": "2025-02-25T04:43:33.842448Z"
    }
   },
   "id": "118114fbb2dd5fd0",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop(columns=['Class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.858845Z",
     "start_time": "2025-02-25T04:43:33.853210Z"
    }
   },
   "id": "7fd4db81cc7da185",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=700)",
      "text/html": "<style>#sk-container-id-4 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-4 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-4 pre {\n  padding: 0;\n}\n\n#sk-container-id-4 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-4 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-4 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-4 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-4 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-4 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-4 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-4 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-4 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-4 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-4 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n#sk-container-id-4 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-4 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-4 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-4 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-4 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-4 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-4 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=700)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=700)</pre></div> </div></div></div></div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=700)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.922080Z",
     "start_time": "2025-02-25T04:43:33.865646Z"
    }
   },
   "id": "dd62d0b2c944a9e3",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Baseline Logistic Regression Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.928354Z",
     "start_time": "2025-02-25T04:43:33.921970Z"
    }
   },
   "id": "258146464d8cdf32",
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we specify a custom method of selecting the next coordinate. For this, lets create a function that takes in X and y, and returns an array specifying the order we want to test in. \n",
    "\n",
    "Solution idea: Cluster the data by features, and then rotate data points from each cluster. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "391738e96fb12e42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.958609Z",
     "start_time": "2025-02-25T04:43:33.929968Z"
    }
   },
   "id": "b1a67f2179b19057",
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need a custom class so we can directly modify the loss function... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d419fb738aa61fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, max_iter=2000, tol=1e-4, fit_intercept=True, reg_lambda=0.01):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol  # Tolerance for weight convergence\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weight = None\n",
    "        self.feature_means = None\n",
    "        self.feature_stds = None\n",
    "        self.convergence_window = 10  # Number of iterations to check for weight stability\n",
    "\n",
    "    def _normalize_features(self, X):\n",
    "        \"\"\"Normalizes features using z-score standardization.\"\"\"\n",
    "        if self.feature_means is None or self.feature_stds is None:\n",
    "            self.feature_means = np.mean(X, axis=0)\n",
    "            self.feature_stds = np.std(X, axis=0) + 1e-8  # Avoid division by zero\n",
    "        \n",
    "        return (X - self.feature_means) / self.feature_stds\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Adds intercept term (bias) to feature matrix.\"\"\"\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip values to prevent overflow\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return expit(z)  # Numerically stable sigmoid\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"Computes the binary cross-entropy loss with regularization.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Add L2 regularization term (excluding bias)\n",
    "        reg_term = 0.5 * self.reg_lambda * np.sum(self.weight[1:] ** 2) / m\n",
    "        \n",
    "        # Compute binary cross-entropy\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        \n",
    "        return loss + reg_term\n",
    "    \n",
    "    def _compute_gradient_for_coordinate(self, X, y, j, sample_indices=None, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Computes the gradient for a single coordinate j using only the specified samples.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        j : int\n",
    "            Index of the coordinate to compute gradient for\n",
    "        sample_indices : array-like or None\n",
    "            Indices of samples to use for gradient computation. If None, use all samples.\n",
    "        sample_weights : array-like or None\n",
    "            Weights for each sample. If None, equal weights are used.\n",
    "        \"\"\"\n",
    "        if sample_indices is not None:\n",
    "            X_batch = X[sample_indices]\n",
    "            y_batch = y[sample_indices]\n",
    "            if sample_weights is not None:\n",
    "                weights_batch = sample_weights[sample_indices]\n",
    "            else:\n",
    "                weights_batch = None\n",
    "        else:\n",
    "            X_batch = X\n",
    "            y_batch = y\n",
    "            weights_batch = sample_weights\n",
    "        \n",
    "        m = len(y_batch)\n",
    "        h = self._sigmoid(X_batch @ self.weight)\n",
    "        \n",
    "        if weights_batch is None:\n",
    "            weights_batch = np.ones(m)\n",
    "            \n",
    "        # Gradient for single coordinate using only the batch\n",
    "        gradient_j = np.sum((h - y_batch) * weights_batch * X_batch[:, j]) / np.sum(weights_batch)\n",
    "        \n",
    "        # Add L2 regularization (except for bias term)\n",
    "        if j > 0 or not self.fit_intercept:\n",
    "            gradient_j += self.reg_lambda * self.weight[j] / m\n",
    "            \n",
    "        return gradient_j\n",
    "    \n",
    "    def _custom_gradient_for_coordinate(self, X, y, j, sample_indices=None, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Custom gradient computation for a single coordinate.\n",
    "        This implements a focal loss variant that puts more emphasis on hard examples.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        j : int\n",
    "            Index of the coordinate to compute gradient for\n",
    "        sample_indices : array-like or None\n",
    "            Indices of samples to use for gradient computation\n",
    "        sample_weights : array-like or None\n",
    "            Weights for each sample\n",
    "        \"\"\"\n",
    "        if sample_indices is not None:\n",
    "            X_batch = X[sample_indices]\n",
    "            y_batch = y[sample_indices]\n",
    "            if sample_weights is not None:\n",
    "                weights_batch = sample_weights[sample_indices]\n",
    "            else:\n",
    "                weights_batch = None\n",
    "        else:\n",
    "            X_batch = X\n",
    "            y_batch = y\n",
    "            weights_batch = sample_weights\n",
    "        \n",
    "        m = len(y_batch)\n",
    "        \n",
    "        # Convert labels from 1/2 to 0/1 for easier calculation\n",
    "        y_binary = (y_batch == 2).astype(int)\n",
    "        \n",
    "        # Get current predictions\n",
    "        h = self._sigmoid(X_batch @ self.weight)\n",
    "        \n",
    "        # Calculate errors with focal loss modulation\n",
    "        # Focal loss puts more weight on hard examples\n",
    "        gamma = 2.0  # Focusing parameter\n",
    "        \n",
    "        # pt is the probability of the true class\n",
    "        pt = h * y_binary + (1 - h) * (1 - y_binary)\n",
    "        # Focal weight is (1-pt)^gamma\n",
    "        focal_weights = np.power(1 - pt, gamma)\n",
    "        \n",
    "        if weights_batch is None:\n",
    "            weights_batch = np.ones(m)\n",
    "            \n",
    "        # Combine focal weights with sample weights\n",
    "        combined_weights = weights_batch * focal_weights\n",
    "        \n",
    "        # Gradient for single coordinate using focal loss\n",
    "        errors = h - y_binary\n",
    "        gradient_j = np.sum(errors * combined_weights * X_batch[:, j]) / np.sum(combined_weights)\n",
    "        \n",
    "        # Add L2 regularization (except for bias term)\n",
    "        if j > 0 or not self.fit_intercept:\n",
    "            gradient_j += self.reg_lambda * self.weight[j] / m\n",
    "            \n",
    "        return gradient_j\n",
    "\n",
    "    def _get_coordinate(self, X, y, n_features, method=\"random\", sample_weights=None):\n",
    "        \"\"\"\n",
    "        Determine the next coordinate to update for stochastic coordinate descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target values\n",
    "        n_features : int\n",
    "            Number of features\n",
    "        method : str\n",
    "            Method to select coordinates (\"random\", \"gradient_based\", \"cyclic\", \n",
    "            \"cyclic_momentum\", \"importance_sampling\", \"greedy_subset\", \"adaptive_frequency\")\n",
    "        sample_weights : array-like or None\n",
    "            Weights for each sample\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        int: Index of the feature (coordinate) to update\n",
    "        \"\"\"\n",
    "        if method == \"random\":\n",
    "            # Randomly select a coordinate\n",
    "            return np.random.randint(0, n_features)\n",
    "        \n",
    "        elif method == \"gradient_based\":\n",
    "            # Select coordinate based on gradient magnitude\n",
    "            gradients = np.zeros(n_features)\n",
    "            for j in range(n_features):\n",
    "                gradients[j] = abs(self._compute_gradient_for_coordinate(X, y, j, sample_weights=sample_weights))\n",
    "            \n",
    "            # Select coordinate with highest gradient magnitude\n",
    "            return np.argmax(gradients)\n",
    "        \n",
    "        elif method == \"cyclic\":\n",
    "            # Use the class attribute to keep track of the current coordinate\n",
    "            if not hasattr(self, '_current_coordinate'):\n",
    "                self._current_coordinate = 0\n",
    "            else:\n",
    "                self._current_coordinate = (self._current_coordinate + 1) % n_features\n",
    "            \n",
    "            return self._current_coordinate\n",
    "        \n",
    "        elif method == \"cyclic_momentum\":\n",
    "            # Cyclic coordinate descent with momentum\n",
    "            if not hasattr(self, '_current_coordinate'):\n",
    "                self._current_coordinate = 0\n",
    "                self._momentum = np.zeros(n_features)\n",
    "            else:\n",
    "                self._current_coordinate = (self._current_coordinate + 1) % n_features\n",
    "            \n",
    "            # Store current gradient for momentum calculation in the update step\n",
    "            current_grad = self._compute_gradient_for_coordinate(X, y, self._current_coordinate, sample_weights=sample_weights)\n",
    "            \n",
    "            # Add momentum to the update (will be used in fit method)\n",
    "            momentum_beta = 0.9\n",
    "            if not hasattr(self, '_gradient_cache'):\n",
    "                self._gradient_cache = {}\n",
    "            self._gradient_cache[self._current_coordinate] = current_grad\n",
    "            \n",
    "            if not hasattr(self, '_momentum'):\n",
    "                self._momentum = np.zeros(n_features)\n",
    "            \n",
    "            self._momentum[self._current_coordinate] = momentum_beta * self._momentum[self._current_coordinate] + \\\n",
    "                (1 - momentum_beta) * current_grad\n",
    "            \n",
    "            return self._current_coordinate\n",
    "        \n",
    "        elif method == \"importance_sampling\":\n",
    "            # Select coordinates with probability proportional to historical gradient magnitudes\n",
    "            if not hasattr(self, '_gradient_history'):\n",
    "                self._gradient_history = np.ones(n_features)\n",
    "            \n",
    "            # Get a sample of gradients to update history\n",
    "            sample_size = min(5, n_features)\n",
    "            sample_coords = np.random.choice(n_features, sample_size, replace=False)\n",
    "            \n",
    "            for j in sample_coords:\n",
    "                grad_j = abs(self._compute_gradient_for_coordinate(X, y, j, sample_weights=sample_weights))\n",
    "                self._gradient_history[j] = 0.9 * self._gradient_history[j] + 0.1 * grad_j\n",
    "            \n",
    "            # Add small constant to avoid zero probabilities\n",
    "            probs = self._gradient_history + 1e-5\n",
    "            probs = probs / np.sum(probs)\n",
    "            \n",
    "            # Sample coordinate based on probabilities\n",
    "            coordinate = np.random.choice(n_features, p=probs)\n",
    "            \n",
    "            return coordinate\n",
    "        \n",
    "        elif method == \"greedy_subset\":\n",
    "            # Compute gradients for a small random subset and pick largest\n",
    "            subset_size = min(5, n_features)\n",
    "            coordinate_subset = np.random.choice(n_features, subset_size, replace=False)\n",
    "            \n",
    "            max_grad = 0\n",
    "            best_coordinate = coordinate_subset[0]  # Default to first in case all gradients are 0\n",
    "            \n",
    "            for j in coordinate_subset:\n",
    "                grad_j = abs(self._compute_gradient_for_coordinate(X, y, j, sample_weights=sample_weights))\n",
    "                if grad_j > max_grad:\n",
    "                    max_grad = grad_j\n",
    "                    best_coordinate = j\n",
    "                    \n",
    "            return best_coordinate\n",
    "        \n",
    "        elif method == \"adaptive_frequency\":\n",
    "            # Update more frequently coordinates with historically larger gradient changes\n",
    "            if not hasattr(self, '_update_frequencies'):\n",
    "                self._update_frequencies = np.ones(n_features)\n",
    "                self._last_gradients = np.zeros(n_features)\n",
    "            \n",
    "            # Sample a coordinate to evaluate\n",
    "            if np.random.random() < 0.1:  # Exploration rate\n",
    "                coordinate = np.random.randint(0, n_features)\n",
    "            else:\n",
    "                # Normalize frequencies to get probabilities\n",
    "                scores = self._update_frequencies / np.sum(self._update_frequencies)\n",
    "                coordinate = np.random.choice(n_features, p=scores)\n",
    "            \n",
    "            # Calculate current gradient for this coordinate\n",
    "            current_grad = self._compute_gradient_for_coordinate(X, y, coordinate, sample_weights=sample_weights)\n",
    "            \n",
    "            # Update frequencies based on gradient changes\n",
    "            grad_change = abs(current_grad - self._last_gradients[coordinate])\n",
    "            self._update_frequencies[coordinate] *= (1.0 + grad_change)\n",
    "            self._last_gradients[coordinate] = current_grad\n",
    "            \n",
    "            return coordinate\n",
    "        \n",
    "        else:\n",
    "            # Default to random if invalid method\n",
    "            return np.random.randint(0, n_features)\n",
    "    \n",
    "    def _handle_class_imbalance(self, X, y):\n",
    "        \"\"\"Calculate class weights for imbalanced datasets.\"\"\"\n",
    "        unique_classes = np.unique(y)\n",
    "        class_weights = {}\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            class_weights[cls] = len(y) / (len(unique_classes) * np.sum(y == cls))\n",
    "            \n",
    "        # Create sample weights array\n",
    "        sample_weights = np.ones(len(y))\n",
    "        for cls in unique_classes:\n",
    "            sample_weights[y == cls] = class_weights[cls]\n",
    "            \n",
    "        return sample_weights\n",
    "    \n",
    "    def fit(self, X, y, loss_type=\"baseline\", coordinate_choice=\"random\", \n",
    "        batch_size=1, handle_imbalance=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the logistic regression model using stochastic coordinate descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        loss_type : str, default='baseline'\n",
    "            Loss function type ('baseline' or 'custom')\n",
    "        coordinate_choice : str, default='random'\n",
    "            How to choose coordinates for SGD ('random', 'gradient_based', or 'cyclic')\n",
    "        batch_size : int, default=1\n",
    "            Number of samples to use in each update (true SGD uses 1)\n",
    "        handle_imbalance : bool, default=True\n",
    "            Whether to weigh samples by inverse class frequency\n",
    "        verbose : bool, default=True\n",
    "            Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracies : array-like\n",
    "            Array of accuracy values during training\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "    \n",
    "        # Initialize weights with small random values\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        n_features = X_normalized.shape[1]\n",
    "        self.weight = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        if handle_imbalance:\n",
    "            sample_weights = self._handle_class_imbalance(X, y)\n",
    "        else:\n",
    "            sample_weights = np.ones(len(y))\n",
    "        \n",
    "        # Store weight history for convergence check\n",
    "        weight_history = [np.copy(self.weight)]\n",
    "        converged = False\n",
    "        \n",
    "        # Array to track accuracies during training\n",
    "        accuracies = []\n",
    "        check_interval = max(1, self.max_iter // 100)  # Check accuracy ~100 times\n",
    "        \n",
    "        min_updates_per_coordinate = 3  # Ensure each coordinate has been updated enough times\n",
    "        coordinate_update_counts = np.zeros(n_features)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Get the next coordinate\n",
    "            j = self._get_coordinate(\n",
    "                X_normalized, y, n_features, method=coordinate_choice, sample_weights=sample_weights\n",
    "            )\n",
    "            \n",
    "            # For true SGD, use a single random sample\n",
    "            if batch_size == 1:\n",
    "                batch_indices = [np.random.choice(n_samples)]\n",
    "            else:\n",
    "                batch_indices = np.random.choice(n_samples, size=batch_size)\n",
    "            \n",
    "            # Track coordinate updates\n",
    "            coordinate_update_counts[j] += 1\n",
    "            \n",
    "            # Use appropriate gradient calculation for the selected coordinate\n",
    "            if loss_type == \"baseline\":\n",
    "                gradient_j = self._compute_gradient_for_coordinate(\n",
    "                    X_normalized, y, j, batch_indices, sample_weights\n",
    "                )\n",
    "            else:  # Custom loss\n",
    "                gradient_j = self._custom_gradient_for_coordinate(\n",
    "                    X_normalized, y, j, batch_indices, sample_weights\n",
    "                )\n",
    "            \n",
    "            # Update only the selected coordinate\n",
    "            self.weight[j] -= self.lr * gradient_j\n",
    "            \n",
    "            # Check convergence and track accuracy periodically\n",
    "            if i % check_interval == 0 or i == self.max_iter - 1:\n",
    "                # Store current weights for convergence check\n",
    "                weight_history.append(np.copy(self.weight))\n",
    "                \n",
    "                # Keep only the most recent weights for convergence check\n",
    "                if len(weight_history) > self.convergence_window:\n",
    "                    weight_history.pop(0)\n",
    "                \n",
    "                # Calculate current accuracy\n",
    "                y_pred = (self._sigmoid(X_normalized @ self.weight) >= 0.5).astype(int) + 1\n",
    "                current_accuracy = np.mean(y_pred == y)\n",
    "                accuracies.append(current_accuracy)\n",
    "                \n",
    "                # Check for weight convergence if we have enough history\n",
    "                if len(weight_history) == self.convergence_window:\n",
    "                    # Check if all coordinates have been updated enough times\n",
    "                    if np.all(coordinate_update_counts >= min_updates_per_coordinate):\n",
    "                        # Calculate max weight change across the window and all coordinates\n",
    "                        weight_changes = []\n",
    "                        for w_idx in range(1, len(weight_history)):\n",
    "                            # Calculate absolute change in weights\n",
    "                            weight_diff = np.abs(weight_history[w_idx] - weight_history[w_idx-1])\n",
    "                            # Get maximum change for any coordinate\n",
    "                            weight_changes.append(np.max(weight_diff))\n",
    "                        \n",
    "                        # Check if max weight change is less than tolerance\n",
    "                        if np.max(weight_changes) < self.tol:\n",
    "                            if verbose:\n",
    "                                print(f'Converged at iteration {i}, weights stabilized with max change: {np.max(weight_changes):.6f}, accuracy: {current_accuracy:.4f}')\n",
    "                            converged = True\n",
    "                            # Fill the rest of the accuracy array with the final value\n",
    "                            accuracies.extend([current_accuracy] * ((self.max_iter // check_interval) - len(accuracies)))\n",
    "                            break\n",
    "                \n",
    "                # Adaptive learning rate\n",
    "                if i > 0 and i % (5 * n_features) == 0:\n",
    "                    self.lr *= 0.90\n",
    "        \n",
    "        if not converged and verbose:\n",
    "            print(f'Did not converge after {self.max_iter} iterations. Final accuracy: {current_accuracy:.4f}')\n",
    "            \n",
    "        # Make sure we return exactly 100 points for easier plotting\n",
    "        if len(accuracies) < 100:\n",
    "            # Repeat last value to fill\n",
    "            accuracies.extend([accuracies[-1]] * (100 - len(accuracies)))\n",
    "        elif len(accuracies) > 100:\n",
    "            # Downsample by taking evenly spaced points\n",
    "            indices = np.linspace(0, len(accuracies)-1, 100, dtype=int)\n",
    "            accuracies = [accuracies[i] for i in indices]\n",
    "            \n",
    "        return np.array(accuracies)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns the probability predictions.\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "\n",
    "        return self._sigmoid(X_normalized @ self.weight)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predicts class labels (0 or 1) based on a threshold.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int) + 1\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Returns the absolute weights as feature importance.\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            return np.abs(self.weight[1:])\n",
    "        return np.abs(self.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.972231Z",
     "start_time": "2025-02-25T04:43:33.934710Z"
    }
   },
   "id": "a2c657a8dc411921",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def grid_search(X, y, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=True, \n",
    "               loss_type=\"baseline\", coordinate_choice=\"random\"):\n",
    "    \"\"\"\n",
    "    Perform grid search to find optimal hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    param_grid : dict\n",
    "        Dictionary with parameter names as keys and lists of parameter values\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    scoring : str, default='accuracy'\n",
    "        Scoring metric ('accuracy', 'precision', 'recall', 'f1', 'auc')\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel (-1 means using all processors)\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress\n",
    "    loss_type : str, default='baseline'\n",
    "        Loss function type ('baseline' or 'custom')\n",
    "    coordinate_choice : str, default='random'\n",
    "        How to choose coordinates for SGD ('random', 'gradient_based', or 'cyclic')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Best parameters, corresponding score, and convergence curves\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays if they're not already\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Prepare parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(product(*param_values))\n",
    "    \n",
    "    # Split data into folds\n",
    "    n_samples = len(y)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.seed(42)  # For reproducible CV splits\n",
    "    np.random.shuffle(indices)\n",
    "    fold_sizes = np.full(cv, n_samples // cv, dtype=int)\n",
    "    fold_sizes[:n_samples % cv] += 1\n",
    "    \n",
    "    current_idx = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        fold_indices = indices[current_idx:current_idx + fold_size]\n",
    "        folds.append(fold_indices)\n",
    "        current_idx += fold_size\n",
    "        \n",
    "    # Define evaluation function for a single parameter combination\n",
    "    def evaluate_params(params):\n",
    "        param_dict = {param_names[i]: params[i] for i in range(len(param_names))}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Evaluating parameters: {param_dict}\")\n",
    "        \n",
    "        scores = []\n",
    "        convergence_curves = []\n",
    "        \n",
    "        for i in range(cv):\n",
    "            # Split data\n",
    "            test_idx = folds[i]\n",
    "            train_idx = np.concatenate([folds[j] for j in range(cv) if j != i])\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Train model with current parameters\n",
    "            model = LogisticRegression(**param_dict)\n",
    "            \n",
    "            # Use specified coordinate selection and loss type\n",
    "            # Store accuracy history from fit method\n",
    "            accuracy_history = model.fit(\n",
    "                X_train, y_train,\n",
    "                loss_type=loss_type,\n",
    "                coordinate_choice=coordinate_choice,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Store convergence curve for this fold\n",
    "            convergence_curves.append(accuracy_history)\n",
    "            \n",
    "            # Evaluate based on scoring metric\n",
    "            if scoring == 'accuracy':\n",
    "                score = model.score(X_test, y_test)\n",
    "            elif scoring == 'precision':\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "            elif scoring == 'recall':\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "            elif scoring == 'f1':\n",
    "                y_pred = model.predict(X_test)\n",
    "                precision = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "                recall = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "                score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            elif scoring == 'auc':\n",
    "                try:\n",
    "                    from sklearn.metrics import roc_auc_score\n",
    "                    y_prob = model.predict_proba(X_test)\n",
    "                    score = roc_auc_score(y_test == 2, y_prob)\n",
    "                except ImportError:\n",
    "                    # Fallback if sklearn is not available\n",
    "                    y_prob = model.predict_proba(X_test)\n",
    "                    # Manual AUC calculation (simplified)\n",
    "                    pos_scores = y_prob[y_test == 2]\n",
    "                    neg_scores = y_prob[y_test != 2]\n",
    "                    if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "                        score = 0.5\n",
    "                    else:\n",
    "                        n_pos = len(pos_scores)\n",
    "                        n_neg = len(neg_scores)\n",
    "                        score = sum(pos > neg for pos in pos_scores for neg in neg_scores) / (n_pos * n_neg)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown scoring metric: {scoring}\")\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        if verbose:\n",
    "            print(f\"Parameters {param_dict} - Mean {scoring}: {mean_score:.4f}\")\n",
    "            # Check if model converged in all folds\n",
    "            avg_iterations = np.mean([len(np.unique(curve)) for curve in convergence_curves])\n",
    "            print(f\"Average unique accuracy points: {avg_iterations:.1f}/100 (higher suggests better convergence)\")\n",
    "        \n",
    "        # Average convergence curve across folds\n",
    "        avg_convergence = np.mean(convergence_curves, axis=0)\n",
    "        \n",
    "        return param_dict, mean_score, avg_convergence\n",
    "    \n",
    "    # For small grid sizes or when parallelism causes issues, run sequentially\n",
    "    if len(param_combinations) <= 4 or n_jobs == 1:\n",
    "        results = [evaluate_params(params) for params in param_combinations]\n",
    "    else:\n",
    "        # Run evaluations in parallel\n",
    "        try:\n",
    "            n_jobs = n_jobs if n_jobs > 0 else multiprocessing.cpu_count()\n",
    "            results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(evaluate_params)(params) for params in param_combinations\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Parallel execution failed with error: {str(e)}\")\n",
    "                print(\"Falling back to sequential execution...\")\n",
    "            results = [evaluate_params(params) for params in param_combinations]\n",
    "    \n",
    "    # Find best parameters\n",
    "    best_params, best_score, best_convergence = max(results, key=lambda x: x[1])\n",
    "    \n",
    "    if verbose:\n",
    "        opt_method = f\"Coordinate descent with {coordinate_choice} selection\"\n",
    "        print(f\"\\nGrid Search Results (using {opt_method}):\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best {scoring}: {best_score:.4f}\")\n",
    "        \n",
    "        # Calculate iterations until convergence for best model\n",
    "        # Find point where accuracy stabilizes (change < 0.001)\n",
    "        diffs = np.abs(np.diff(best_convergence))\n",
    "        stable_idx = np.argmax(diffs < 0.001)\n",
    "        if stable_idx > 0:\n",
    "            print(f\"Best model converged after ~{stable_idx} iterations\")\n",
    "        else:\n",
    "            print(\"Best model may not have fully converged\")\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'best_convergence': best_convergence,\n",
    "        'all_results': [(params, score) for params, score, _ in results],\n",
    "        'convergence_curves': {str(params): curve for params, _, curve in results}\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:33.989142Z",
     "start_time": "2025-02-25T04:43:33.986360Z"
    }
   },
   "id": "39ea7861fad1eb60",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fullBreakdown(X, y, param_grid=None, cv=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Runs a comprehensive analysis of the LogisticRegression model:\n",
    "    1. Finds optimal hyperparameters using grid search\n",
    "    2. Compares baseline vs custom loss with random coordinate selection\n",
    "    3. Compares different coordinate selection methods with the custom loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    y : array-like\n",
    "        Target values\n",
    "    param_grid : dict, optional\n",
    "        Parameter grid for grid search (default provides a basic grid)\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    n_jobs : int, default=-1\n",
    "        Number of parallel jobs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Results of the analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert inputs to numpy arrays\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Default parameter grid if none provided\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'lr': [0.001, 0.01, 0.1],\n",
    "            'max_iter': [1000],\n",
    "            'reg_lambda': [0.001, 0.01, 0.1],\n",
    "            'fit_intercept': [True]\n",
    "        }\n",
    "    \n",
    "    print(\"Phase 1: Finding optimal hyperparameters...\")\n",
    "    grid_results = grid_search(\n",
    "        X_train, y_train, \n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        loss_type=\"baseline\",\n",
    "        coordinate_choice=\"random\"\n",
    "    )\n",
    "    \n",
    "    best_params = grid_results['best_params']\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    \n",
    "    # Fix max_iter to a larger value for convergence tracking\n",
    "    best_params['max_iter'] = 1000\n",
    "    \n",
    "    # Phase 2: Compare baseline vs custom loss\n",
    "    print(\"\\nPhase 2: Comparing baseline vs custom loss with random coordinate selection...\")\n",
    "    baseline_model = LogisticRegression(**best_params)\n",
    "    baseline_accuracies = baseline_model.fit(X_train, y_train, loss_type=\"baseline\", \n",
    "                                           coordinate_choice=\"random\", verbose=False)\n",
    "    \n",
    "    custom_model = LogisticRegression(**best_params)\n",
    "    custom_accuracies = custom_model.fit(X_train, y_train, loss_type=\"custom\", \n",
    "                                        coordinate_choice=\"random\", verbose=False)\n",
    "    \n",
    "    # Phase 3: Compare all coordinate selection methods\n",
    "    print(\"\\nPhase 3: Comparing all coordinate selection methods with custom loss...\")\n",
    "    \n",
    "    # Already have random model from Phase 2\n",
    "    random_model = custom_model\n",
    "    random_accuracies = custom_accuracies\n",
    "    \n",
    "    # Train models with all coordinate selection methods\n",
    "    coordinate_methods = [\"gradient_based\", \"cyclic\", \"cyclic_momentum\", \n",
    "                         \"importance_sampling\", \"greedy_subset\", \"adaptive_frequency\"]\n",
    "    \n",
    "    models = {}\n",
    "    accuracies = {}\n",
    "    test_accuracies = {}\n",
    "    \n",
    "    # Random and baseline already calculated\n",
    "    models[\"random\"] = random_model\n",
    "    accuracies[\"random\"] = random_accuracies\n",
    "    test_accuracies[\"random\"] = random_model.score(X_test, y_test)\n",
    "    \n",
    "    # Train for each coordinate method\n",
    "    for method in coordinate_methods:\n",
    "        print(f\"  Training with {method} coordinate selection...\")\n",
    "        model = LogisticRegression(**best_params)\n",
    "        acc = model.fit(X_train, y_train, loss_type=\"custom\", \n",
    "                      coordinate_choice=method, verbose=False)\n",
    "        \n",
    "        models[method] = model\n",
    "        accuracies[method] = acc\n",
    "        test_accuracies[method] = model.score(X_test, y_test)\n",
    "    \n",
    "    # Also train baseline model with gradient-based coordinate selection for comparison\n",
    "    baseline_gradient_model = LogisticRegression(**best_params)\n",
    "    baseline_gradient_accuracies = baseline_gradient_model.fit(X_train, y_train, loss_type=\"baseline\", coordinate_choice=\"gradient_based\", verbose=False)\n",
    "    \n",
    "    # Print test accuracies\n",
    "    print(f\"\\nTest accuracies:\")\n",
    "    print(f\"Baseline loss with random coord: {baseline_model.score(X_test, y_test):.4f}\")\n",
    "    print(f\"Custom loss with random coord: {test_accuracies['random']:.4f}\")\n",
    "    \n",
    "    for method in coordinate_methods:\n",
    "        print(f\"Custom loss with {method} coord: {test_accuracies[method]:.4f}\")\n",
    "    \n",
    "    print(f\"Baseline loss with gradient-based coord: {baseline_gradient_model.score(X_test, y_test):.4f}\")\n",
    "    \n",
    "    # Create and save separate figures\n",
    "    \n",
    "    # Figure 1: Loss comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    iterations = np.arange(len(baseline_accuracies))\n",
    "    plt.plot(iterations, baseline_accuracies, label='Baseline Loss')\n",
    "    plt.plot(iterations, custom_accuracies, label='Custom Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Loss Function Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_function_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Figure 2: Coordinate selection comparison with all methods\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot for random method (already calculated)\n",
    "    plt.plot(iterations, accuracies[\"random\"], label='Random', linewidth=2)\n",
    "    \n",
    "    # Colors for better visualization\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "    \n",
    "    # Plot each coordinate method\n",
    "    for i, method in enumerate(coordinate_methods):\n",
    "        plt.plot(iterations, accuracies[method], label=method.replace('_', ' ').title(), \n",
    "                 linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Comparison of All Coordinate Selection Methods')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_coordinate_methods_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Figure 3: Coordinate selection with baseline loss comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, baseline_accuracies, label='Baseline Loss - Random')\n",
    "    plt.plot(iterations, baseline_gradient_accuracies, label='Baseline Loss - Gradient-based')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Coordinate Selection Comparison with Baseline Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_coordinate_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'grid_search_results': grid_results,\n",
    "        'test_accuracies': {\n",
    "            'baseline_random': baseline_model.score(X_test, y_test),\n",
    "            'custom_random': test_accuracies['random'],\n",
    "        },\n",
    "        'training_accuracies': {\n",
    "            'baseline_random': baseline_accuracies,\n",
    "            'custom_random': random_accuracies,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add all coordinate methods to results\n",
    "    for method in coordinate_methods:\n",
    "        result['test_accuracies'][f'custom_{method}'] = test_accuracies[method]\n",
    "        result['training_accuracies'][f'custom_{method}'] = accuracies[method]\n",
    "    \n",
    "    # Add baseline with gradient\n",
    "    result['test_accuracies']['baseline_gradient'] = baseline_gradient_model.score(X_test, y_test)\n",
    "    result['training_accuracies']['baseline_gradient'] = baseline_gradient_accuracies\n",
    "    \n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:34.009128Z",
     "start_time": "2025-02-25T04:43:34.003726Z"
    }
   },
   "id": "d253c24d65992d4d",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution failed with error: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "Falling back to sequential execution...\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100} - Mean f1: 0.6617\n",
      "Average unique accuracy points: 30.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500} - Mean f1: 0.8218\n",
      "Average unique accuracy points: 23.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean f1: 0.8374\n",
      "Average unique accuracy points: 20.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100} - Mean f1: 0.6617\n",
      "Average unique accuracy points: 31.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500} - Mean f1: 0.7987\n",
      "Average unique accuracy points: 23.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean f1: 0.8089\n",
      "Average unique accuracy points: 20.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100} - Mean f1: 0.6065\n",
      "Average unique accuracy points: 16.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500} - Mean f1: 0.7809\n",
      "Average unique accuracy points: 26.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean f1: 0.8317\n",
      "Average unique accuracy points: 28.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100} - Mean f1: 0.6065\n",
      "Average unique accuracy points: 17.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500} - Mean f1: 0.7809\n",
      "Average unique accuracy points: 26.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean f1: 0.8317\n",
      "Average unique accuracy points: 28.2/100 (higher suggests better convergence)\n",
      "\n",
      "Grid Search Results (using Coordinate descent with random selection):\n",
      "Best parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Best f1: 0.8374\n",
      "Best model converged after ~12 iterations\n",
      "Did not converge after 1000 iterations. Final accuracy: 0.7788\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0.47115385, 0.75961538, 0.61538462, 0.85576923, 0.77884615,\n       0.75      , 0.72115385, 0.76923077, 0.76923077, 0.77884615,\n       0.79807692, 0.77884615, 0.68269231, 0.78846154, 0.78846154,\n       0.79807692, 0.79807692, 0.81730769, 0.81730769, 0.82692308,\n       0.79807692, 0.79807692, 0.81730769, 0.79807692, 0.81730769,\n       0.81730769, 0.79807692, 0.79807692, 0.79807692, 0.78846154,\n       0.80769231, 0.79807692, 0.80769231, 0.78846154, 0.80769231,\n       0.79807692, 0.80769231, 0.81730769, 0.78846154, 0.81730769,\n       0.81730769, 0.80769231, 0.78846154, 0.76923077, 0.75      ,\n       0.73076923, 0.72115385, 0.76923077, 0.76923077, 0.75      ,\n       0.76923077, 0.77884615, 0.77884615, 0.77884615, 0.77884615,\n       0.78846154, 0.79807692, 0.77884615, 0.76923077, 0.77884615,\n       0.79807692, 0.80769231, 0.78846154, 0.78846154, 0.76923077,\n       0.77884615, 0.77884615, 0.78846154, 0.78846154, 0.77884615,\n       0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n       0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n       0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n       0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n       0.76923077, 0.76923077, 0.76923077, 0.77884615, 0.77884615,\n       0.77884615, 0.77884615, 0.77884615, 0.77884615, 0.77884615])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.01, 0.001],\n",
    "    'reg_lambda': [0.1, 1.0],\n",
    "    'max_iter': [100,500,1000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = grid_search(X, y, param_grid, verbose=True, scoring='f1')\n",
    "\n",
    "# Create model with best parameters\n",
    "best_model = LogisticRegression(**results['best_params'])\n",
    "best_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:35.076715Z",
     "start_time": "2025-02-25T04:43:34.007701Z"
    }
   },
   "id": "188ffcd897f4717",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Finding optimal hyperparameters...\n",
      "Parallel execution failed with error: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "Falling back to sequential execution...\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.8752\n",
      "Average unique accuracy points: 23.0/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.9229\n",
      "Average unique accuracy points: 13.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.8752\n",
      "Average unique accuracy points: 11.6/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.8752\n",
      "Average unique accuracy points: 23.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.8943\n",
      "Average unique accuracy points: 14.8/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.8848\n",
      "Average unique accuracy points: 13.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100} - Mean accuracy: 0.8162\n",
      "Average unique accuracy points: 26.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500} - Mean accuracy: 0.9129\n",
      "Average unique accuracy points: 20.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000} - Mean accuracy: 0.8843\n",
      "Average unique accuracy points: 16.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100} - Mean accuracy: 0.8162\n",
      "Average unique accuracy points: 26.4/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500} - Mean accuracy: 0.9129\n",
      "Average unique accuracy points: 20.2/100 (higher suggests better convergence)\n",
      "Evaluating parameters: {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\n",
      "Parameters {'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000} - Mean accuracy: 0.8743\n",
      "Average unique accuracy points: 16.6/100 (higher suggests better convergence)\n",
      "\n",
      "Grid Search Results (using Coordinate descent with random selection):\n",
      "Best parameters: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "Best accuracy: 0.9229\n",
      "Best model converged after ~23 iterations\n",
      "Best parameters found: {'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\n",
      "\n",
      "Phase 2: Comparing baseline vs custom loss with random coordinate selection...\n",
      "\n",
      "Phase 3: Comparing all coordinate selection methods with custom loss...\n",
      "  Training with gradient_based coordinate selection...\n",
      "  Training with cyclic coordinate selection...\n",
      "  Training with cyclic_momentum coordinate selection...\n",
      "  Training with importance_sampling coordinate selection...\n",
      "  Training with greedy_subset coordinate selection...\n",
      "  Training with adaptive_frequency coordinate selection...\n",
      "\n",
      "Test accuracies:\n",
      "Baseline loss with random coord: 0.7692\n",
      "Custom loss with random coord: 1.0000\n",
      "Custom loss with gradient_based coord: 0.4615\n",
      "Custom loss with cyclic coord: 1.0000\n",
      "Custom loss with cyclic_momentum coord: 1.0000\n",
      "Custom loss with importance_sampling coord: 1.0000\n",
      "Custom loss with greedy_subset coord: 0.9615\n",
      "Custom loss with adaptive_frequency coord: 1.0000\n",
      "Baseline loss with gradient-based coord: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'grid_search_results': {'best_params': {'lr': 0.01,\n   'reg_lambda': 0.1,\n   'max_iter': 1000},\n  'best_score': np.float64(0.9228571428571429),\n  'best_convergence': array([0.45430293, 0.58872633, 0.60820425, 0.75246701, 0.8294607 ,\n         0.88711991, 0.90886403, 0.89202524, 0.87997705, 0.89911073,\n         0.90381526, 0.91095812, 0.91818703, 0.9109868 , 0.90149168,\n         0.89176707, 0.88453815, 0.87251865, 0.86537579, 0.87971888,\n         0.87257602, 0.87016638, 0.86537579, 0.87983362, 0.87983362,\n         0.89905336, 0.8894148 , 0.90625359, 0.90140562, 0.89899598,\n         0.90857717, 0.90146299, 0.90863454, 0.89902467, 0.91345382,\n         0.91583477, 0.91101549, 0.91583477, 0.91583477, 0.91342513,\n         0.91104418, 0.90387263, 0.90387263, 0.90384395, 0.89905336,\n         0.9062249 , 0.91342513, 0.91339644, 0.92071142, 0.92071142,\n         0.92065404, 0.92306368, 0.91827309, 0.92068273, 0.91113024,\n         0.91113024, 0.91353987, 0.9134825 , 0.9087206 , 0.9087206 ,\n         0.91113024, 0.91113024, 0.91113024, 0.91113024, 0.91351119,\n         0.91110155, 0.91110155, 0.91351119, 0.9087206 , 0.9087206 ,\n         0.90631096, 0.9087206 , 0.90631096, 0.90631096, 0.91110155,\n         0.91110155, 0.91592083, 0.91592083, 0.90869191, 0.90869191,\n         0.90631096, 0.90152037, 0.90152037, 0.90390132, 0.90152037,\n         0.90390132, 0.90390132, 0.90390132, 0.90390132, 0.90628227,\n         0.90390132, 0.90628227, 0.90628227, 0.90628227, 0.90628227,\n         0.90387263, 0.90628227, 0.90628227, 0.90869191, 0.90866322]),\n  'all_results': [({'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100},\n    np.float64(0.8752380952380954)),\n   ({'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000},\n    np.float64(0.9228571428571429)),\n   ({'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000},\n    np.float64(0.8752380952380954)),\n   ({'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100},\n    np.float64(0.8752380952380954)),\n   ({'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500},\n    np.float64(0.8942857142857144)),\n   ({'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000},\n    np.float64(0.8847619047619049)),\n   ({'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100},\n    np.float64(0.8161904761904761)),\n   ({'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500},\n    np.float64(0.9128571428571428)),\n   ({'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000},\n    np.float64(0.8842857142857143)),\n   ({'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100},\n    np.float64(0.8161904761904761)),\n   ({'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500},\n    np.float64(0.9128571428571428)),\n   ({'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000},\n    np.float64(0.8742857142857143))],\n  'convergence_curves': {\"{'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 100}\": array([0.45430293, 0.63462421, 0.64658635, 0.55771658, 0.61511761,\n          0.58872633, 0.58872633, 0.59125072, 0.57438325, 0.56724039,\n          0.60820425, 0.68514056, 0.71391279, 0.77395295, 0.74759036,\n          0.75246701, 0.77174412, 0.77894435, 0.7885829 , 0.80051635,\n          0.8294607 , 0.86072863, 0.86790017, 0.8630809 , 0.88232932,\n          0.88711991, 0.88955823, 0.90398738, 0.89681583, 0.90407344,\n          0.90886403, 0.91124498, 0.9064257 , 0.8944062 , 0.88958692,\n          0.89202524, 0.88961561, 0.89202524, 0.88720597, 0.8775961 ,\n          0.87997705, 0.87997705, 0.88000574, 0.89919679, 0.90393001,\n          0.89911073, 0.90152037, 0.8846529 , 0.90384395, 0.89902467,\n          0.90381526, 0.91107286, 0.92059667, 0.91818703, 0.92300631,\n          0.91095812, 0.91095812, 0.9157774 , 0.91818703, 0.9157774 ,\n          0.91818703, 0.92059667, 0.91580608, 0.90378657, 0.91580608,\n          0.9109868 , 0.90860585, 0.91101549, 0.90619621, 0.89908204,\n          0.90149168, 0.90863454, 0.90863454, 0.88938612, 0.88456684,\n          0.89176707, 0.89176707, 0.88456684, 0.88453815, 0.88212851,\n          0.88453815, 0.88212851, 0.87971888, 0.87251865, 0.87730924,\n          0.87251865, 0.87254733, 0.86775674, 0.87013769, 0.86775674,\n          0.86537579, 0.87013769, 0.87254733, 0.87730924, 0.87730924,\n          0.87971888, 0.87971888, 0.88218589, 0.87977625, 0.87495697]),\n   \"{'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 500}\": array([0.45430293, 0.58872633, 0.60820425, 0.75246701, 0.8294607 ,\n          0.88711991, 0.90886403, 0.89202524, 0.87997705, 0.89911073,\n          0.90381526, 0.91095812, 0.91818703, 0.9109868 , 0.90149168,\n          0.89176707, 0.88453815, 0.87251865, 0.86537579, 0.87971888,\n          0.87257602, 0.87016638, 0.86537579, 0.87983362, 0.87983362,\n          0.89905336, 0.8894148 , 0.90625359, 0.90140562, 0.89899598,\n          0.90857717, 0.90146299, 0.90863454, 0.89902467, 0.91345382,\n          0.91583477, 0.91101549, 0.91583477, 0.91583477, 0.91342513,\n          0.91104418, 0.90387263, 0.90387263, 0.90384395, 0.89905336,\n          0.9062249 , 0.91342513, 0.91339644, 0.92071142, 0.92071142,\n          0.92065404, 0.92306368, 0.91827309, 0.92068273, 0.91113024,\n          0.91113024, 0.91353987, 0.9134825 , 0.9087206 , 0.9087206 ,\n          0.91113024, 0.91113024, 0.91113024, 0.91113024, 0.91351119,\n          0.91110155, 0.91110155, 0.91351119, 0.9087206 , 0.9087206 ,\n          0.90631096, 0.9087206 , 0.90631096, 0.90631096, 0.91110155,\n          0.91110155, 0.91592083, 0.91592083, 0.90869191, 0.90869191,\n          0.90631096, 0.90152037, 0.90152037, 0.90390132, 0.90152037,\n          0.90390132, 0.90390132, 0.90390132, 0.90390132, 0.90628227,\n          0.90390132, 0.90628227, 0.90628227, 0.90628227, 0.90628227,\n          0.90387263, 0.90628227, 0.90628227, 0.90869191, 0.90866322]),\n   \"{'lr': 0.01, 'reg_lambda': 0.1, 'max_iter': 1000}\": array([0.45430293, 0.60820425, 0.8294607 , 0.90886403, 0.87997705,\n          0.90381526, 0.91818703, 0.90149168, 0.88453815, 0.86537579,\n          0.87257602, 0.86537579, 0.87983362, 0.8894148 , 0.90140562,\n          0.90857717, 0.90863454, 0.91345382, 0.91101549, 0.91583477,\n          0.91104418, 0.90387263, 0.89905336, 0.91342513, 0.92071142,\n          0.92065404, 0.91827309, 0.91113024, 0.91353987, 0.9087206 ,\n          0.91113024, 0.91113024, 0.91351119, 0.91110155, 0.9087206 ,\n          0.90631096, 0.90631096, 0.91110155, 0.91592083, 0.90869191,\n          0.90631096, 0.90152037, 0.90152037, 0.90390132, 0.90390132,\n          0.90390132, 0.90628227, 0.90628227, 0.90628227, 0.90869191,\n          0.90866322, 0.90143431, 0.90387263, 0.89905336, 0.89664372,\n          0.89182444, 0.88459552, 0.89423408, 0.89182444, 0.89423408,\n          0.89423408, 0.89664372, 0.89664372, 0.89423408, 0.89905336,\n          0.89182444, 0.89182444, 0.8894148 , 0.89423408, 0.89185313,\n          0.89423408, 0.89182444, 0.88703385, 0.88944349, 0.89185313,\n          0.88944349, 0.88944349, 0.8894148 , 0.88700516, 0.88700516,\n          0.89182444, 0.8894148 , 0.8894148 , 0.89182444, 0.8894148 ,\n          0.89182444, 0.89423408, 0.8894148 , 0.8894148 , 0.8846529 ,\n          0.88706254, 0.88706254, 0.88224326, 0.8846529 , 0.88224326,\n          0.8846529 , 0.8846529 , 0.8846529 , 0.8846529 , 0.87983362]),\n   \"{'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 100}\": array([0.45430293, 0.63462421, 0.64658635, 0.55051635, 0.61270797,\n          0.58872633, 0.5863167 , 0.58643144, 0.56715433, 0.56724039,\n          0.60579461, 0.68993115, 0.71632243, 0.77395295, 0.74759036,\n          0.75246701, 0.77174412, 0.77897303, 0.7885829 , 0.79810671,\n          0.8294607 , 0.85831899, 0.86790017, 0.8630809 , 0.88232932,\n          0.88232932, 0.88955823, 0.90157774, 0.89681583, 0.90169248,\n          0.90886403, 0.90883534, 0.90401606, 0.8944062 , 0.88958692,\n          0.89202524, 0.88961561, 0.88961561, 0.88720597, 0.8775961 ,\n          0.87997705, 0.87997705, 0.88000574, 0.89681583, 0.90152037,\n          0.89911073, 0.90390132, 0.8894148 , 0.90146299, 0.89664372,\n          0.90381526, 0.90866322, 0.91818703, 0.91818703, 0.92300631,\n          0.91095812, 0.90857717, 0.91339644, 0.91339644, 0.91339644,\n          0.91580608, 0.92059667, 0.91580608, 0.90378657, 0.91339644,\n          0.9109868 , 0.90860585, 0.90860585, 0.9062249 , 0.89905336,\n          0.90149168, 0.90863454, 0.90381526, 0.88935743, 0.88694779,\n          0.89176707, 0.89176707, 0.88694779, 0.8869191 , 0.88212851,\n          0.88453815, 0.88212851, 0.87971888, 0.87251865, 0.87492828,\n          0.87251865, 0.87492828, 0.86775674, 0.87013769, 0.87013769,\n          0.8653471 , 0.87013769, 0.87495697, 0.87971888, 0.87492828,\n          0.87971888, 0.88212851, 0.88218589, 0.8773953 , 0.87016638]),\n   \"{'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 500}\": array([0.45430293, 0.58872633, 0.60579461, 0.75246701, 0.8294607 ,\n          0.88232932, 0.90886403, 0.89202524, 0.87997705, 0.89911073,\n          0.90381526, 0.91095812, 0.91580608, 0.9109868 , 0.90149168,\n          0.89176707, 0.88453815, 0.87251865, 0.8653471 , 0.87971888,\n          0.87016638, 0.86537579, 0.86778543, 0.8773953 , 0.87983362,\n          0.89664372, 0.88221457, 0.90143431, 0.89658635, 0.89661503,\n          0.89655766, 0.89905336, 0.90146299, 0.89902467, 0.91104418,\n          0.90378657, 0.90860585, 0.91101549, 0.90860585, 0.90619621,\n          0.90143431, 0.89426277, 0.88944349, 0.89182444, 0.89423408,\n          0.89664372, 0.90381526, 0.90860585, 0.91586345, 0.91107286,\n          0.91824441, 0.92306368, 0.90863454, 0.89908204, 0.89908204,\n          0.90390132, 0.90631096, 0.90633964, 0.90154905, 0.90390132,\n          0.90393001, 0.90631096, 0.90631096, 0.90390132, 0.90149168,\n          0.90149168, 0.89672978, 0.89672978, 0.90633964, 0.90393001,\n          0.90393001, 0.9087206 , 0.9087206 , 0.90149168, 0.90149168,\n          0.90390132, 0.9087206 , 0.90631096, 0.9087206 , 0.90390132,\n          0.90152037, 0.89670109, 0.89908204, 0.8966724 , 0.8966724 ,\n          0.8966724 , 0.8966724 , 0.89429145, 0.89423408, 0.89423408,\n          0.8894148 , 0.8894148 , 0.89426277, 0.88944349, 0.88703385,\n          0.89185313, 0.89664372, 0.89426277, 0.8966724 , 0.89423408]),\n   \"{'lr': 0.01, 'reg_lambda': 1.0, 'max_iter': 1000}\": array([0.45430293, 0.60579461, 0.8294607 , 0.90886403, 0.87997705,\n          0.90381526, 0.91580608, 0.90149168, 0.88453815, 0.8653471 ,\n          0.87016638, 0.86778543, 0.87983362, 0.88221457, 0.89658635,\n          0.89655766, 0.90146299, 0.91104418, 0.90860585, 0.90860585,\n          0.90143431, 0.88944349, 0.89423408, 0.90381526, 0.91586345,\n          0.91824441, 0.90863454, 0.89908204, 0.90631096, 0.90154905,\n          0.90393001, 0.90631096, 0.90149168, 0.89672978, 0.90633964,\n          0.90393001, 0.9087206 , 0.90149168, 0.9087206 , 0.9087206 ,\n          0.90152037, 0.89908204, 0.8966724 , 0.8966724 , 0.89423408,\n          0.8894148 , 0.89426277, 0.88703385, 0.89664372, 0.8966724 ,\n          0.89423408, 0.88221457, 0.88462421, 0.88459552, 0.87257602,\n          0.87016638, 0.86052783, 0.8653471 , 0.86775674, 0.8773953 ,\n          0.8773953 , 0.8773953 , 0.87498566, 0.87257602, 0.87016638,\n          0.8653471 , 0.8653471 , 0.86296615, 0.86775674, 0.86775674,\n          0.87016638, 0.86293746, 0.86052783, 0.86293746, 0.86293746,\n          0.86293746, 0.86293746, 0.85814687, 0.85573723, 0.85335628,\n          0.85335628, 0.85335628, 0.85094664, 0.85573723, 0.86055651,\n          0.85814687, 0.85576592, 0.85576592, 0.85576592, 0.85817556,\n          0.85817556, 0.85817556, 0.85817556, 0.85817556, 0.85817556,\n          0.85335628, 0.85097533, 0.85338497, 0.86296615, 0.84856569]),\n   \"{'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 100}\": array([0.45665519, 0.47108434, 0.48066552, 0.47346529, 0.48066552,\n          0.48542742, 0.48542742, 0.48060815, 0.48060815, 0.48542742,\n          0.4902467 , 0.49506598, 0.50223752, 0.51428571, 0.5070568 ,\n          0.51669535, 0.51913368, 0.52392427, 0.52392427, 0.52154332,\n          0.54320138, 0.55522088, 0.56233505, 0.56233505, 0.57432587,\n          0.57432587, 0.58155479, 0.60562249, 0.59119334, 0.59133678,\n          0.59133678, 0.59606999, 0.59606999, 0.60329891, 0.62498566,\n          0.62736661, 0.62736661, 0.6226047 , 0.6226047 , 0.62742398,\n          0.63456684, 0.62492828, 0.6321572 , 0.63700516, 0.63456684,\n          0.63456684, 0.63218589, 0.63700516, 0.6394148 , 0.6587206 ,\n          0.67550201, 0.66824441, 0.66586345, 0.66586345, 0.68032129,\n          0.67550201, 0.67550201, 0.69236948, 0.68514056, 0.68995984,\n          0.68273092, 0.68032129, 0.68752151, 0.72604705, 0.75246701,\n          0.77653471, 0.78605852, 0.78608721, 0.78849684, 0.7956397 ,\n          0.7956397 , 0.79320138, 0.79804934, 0.80043029, 0.79079174,\n          0.80524957, 0.80043029, 0.78841079, 0.78602983, 0.80283993,\n          0.80524957, 0.80048766, 0.81973609, 0.83416523, 0.83175559,\n          0.82211704, 0.82452668, 0.82452668, 0.81973609, 0.81259323,\n          0.82211704, 0.81256454, 0.83184165, 0.83660356, 0.84145152,\n          0.85820425, 0.8510327 , 0.84383247, 0.84142283, 0.85347103]),\n   \"{'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 500}\": array([0.45665519, 0.48542742, 0.4902467 , 0.51669535, 0.54320138,\n          0.57432587, 0.59133678, 0.62736661, 0.63456684, 0.63456684,\n          0.67550201, 0.67550201, 0.68273092, 0.77653471, 0.7956397 ,\n          0.80524957, 0.80524957, 0.82211704, 0.82211704, 0.85820425,\n          0.85582329, 0.85341365, 0.84621343, 0.86540448, 0.87016638,\n          0.87980493, 0.86775674, 0.87492828, 0.87016638, 0.87254733,\n          0.88448078, 0.87977625, 0.8894148 , 0.87263339, 0.88947217,\n          0.87748135, 0.87507172, 0.87504303, 0.87980493, 0.87257602,\n          0.88227194, 0.8894148 , 0.88459552, 0.88462421, 0.88462421,\n          0.88221457, 0.89423408, 0.88944349, 0.8966724 , 0.89423408,\n          0.89182444, 0.89182444, 0.88218589, 0.88221457, 0.88459552,\n          0.88218589, 0.88700516, 0.88703385, 0.88462421, 0.88462421,\n          0.88703385, 0.88703385, 0.88944349, 0.88944349, 0.88703385,\n          0.88462421, 0.88944349, 0.88944349, 0.89426277, 0.89429145,\n          0.89429145, 0.89188181, 0.88947217, 0.8846529 , 0.88224326,\n          0.88706254, 0.89426277, 0.88944349, 0.89185313, 0.88944349,\n          0.88703385, 0.88703385, 0.88944349, 0.88703385, 0.88703385,\n          0.89185313, 0.90866322, 0.90625359, 0.90381526, 0.9062249 ,\n          0.89420539, 0.90625359, 0.90143431, 0.90625359, 0.90866322,\n          0.90384395, 0.91351119, 0.91351119, 0.90387263, 0.90387263]),\n   \"{'lr': 0.001, 'reg_lambda': 0.1, 'max_iter': 1000}\": array([0.45665519, 0.4902467 , 0.54320138, 0.59133678, 0.63456684,\n          0.67550201, 0.68273092, 0.7956397 , 0.80524957, 0.82211704,\n          0.85582329, 0.84621343, 0.87016638, 0.86775674, 0.87016638,\n          0.88448078, 0.8894148 , 0.88947217, 0.87507172, 0.87980493,\n          0.88227194, 0.88459552, 0.88462421, 0.89423408, 0.8966724 ,\n          0.89182444, 0.88218589, 0.88459552, 0.88700516, 0.88462421,\n          0.88703385, 0.88944349, 0.88703385, 0.88944349, 0.89426277,\n          0.89429145, 0.88947217, 0.88224326, 0.89426277, 0.89185313,\n          0.88703385, 0.88944349, 0.88703385, 0.90866322, 0.90381526,\n          0.89420539, 0.90143431, 0.90866322, 0.91351119, 0.90387263,\n          0.90387263, 0.89185313, 0.89664372, 0.88944349, 0.88459552,\n          0.88700516, 0.87254733, 0.8894148 , 0.8894148 , 0.89182444,\n          0.89179575, 0.89420539, 0.89182444, 0.90143431, 0.90143431,\n          0.89902467, 0.90143431, 0.90143431, 0.90625359, 0.90625359,\n          0.90384395, 0.90384395, 0.89902467, 0.90384395, 0.90143431,\n          0.90143431, 0.90143431, 0.90381526, 0.90381526, 0.90140562,\n          0.90140562, 0.90140562, 0.90140562, 0.90381526, 0.90140562,\n          0.90381526, 0.90381526, 0.9062249 , 0.9062249 , 0.90381526,\n          0.9062249 , 0.90384395, 0.90384395, 0.90384395, 0.90384395,\n          0.90384395, 0.90384395, 0.90384395, 0.90625359, 0.90384395]),\n   \"{'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 100}\": array([0.45665519, 0.47108434, 0.48066552, 0.47346529, 0.48066552,\n          0.48542742, 0.48542742, 0.48060815, 0.48060815, 0.48542742,\n          0.4902467 , 0.49506598, 0.50223752, 0.51428571, 0.5070568 ,\n          0.51669535, 0.51913368, 0.52392427, 0.52392427, 0.52154332,\n          0.54320138, 0.55522088, 0.56233505, 0.56233505, 0.57432587,\n          0.57191624, 0.58155479, 0.60562249, 0.59119334, 0.58892714,\n          0.59133678, 0.59606999, 0.59606999, 0.60329891, 0.62498566,\n          0.62736661, 0.62736661, 0.6226047 , 0.62501434, 0.62742398,\n          0.6321572 , 0.62492828, 0.6321572 , 0.63700516, 0.63456684,\n          0.63456684, 0.63218589, 0.63700516, 0.6394148 , 0.6587206 ,\n          0.67550201, 0.66824441, 0.66586345, 0.66586345, 0.68273092,\n          0.67550201, 0.67550201, 0.69236948, 0.68514056, 0.68995984,\n          0.68032129, 0.68032129, 0.68993115, 0.72604705, 0.75246701,\n          0.77653471, 0.78364888, 0.78367757, 0.78849684, 0.79323006,\n          0.7956397 , 0.79320138, 0.79804934, 0.80043029, 0.79079174,\n          0.80765921, 0.80283993, 0.78841079, 0.79084911, 0.80283993,\n          0.80524957, 0.80286862, 0.81973609, 0.83416523, 0.83416523,\n          0.82211704, 0.82452668, 0.82452668, 0.81973609, 0.81735513,\n          0.81973609, 0.8149455 , 0.8294607 , 0.83898451, 0.84145152,\n          0.85820425, 0.85341365, 0.84383247, 0.84383247, 0.85347103]),\n   \"{'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 500}\": array([0.45665519, 0.48542742, 0.4902467 , 0.51669535, 0.54320138,\n          0.57191624, 0.59133678, 0.62736661, 0.6321572 , 0.63456684,\n          0.67550201, 0.67550201, 0.68032129, 0.77653471, 0.7956397 ,\n          0.80765921, 0.80524957, 0.82211704, 0.81973609, 0.85820425,\n          0.85582329, 0.85100402, 0.84862306, 0.87016638, 0.87016638,\n          0.87980493, 0.86775674, 0.87492828, 0.87013769, 0.87492828,\n          0.88448078, 0.87977625, 0.8894148 , 0.87022375, 0.89185313,\n          0.87748135, 0.8678428 , 0.87504303, 0.88221457, 0.87016638,\n          0.88227194, 0.88700516, 0.88218589, 0.88221457, 0.87498566,\n          0.88221457, 0.89185313, 0.88703385, 0.89426277, 0.89182444,\n          0.89182444, 0.88700516, 0.88218589, 0.87980493, 0.88218589,\n          0.88218589, 0.88221457, 0.88462421, 0.88221457, 0.88462421,\n          0.88462421, 0.88703385, 0.88944349, 0.88944349, 0.88703385,\n          0.88462421, 0.88944349, 0.89185313, 0.89185313, 0.89670109,\n          0.89429145, 0.89188181, 0.88947217, 0.88224326, 0.88224326,\n          0.8846529 , 0.88944349, 0.88944349, 0.89185313, 0.88944349,\n          0.88703385, 0.88944349, 0.88944349, 0.88703385, 0.88462421,\n          0.88944349, 0.90625359, 0.90384395, 0.90381526, 0.90381526,\n          0.89420539, 0.90625359, 0.89905336, 0.90625359, 0.90384395,\n          0.89905336, 0.91351119, 0.91110155, 0.90387263, 0.90387263]),\n   \"{'lr': 0.001, 'reg_lambda': 1.0, 'max_iter': 1000}\": array([0.45665519, 0.4902467 , 0.54320138, 0.59133678, 0.6321572 ,\n          0.67550201, 0.68032129, 0.7956397 , 0.80524957, 0.81973609,\n          0.85582329, 0.84862306, 0.87016638, 0.86775674, 0.87013769,\n          0.88448078, 0.8894148 , 0.89185313, 0.8678428 , 0.88221457,\n          0.88227194, 0.88218589, 0.87498566, 0.89185313, 0.89426277,\n          0.89182444, 0.88218589, 0.88218589, 0.88221457, 0.88221457,\n          0.88462421, 0.88944349, 0.88703385, 0.88944349, 0.89185313,\n          0.89429145, 0.88947217, 0.88224326, 0.88944349, 0.89185313,\n          0.88703385, 0.88944349, 0.88462421, 0.90625359, 0.90381526,\n          0.89420539, 0.89905336, 0.90384395, 0.91351119, 0.90387263,\n          0.90387263, 0.89423408, 0.89423408, 0.88944349, 0.88459552,\n          0.88459552, 0.86772806, 0.87977625, 0.88459552, 0.88700516,\n          0.88700516, 0.89420539, 0.8894148 , 0.89902467, 0.90143431,\n          0.89902467, 0.89902467, 0.90143431, 0.90384395, 0.90384395,\n          0.90384395, 0.90143431, 0.89902467, 0.90143431, 0.90143431,\n          0.90143431, 0.90143431, 0.90381526, 0.90143431, 0.89902467,\n          0.90140562, 0.89902467, 0.89902467, 0.90140562, 0.90140562,\n          0.90140562, 0.90140562, 0.90143431, 0.90143431, 0.90384395,\n          0.90384395, 0.90384395, 0.90384395, 0.90384395, 0.90384395,\n          0.90384395, 0.90384395, 0.90384395, 0.90625359, 0.90143431])}},\n 'test_accuracies': {'baseline_random': np.float64(0.7692307692307693),\n  'custom_random': np.float64(1.0),\n  'custom_gradient_based': np.float64(0.46153846153846156),\n  'custom_cyclic': np.float64(1.0),\n  'custom_cyclic_momentum': np.float64(1.0),\n  'custom_importance_sampling': np.float64(1.0),\n  'custom_greedy_subset': np.float64(0.9615384615384616),\n  'custom_adaptive_frequency': np.float64(1.0),\n  'baseline_gradient': np.float64(0.5)},\n 'training_accuracies': {'baseline_random': array([0.45192308, 0.625     , 0.80769231, 0.84615385, 0.92307692,\n         0.875     , 0.91346154, 0.90384615, 0.84615385, 0.84615385,\n         0.85576923, 0.80769231, 0.81730769, 0.83653846, 0.85576923,\n         0.83653846, 0.82692308, 0.84615385, 0.84615385, 0.85576923,\n         0.86538462, 0.84615385, 0.84615385, 0.85576923, 0.86538462,\n         0.86538462, 0.85576923, 0.86538462, 0.86538462, 0.86538462,\n         0.86538462, 0.85576923, 0.86538462, 0.84615385, 0.83653846,\n         0.83653846, 0.83653846, 0.84615385, 0.84615385, 0.83653846,\n         0.84615385, 0.83653846, 0.82692308, 0.80769231, 0.80769231,\n         0.81730769, 0.80769231, 0.81730769, 0.80769231, 0.80769231,\n         0.81730769, 0.80769231, 0.79807692, 0.79807692, 0.81730769,\n         0.81730769, 0.80769231, 0.81730769, 0.79807692, 0.81730769,\n         0.79807692, 0.79807692, 0.78846154, 0.78846154, 0.78846154,\n         0.78846154, 0.77884615, 0.78846154, 0.78846154, 0.78846154,\n         0.79807692, 0.77884615, 0.77884615, 0.77884615, 0.76923077,\n         0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n         0.77884615, 0.77884615, 0.77884615, 0.78846154, 0.79807692,\n         0.79807692, 0.78846154, 0.79807692, 0.79807692, 0.79807692,\n         0.79807692, 0.79807692, 0.79807692, 0.81730769, 0.81730769,\n         0.81730769, 0.81730769, 0.80769231, 0.80769231, 0.80769231]),\n  'custom_random': array([0.46153846, 0.69230769, 0.75961538, 0.79807692, 0.875     ,\n         0.93269231, 0.91346154, 0.89423077, 0.875     , 0.89423077,\n         0.89423077, 0.89423077, 0.89423077, 0.89423077, 0.89423077,\n         0.89423077, 0.89423077, 0.89423077, 0.89423077, 0.90384615,\n         0.89423077, 0.88461538, 0.88461538, 0.90384615, 0.89423077,\n         0.89423077, 0.89423077, 0.89423077, 0.89423077, 0.88461538,\n         0.89423077, 0.89423077, 0.90384615, 0.90384615, 0.90384615,\n         0.92307692, 0.92307692, 0.93269231, 0.92307692, 0.93269231,\n         0.92307692, 0.92307692, 0.92307692, 0.92307692, 0.93269231,\n         0.94230769, 0.93269231, 0.93269231, 0.93269231, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.93269231, 0.92307692, 0.93269231,\n         0.93269231, 0.93269231, 0.94230769, 0.94230769, 0.93269231,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.93269231, 0.94230769, 0.93269231, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308]),\n  'custom_gradient_based': array([0.47115385, 0.53846154, 0.53846154, 0.50961538, 0.47115385,\n         0.46153846, 0.48076923, 0.43269231, 0.44230769, 0.45192308,\n         0.47115385, 0.45192308, 0.47115385, 0.53846154, 0.54807692,\n         0.54807692, 0.54807692, 0.54807692, 0.54807692, 0.53846154,\n         0.54807692, 0.55769231, 0.54807692, 0.55769231, 0.55769231,\n         0.54807692, 0.55769231, 0.54807692, 0.54807692, 0.53846154,\n         0.47115385, 0.47115385, 0.46153846, 0.47115385, 0.51923077,\n         0.49038462, 0.49038462, 0.43269231, 0.43269231, 0.45192308,\n         0.47115385, 0.43269231, 0.47115385, 0.44230769, 0.43269231,\n         0.45192308, 0.47115385, 0.46153846, 0.45192308, 0.45192308,\n         0.45192308, 0.46153846, 0.48076923, 0.48076923, 0.48076923,\n         0.46153846, 0.46153846, 0.45192308, 0.47115385, 0.46153846,\n         0.42307692, 0.42307692, 0.45192308, 0.46153846, 0.46153846,\n         0.46153846, 0.46153846, 0.47115385, 0.47115385, 0.47115385,\n         0.51923077, 0.49038462, 0.51923077, 0.51923077, 0.49038462,\n         0.51923077, 0.51923077, 0.47115385, 0.47115385, 0.49038462,\n         0.49038462, 0.48076923, 0.46153846, 0.45192308, 0.48076923,\n         0.47115385, 0.47115385, 0.47115385, 0.49038462, 0.49038462,\n         0.50961538, 0.49038462, 0.51923077, 0.50961538, 0.50961538,\n         0.51923077, 0.52884615, 0.53846154, 0.54807692, 0.54807692]),\n  'custom_cyclic': array([0.47115385, 0.76923077, 0.78846154, 0.84615385, 0.95192308,\n         0.875     , 0.90384615, 0.90384615, 0.90384615, 0.91346154,\n         0.94230769, 0.94230769, 0.93269231, 0.95192308, 0.94230769,\n         0.95192308, 0.94230769, 0.93269231, 0.94230769, 0.93269231,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.93269231,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.93269231, 0.93269231, 0.92307692, 0.95192308, 0.93269231,\n         0.93269231, 0.93269231, 0.92307692, 0.92307692, 0.92307692,\n         0.92307692, 0.92307692, 0.92307692, 0.93269231, 0.93269231,\n         0.93269231, 0.93269231, 0.93269231, 0.94230769, 0.94230769,\n         0.93269231, 0.93269231, 0.94230769, 0.93269231, 0.92307692,\n         0.93269231, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.93269231, 0.93269231, 0.93269231, 0.94230769,\n         0.93269231, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.93269231, 0.94230769, 0.94230769]),\n  'custom_cyclic_momentum': array([0.47115385, 0.76923077, 0.78846154, 0.84615385, 0.95192308,\n         0.875     , 0.90384615, 0.90384615, 0.90384615, 0.91346154,\n         0.94230769, 0.94230769, 0.93269231, 0.95192308, 0.94230769,\n         0.95192308, 0.94230769, 0.93269231, 0.94230769, 0.93269231,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.93269231,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.93269231, 0.93269231, 0.92307692, 0.95192308, 0.93269231,\n         0.93269231, 0.93269231, 0.92307692, 0.92307692, 0.92307692,\n         0.92307692, 0.92307692, 0.92307692, 0.93269231, 0.93269231,\n         0.93269231, 0.93269231, 0.93269231, 0.94230769, 0.94230769,\n         0.93269231, 0.93269231, 0.94230769, 0.93269231, 0.92307692,\n         0.93269231, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.93269231, 0.93269231, 0.93269231, 0.94230769,\n         0.93269231, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.94230769, 0.93269231, 0.94230769, 0.94230769]),\n  'custom_importance_sampling': array([0.47115385, 0.64423077, 0.875     , 0.88461538, 0.93269231,\n         0.90384615, 0.93269231, 0.94230769, 0.93269231, 0.96153846,\n         0.95192308, 0.92307692, 0.93269231, 0.94230769, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.96153846,\n         0.96153846, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.95192308,\n         0.95192308, 0.96153846, 0.95192308, 0.95192308, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.95192308, 0.95192308,\n         0.95192308, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.95192308, 0.95192308, 0.96153846, 0.96153846, 0.96153846,\n         0.95192308, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.97115385, 0.97115385, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.95192308, 0.96153846, 0.95192308, 0.95192308, 0.96153846,\n         0.96153846, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.96153846, 0.95192308, 0.96153846, 0.95192308]),\n  'custom_greedy_subset': array([0.42307692, 0.72115385, 0.88461538, 0.93269231, 0.95192308,\n         0.95192308, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.97115385,\n         0.97115385, 0.96153846, 0.96153846, 0.97115385, 0.97115385,\n         0.97115385, 0.97115385, 0.97115385, 0.97115385, 0.97115385,\n         0.97115385, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846]),\n  'custom_adaptive_frequency': array([0.48076923, 0.72115385, 0.81730769, 0.89423077, 0.91346154,\n         0.91346154, 0.90384615, 0.88461538, 0.88461538, 0.89423077,\n         0.94230769, 0.92307692, 0.92307692, 0.92307692, 0.92307692,\n         0.92307692, 0.92307692, 0.92307692, 0.92307692, 0.93269231,\n         0.92307692, 0.94230769, 0.95192308, 0.95192308, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.95192308, 0.96153846,\n         0.96153846, 0.96153846, 0.95192308, 0.95192308, 0.95192308,\n         0.94230769, 0.94230769, 0.94230769, 0.94230769, 0.94230769,\n         0.94230769, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.95192308,\n         0.95192308, 0.95192308, 0.95192308, 0.95192308, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846,\n         0.96153846, 0.96153846, 0.96153846, 0.96153846, 0.96153846]),\n  'baseline_gradient': array([0.47115385, 0.54807692, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231,\n         0.55769231, 0.55769231, 0.55769231, 0.55769231, 0.55769231])}}"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullBreakdown(X, y, param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:36.553211Z",
     "start_time": "2025-02-25T04:43:35.077292Z"
    }
   },
   "id": "2e7e1cd3c72d61ea",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Baseline Logistic Regression Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Evaluate performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Custom Baseline Logistic Regression Accuracy: {accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:36.557863Z",
     "start_time": "2025-02-25T04:43:36.556372Z"
    }
   },
   "id": "86f606e51141a5d6",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 1 1 2 2 2 2 2 2 2 1 1 2 1 2 2 2 1]\n",
      "[1 2 2 1 1 2 2 2 2 2 2 2 1 1 2 1 2 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:20])\n",
    "print(y_test[:20].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:36.562043Z",
     "start_time": "2025-02-25T04:43:36.558868Z"
    }
   },
   "id": "88e9743176381ab8",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T04:43:36.562892Z",
     "start_time": "2025-02-25T04:43:36.560760Z"
    }
   },
   "id": "a22005022a497800",
   "execution_count": 80
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
