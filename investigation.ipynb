{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "get and format dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81dcf1a8e907311"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "0      1    14.23       1.71  2.43               15.6        127   \n",
      "1      1    13.20       1.78  2.14               11.2        100   \n",
      "2      1    13.16       2.36  2.67               18.6        101   \n",
      "3      1    14.37       1.95  2.50               16.8        113   \n",
      "4      1    13.24       2.59  2.87               21.0        118   \n",
      "\n",
      "   Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# load in wine \n",
    "import pandas as pd\n",
    "\n",
    "# Define column names\n",
    "column_names = [\n",
    "    \"Class\", \"Alcohol\", \"Malicacid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\",\n",
    "    \"Total_phenols\", \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\",\n",
    "    \"Color_intensity\", \"Hue\", \"OD280_OD315_of_diluted_wines\", \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"wine/wine.data\", names=column_names)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-20T22:55:53.764464Z",
     "start_time": "2025-02-20T22:55:53.064792Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
      "125      2    12.07       2.16  2.17               21.0         85   \n",
      "126      2    12.43       1.53  2.29               21.5         86   \n",
      "127      2    11.79       2.13  2.78               28.5         92   \n",
      "128      2    12.37       1.63  2.30               24.5         88   \n",
      "129      2    12.04       4.30  2.38               22.0         80   \n",
      "\n",
      "     Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
      "125           2.60        2.65                  0.37             1.35   \n",
      "126           2.74        3.15                  0.39             1.77   \n",
      "127           2.13        2.24                  0.58             1.76   \n",
      "128           2.22        2.45                  0.40             1.90   \n",
      "129           2.10        1.75                  0.42             1.35   \n",
      "\n",
      "     Color_intensity   Hue  OD280_OD315_of_diluted_wines  Proline  \n",
      "125             2.76  0.86                          3.28      378  \n",
      "126             3.94  0.69                          2.84      352  \n",
      "127             3.00  0.97                          2.44      466  \n",
      "128             2.12  0.89                          2.78      342  \n",
      "129             2.60  0.79                          2.57      580  \n"
     ]
    }
   ],
   "source": [
    "# Remove rows where Class is 3\n",
    "df = df[df[\"Class\"] != 3]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.tail())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T22:55:53.772200Z",
     "start_time": "2025-02-20T22:55:53.767367Z"
    }
   },
   "id": "2d8045463ae6a81c",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement a logistic regression baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5225897f44a5679"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T22:55:54.983594Z",
     "start_time": "2025-02-20T22:55:53.773188Z"
    }
   },
   "id": "118114fbb2dd5fd0",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop(columns=['Class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T22:55:54.990975Z",
     "start_time": "2025-02-20T22:55:54.985482Z"
    }
   },
   "id": "7fd4db81cc7da185",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=700)",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=700)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=700)</pre></div> </div></div></div></div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=700)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T22:55:55.056476Z",
     "start_time": "2025-02-20T22:55:54.991380Z"
    }
   },
   "id": "dd62d0b2c944a9e3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Baseline Logistic Regression Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T22:55:55.057110Z",
     "start_time": "2025-02-20T22:55:55.049653Z"
    }
   },
   "id": "258146464d8cdf32",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need a custom class so we can directly modify the loss function... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d419fb738aa61fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, max_iter=2000, tol=1e-4, fit_intercept=True, reg_lambda=0.01):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol  # Tolerance for convergence (minimum improvement required)\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weight = None\n",
    "        self.feature_means = None\n",
    "        self.feature_stds = None\n",
    "\n",
    "    def _normalize_features(self, X):\n",
    "        \"\"\"Normalizes features using z-score standardization.\"\"\"\n",
    "        if self.feature_means is None or self.feature_stds is None:\n",
    "            self.feature_means = np.mean(X, axis=0)\n",
    "            self.feature_stds = np.std(X, axis=0) + 1e-8  # Avoid division by zero\n",
    "        \n",
    "        return (X - self.feature_means) / self.feature_stds\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Adds intercept term (bias) to feature matrix.\"\"\"\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip values to prevent overflow\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return expit(z)  # Numerically stable sigmoid\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"Computes the binary cross-entropy loss with regularization.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Add L2 regularization term (excluding bias)\n",
    "        reg_term = 0.5 * self.reg_lambda * np.sum(self.weight[1:] ** 2) / m\n",
    "        \n",
    "        # Compute binary cross-entropy\n",
    "        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        \n",
    "        return loss + reg_term\n",
    "\n",
    "    def _compute_gradient(self, X, y):\n",
    "        \"\"\"Computes the gradient of the loss function with regularization.\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(X @ self.weight)\n",
    "        \n",
    "        # Basic gradient\n",
    "        gradient = (X.T @ (h - y)) / m\n",
    "        \n",
    "        # Add L2 regularization\n",
    "        reg_term = np.zeros_like(self.weight)\n",
    "        reg_term[1:] = self.reg_lambda * self.weight[1:] / m  # Don't regularize bias\n",
    "        \n",
    "        return gradient + reg_term\n",
    "\n",
    "    def _handle_class_imbalance(self, X, y):\n",
    "        \"\"\"Calculate class weights for imbalanced datasets.\"\"\"\n",
    "        unique_classes = np.unique(y)\n",
    "        class_weights = {}\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            class_weights[cls] = len(y) / (len(unique_classes) * np.sum(y == cls))\n",
    "            \n",
    "        # Create sample weights array\n",
    "        sample_weights = np.ones(len(y))\n",
    "        for cls in unique_classes:\n",
    "            sample_weights[y == cls] = class_weights[cls]\n",
    "            \n",
    "        return sample_weights\n",
    "    \n",
    "    def fit(self, X, y, handle_imbalance=True, verbose=True):\n",
    "        \"\"\"Trains the logistic regression model using gradient descent.\"\"\"\n",
    "        # Convert inputs to numpy arrays if they're not already\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "\n",
    "        # Initialize weights with small random values\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.weight = np.random.randn(X_normalized.shape[1]) * 0.01\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        if handle_imbalance:\n",
    "            sample_weights = self._handle_class_imbalance(X, y)\n",
    "        else:\n",
    "            sample_weights = np.ones(len(y))\n",
    "            \n",
    "        previous_loss = float('inf')\n",
    "        converged = False\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Compute weighted gradient\n",
    "            h = self._sigmoid(X_normalized @ self.weight)\n",
    "            gradient = (X_normalized.T @ ((h - y) * sample_weights)) / np.sum(sample_weights)\n",
    "            \n",
    "            # Add regularization\n",
    "            reg_term = np.zeros_like(self.weight)\n",
    "            reg_term[1:] = self.reg_lambda * self.weight[1:] / len(y)\n",
    "            gradient += reg_term\n",
    "            \n",
    "            # Update weights\n",
    "            new_weight = self.weight - self.lr * gradient\n",
    "            \n",
    "            # Calculate loss for convergence check\n",
    "            current_loss = self._compute_loss(X_normalized, y)\n",
    "            \n",
    "            # Check convergence based on loss improvement\n",
    "            if abs(previous_loss - current_loss) < self.tol:\n",
    "                if verbose:\n",
    "                    print(f'Converged at iteration {i}, loss: {current_loss:.6f}')\n",
    "                converged = True\n",
    "                break\n",
    "                \n",
    "            self.weight = new_weight\n",
    "            previous_loss = current_loss\n",
    "            \n",
    "            # Optional: adaptive learning rate\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                self.lr *= 0.9  # Reduce learning rate over time\n",
    "                \n",
    "        if not converged and verbose:\n",
    "            print(f'Did not converge after {self.max_iter} iterations. Final loss: {current_loss:.6f}')\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns the probability predictions.\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "\n",
    "        return self._sigmoid(X_normalized @ self.weight)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predicts class labels (0 or 1) based on a threshold.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int) + 1\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Returns the absolute weights as feature importance.\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            return np.abs(self.weight[1:])\n",
    "        return np.abs(self.weight)\n",
    "    \n",
    "    \n",
    "    def fit_stochastic_coordinate_descent(self, X, y, handle_imbalance=True, verbose=True):\n",
    "        \"\"\"Trains the logistic regression model using stochastic coordinate descent.\"\"\"\n",
    "        # Convert inputs to numpy arrays\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_normalized = self._normalize_features(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_normalized = self._add_intercept(X_normalized)\n",
    "        \n",
    "        # Initialize weights\n",
    "        np.random.seed(42)\n",
    "        n_features = X_normalized.shape[1]\n",
    "        self.weight = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        if handle_imbalance:\n",
    "            sample_weights = self._handle_class_imbalance(X, y)\n",
    "        else:\n",
    "            sample_weights = np.ones(len(y))\n",
    "        \n",
    "        previous_loss = float('inf')\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Randomly select a coordinate (feature)\n",
    "            j = np.random.randint(0, n_features)\n",
    "            \n",
    "            # Compute gradient for the selected coordinate\n",
    "            h = self._sigmoid(X_normalized @ self.weight)\n",
    "            gradient_j = np.sum((h - y) * sample_weights * X_normalized[:, j]) / np.sum(sample_weights)\n",
    "            \n",
    "            # Add regularization (except for bias term)\n",
    "            if j > 0 or not self.fit_intercept:\n",
    "                gradient_j += self.reg_lambda * self.weight[j] / len(y)\n",
    "            \n",
    "            # Update only the selected coordinate\n",
    "            self.weight[j] -= self.lr * gradient_j\n",
    "            \n",
    "            # Check convergence every 10*n_features updates\n",
    "            if i % (10 * n_features) == 0:\n",
    "                current_loss = self._compute_loss(X_normalized, y)\n",
    "                if abs(previous_loss - current_loss) < self.tol:\n",
    "                    if verbose:\n",
    "                        print(f'Converged at iteration {i}, loss: {current_loss:.6f}')\n",
    "                    break\n",
    "                previous_loss = current_loss\n",
    "                \n",
    "                # Adaptive learning rate\n",
    "                if i > 0 and i % (50 * n_features) == 0:\n",
    "                    self.lr *= 0.9\n",
    "        \n",
    "        if verbose and i == self.max_iter - 1:\n",
    "            print(f'Did not converge after {self.max_iter} iterations. Final loss: {current_loss:.6f}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def grid_search(X, y, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=True, random_SGD=False):\n",
    "       \"\"\"\n",
    "       Perform grid search to find optimal hyperparameters.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       X : array-like, shape (n_samples, n_features)\n",
    "           Training data\n",
    "       y : array-like, shape (n_samples,)\n",
    "           Target values\n",
    "       param_grid : dict\n",
    "           Dictionary with parameter names as keys and lists of parameter values\n",
    "       cv : int, default=5\n",
    "           Number of cross-validation folds\n",
    "       scoring : str, default='accuracy'\n",
    "           Scoring metric ('accuracy', 'precision', 'recall', 'f1', 'auc')\n",
    "       n_jobs : int, default=-1\n",
    "           Number of jobs to run in parallel (-1 means using all processors)\n",
    "       verbose : bool, default=True\n",
    "           Whether to print progress\n",
    "       use_stochastic_cd : bool, default=False\n",
    "           Whether to use stochastic coordinate descent instead of batch gradient descent\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       dict : Best parameters and corresponding score\n",
    "       \"\"\"\n",
    "       # Convert inputs to numpy arrays if they're not already\n",
    "       X = np.asarray(X)\n",
    "       y = np.asarray(y)\n",
    "       \n",
    "       # Prepare parameter combinations\n",
    "       param_names = list(param_grid.keys())\n",
    "       param_values = list(param_grid.values())\n",
    "       param_combinations = list(product(*param_values))\n",
    "       \n",
    "       # Split data into folds\n",
    "       n_samples = len(y)\n",
    "       indices = np.arange(n_samples)\n",
    "       np.random.seed(42)  # For reproducible CV splits\n",
    "       np.random.shuffle(indices)\n",
    "       fold_sizes = np.full(cv, n_samples // cv, dtype=int)\n",
    "       fold_sizes[:n_samples % cv] += 1\n",
    "       \n",
    "       current_idx = 0\n",
    "       folds = []\n",
    "       for fold_size in fold_sizes:\n",
    "           fold_indices = indices[current_idx:current_idx + fold_size]\n",
    "           folds.append(fold_indices)\n",
    "           current_idx += fold_size\n",
    "           \n",
    "       # Define evaluation function for a single parameter combination\n",
    "       def evaluate_params(params):\n",
    "           param_dict = {param_names[i]: params[i] for i in range(len(param_names))}\n",
    "           \n",
    "           if verbose:\n",
    "               print(f\"Evaluating parameters: {param_dict}\")\n",
    "           \n",
    "           scores = []\n",
    "           for i in range(cv):\n",
    "               # Split data\n",
    "               test_idx = folds[i]\n",
    "               train_idx = np.concatenate([folds[j] for j in range(cv) if j != i])\n",
    "               \n",
    "               X_train, X_test = X[train_idx], X[test_idx]\n",
    "               y_train, y_test = y[train_idx], y[test_idx]\n",
    "               \n",
    "               # Train model with current parameters\n",
    "               model = LogisticRegression(**param_dict)\n",
    "               \n",
    "               # Use stochastic coordinate descent if specified\n",
    "               if random_SGD:\n",
    "                   model.fit_stochastic_coordinate_descent(X_train, y_train, verbose=False)\n",
    "               else:\n",
    "                   model.fit(X_train, y_train, verbose=False)\n",
    "               \n",
    "               # Evaluate based on scoring metric\n",
    "               if scoring == 'accuracy':\n",
    "                   score = model.score(X_test, y_test)\n",
    "               elif scoring == 'precision':\n",
    "                   y_pred = model.predict(X_test)\n",
    "                   score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "               elif scoring == 'recall':\n",
    "                   y_pred = model.predict(X_test)\n",
    "                   score = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "               elif scoring == 'f1':\n",
    "                   y_pred = model.predict(X_test)\n",
    "                   precision = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_pred == 2) if np.sum(y_pred == 2) > 0 else 0\n",
    "                   recall = np.sum((y_pred == 2) & (y_test == 2)) / np.sum(y_test == 2) if np.sum(y_test == 2) > 0 else 0\n",
    "                   score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "               elif scoring == 'auc':\n",
    "                   try:\n",
    "                       from sklearn.metrics import roc_auc_score\n",
    "                       y_prob = model.predict_proba(X_test)\n",
    "                       score = roc_auc_score(y_test == 2, y_prob)\n",
    "                   except ImportError:\n",
    "                       # Fallback if sklearn is not available\n",
    "                       y_prob = model.predict_proba(X_test)\n",
    "                       # Manual AUC calculation (simplified)\n",
    "                       pos_scores = y_prob[y_test == 2]\n",
    "                       neg_scores = y_prob[y_test != 2]\n",
    "                       if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "                           score = 0.5\n",
    "                       else:\n",
    "                           n_pos = len(pos_scores)\n",
    "                           n_neg = len(neg_scores)\n",
    "                           score = sum(pos > neg for pos in pos_scores for neg in neg_scores) / (n_pos * n_neg)\n",
    "               else:\n",
    "                   raise ValueError(f\"Unknown scoring metric: {scoring}\")\n",
    "               \n",
    "               scores.append(score)\n",
    "           \n",
    "           mean_score = np.mean(scores)\n",
    "           if verbose:\n",
    "               print(f\"Parameters {param_dict} - Mean {scoring}: {mean_score:.4f}\")\n",
    "           \n",
    "           return param_dict, mean_score\n",
    "       \n",
    "       # For small grid sizes or when parallelism causes issues, run sequentially\n",
    "       if len(param_combinations) <= 4 or n_jobs == 1:\n",
    "           results = [evaluate_params(params) for params in param_combinations]\n",
    "       else:\n",
    "           # Run evaluations in parallel\n",
    "           try:\n",
    "               n_jobs = n_jobs if n_jobs > 0 else multiprocessing.cpu_count()\n",
    "               results = Parallel(n_jobs=n_jobs)(\n",
    "                   delayed(evaluate_params)(params) for params in param_combinations\n",
    "               )\n",
    "           except Exception as e:\n",
    "               if verbose:\n",
    "                   print(f\"Parallel execution failed with error: {str(e)}\")\n",
    "                   print(\"Falling back to sequential execution...\")\n",
    "               results = [evaluate_params(params) for params in param_combinations]\n",
    "       \n",
    "       # Find best parameters\n",
    "       best_params, best_score = max(results, key=lambda x: x[1])\n",
    "       \n",
    "       if verbose:\n",
    "           opt_method = \"stochastic coordinate descent\" if random_SGD else \"batch gradient descent\"\n",
    "           print(f\"\\nGrid Search Results (using {opt_method}):\")\n",
    "           print(f\"Best parameters: {best_params}\")\n",
    "           print(f\"Best {scoring}: {best_score:.4f}\")\n",
    "       \n",
    "       return {\n",
    "           'best_params': best_params,\n",
    "           'best_score': best_score,\n",
    "           'all_results': results\n",
    "       }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T23:27:43.623737Z",
     "start_time": "2025-02-20T23:27:43.619245Z"
    }
   },
   "id": "a2c657a8dc411921",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Results (using stochastic coordinate descent):\n",
      "Best parameters: {'lr': 0.001, 'reg_lambda': 0.0, 'max_iter': 1000}\n",
      "Best accuracy: 0.7846\n",
      "Did not converge after 1000 iterations. Final loss: 0.085889\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.LogisticRegression at 0x144f20430>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'reg_lambda': [0.0, 0.01, 0.1, 1.0],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = LogisticRegression.grid_search(X, y, param_grid, verbose=True, random_SGD=True)\n",
    "\n",
    "# Create model with best parameters\n",
    "best_model = LogisticRegression(**results['best_params'])\n",
    "best_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T23:27:59.524576Z",
     "start_time": "2025-02-20T23:27:59.240923Z"
    }
   },
   "id": "188ffcd897f4717",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Baseline Logistic Regression Accuracy: 0.8076923076923077\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Evaluate performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Custom Baseline Logistic Regression Accuracy: {accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T23:15:44.344599Z",
     "start_time": "2025-02-20T23:15:44.334778Z"
    }
   },
   "id": "86f606e51141a5d6",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 2 2 1 2 2 2 2 1 2 1 2 2 1 1 2 1 2]\n",
      "[1 1 1 1 2 1 2 2 1 2 1 2 1 2 1 1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:20])\n",
    "print(y_test[:20].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T23:14:01.149376Z",
     "start_time": "2025-02-20T23:14:01.140486Z"
    }
   },
   "id": "88e9743176381ab8",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Class'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/Human-AI Research/Coordinate Descent/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:175\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/index_class_helper.pxi:70\u001B[0m, in \u001B[0;36mpandas._libs.index.Int64Engine._check_type\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43my_test\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mClass\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtail)\n",
      "File \u001B[0;32m~/Documents/Human-AI Research/Coordinate Descent/.venv/lib/python3.9/site-packages/pandas/core/series.py:1121\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[0;32m-> 1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[1;32m   1124\u001B[0m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n",
      "File \u001B[0;32m~/Documents/Human-AI Research/Coordinate Descent/.venv/lib/python3.9/site-packages/pandas/core/series.py:1237\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[0;34m(self, label, takeable)\u001B[0m\n\u001B[1;32m   1234\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[1;32m   1236\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[0;32m-> 1237\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[1;32m   1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[loc]\n",
      "File \u001B[0;32m~/Documents/Human-AI Research/Coordinate Descent/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Class'"
     ]
    }
   ],
   "source": [
    "print(y_test['Class'].tail)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T23:12:48.120412Z",
     "start_time": "2025-02-20T23:12:48.070604Z"
    }
   },
   "id": "f2df06af2e8b978",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a22005022a497800"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
